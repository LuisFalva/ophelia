{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', int(1e7))\n",
    "pd.set_option('display.max_rows', int(1e7))\n",
    "pd.set_option('display.width', int(1e7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining OpheliaSpark Session for pseudo-distributed computing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Single_Value_Decomposition_Portfolio').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading persisted Portfolio Yields dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_yield_window_path = 'data/staging/benchmark/close_day_price'\n",
    "portfolio_yield_df = spark.read.parquet(portfolio_yield_window_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "portfolio_yield_df.show(5, False)\n",
    "portfolio_yield_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The calculation is performed using Singular Value Decomposition (SVD). The SVD of any $m x n$ array is calculated as follows:\n",
    "\n",
    "$$A = U \\sum V^{T}$$\n",
    "\n",
    "### Where $U$ is an orthogonal matrix $m x m$ whose columns are the eigenvectors (eigenvectors) of $AA^{T}$, $V$ is an orthogonal matrix $n x n$ whose columns are the eigenvectors of $A^{T}A$, and $\\sum$ is a diagonal matrix $m x n$ and its values are zero except along the diagonal.\n",
    "\n",
    "### When applying PCA, we have to center our data, that is, depending on its nature, we may need to standardize (make each characteristic have a variance of 1 and a mean of 0). If the columns are on different scales like the year, the temperature, the concentration of carbon dioxide, we have to standardize the data. If the data is on the same drive, on the other hand, standardization can lead to the loss of important information. In the first case, when the columns are in the same unit and on a similar scale, we use the covariance matrix for SVD but when the units are different since we standardize the data, we use the correlation matrix.\n",
    "\n",
    "### The principal components (PC) are the matrix product of the original data and the matrix $V$, which is equal to the product of the matrices $U$ and $\\sum$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Value Decomposition analysis.\n",
    "\n",
    "### At the very first step we have to take two input parameters, one is called ___n___, that refers to the total count of rows in dataframe. The second refers to the total number of columns called _features_, i.e. ___d___. Thus we will find this matrix with _(n, d)_ dimensions.\n",
    "\n",
    "### What do we want to confirm is that every vector $\\vec{V_i}$ of length d is a _dense vector_. This is, we want to get full vectors without any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_features = portfolio_yield_df.columns[1:-2]\n",
    "vec_df = VectorAssembler(inputCols=d_features, outputCol='features').transform(portfolio_yield_df)\\\n",
    "                                                                    .select('features', monotonically_increasing_id().alias('id'))\n",
    "n = vec_df.count()\n",
    "d = len(d_features)\n",
    "print('test with d =', d, 'features and n =', n, 'samples')\n",
    "vec_df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's standarize this dense vectors of length __d__ with the _Standard Scaler_ method, i.e. Mean and Standard Deviation are involved for this standarization (re-scaled vectors of features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(\n",
    "    withMean=True, withStd=True, inputCol=\"features\", outputCol=\"scaled_features\"\n",
    ")\n",
    "\n",
    "vec_scale_df = scaler.fit(vec_df).transform(vec_df)\n",
    "vec_scale_df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to compute SVD we have to transfrom spark-dataframe to a matrix object with indexed elements from scaled features, for that, we will use _IndexedRowMatrix_ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.linalg.distributed import IndexedRowMatrix, IndexedRow\n",
    "\n",
    "df = MLUtils.convertVectorColumnsFromML(vec_scale_df, \"scaled_features\").drop('features')\n",
    "monthly_return_irm = IndexedRowMatrix(df.select('scaled_features', 'id').rdd.map(lambda x: IndexedRow(x[1], x[0])))\n",
    "monthly_return_irm.numCols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = {\n",
    "    'd': 10,\n",
    "    'n': 50,\n",
    "    'U': 'matrix_u'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = {\n",
    "    'params': svd,\n",
    "    'pc': [1,2,3,4,5],\n",
    "    'k': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc['params']['d']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's compute the singular value decomposition of the IndexedRowMatrix. The given row matrix $A$ of dimension __$(m x n)$__ is decomposed into\n",
    "### _$$U s V^{T}$$ where:_\n",
    "* $U$: $(m x k)$ __*left singular vectors* is a IndexedRowMatrix whose columns are the eigenvectors of $(A X A')$__\n",
    "* $s$: __DenseVector consisting of square root of the eigenvalues *singular values* in descending order.__\n",
    "* $V$: $(n x k)$ __*right singular vectors* is a Matrix whose columns are the eigenvectors of $(A' X A)$__\n",
    "\n",
    "### This _computeSVD_ interface recieves two main arguments:\n",
    "* $k$, for $k^{th}$ int number, thus each element $k$ = {${k_{i} \\in \\Bbb R}$}\n",
    "* $U$, with _computeU_ boolean __True__, whether or not to compute $U$. If set to be __True__, then $U$ is computed by $A  V  s^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD = monthly_return_irm.computeSVD(k=10, computeU=True)\n",
    "print(\"SVD:\", SVD)\n",
    "\n",
    "U = SVD.U\n",
    "print(\"\\nU matrix:\", U.toRowMatrix().computePrincipalComponents(10))\n",
    "\n",
    "S = SVD.s.toArray()\n",
    "print(\"\\nS matrix:\", S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.toRowMatrix().rows.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.rows.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.flipud(np.sort(eigen_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.diag([1.0, 2, 3])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.flipud(np.sort(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_vals = S**2 / (n-1)\n",
    "print(\"eigen values:\", eigen_vals)\n",
    "\n",
    "eigvals = np.flipud(np.sort(eigen_vals))\n",
    "print(\"\\neigen vecs:\", eigvals)\n",
    "\n",
    "cumsum = eigvals.cumsum()\n",
    "print(\"\\ncumsum:\", cumsum)\n",
    "\n",
    "total_variance_explained = cumsum / eigvals.sum()\n",
    "print(\"\\ntotal variance explained:\", total_variance_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.argmax(total_variance_explained > 0.95) + 1\n",
    "print(\"total K's find:\", K)\n",
    "\n",
    "V = SVD.V\n",
    "\n",
    "U_ = U.rows.map(lambda x: (x.index, x.vector[0:K] * S[0:K]))\n",
    "print(\"\\nmatrix U:\")\n",
    "U_.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "princ_comps = np.array(list(map(lambda x: x[1], sorted(U_.collect(), key=lambda x: x[0]))))\n",
    "princ_comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=K, inputCol=scaler.getOutputCol(), outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(vec_scale_df)\n",
    "transformed_feature = model.transform(vec_scale_df)\n",
    "\n",
    "print(\"total explained variance by PC:\", np.round(100.00 * model.explainedVariance.toArray(), 4))\n",
    "transformed_feature.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pc.toArray()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pcs = np.round(100.00 * model.pc.toArray(), 4)\n",
    "df_pc = pd.DataFrame(pcs, columns = ['PC_'+str(i) for i in range(1, K+1)], index=d_features)\n",
    "print(\"PC's sum:\\n\", df_pc.sum())\n",
    "df_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "for k in range(1, K):\n",
    "    plt.scatter(princ_comps[:, 0], princ_comps[:, k], alpha=0.05)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(featuresCol='pcaFeatures', k=K, seed=12345)\n",
    "model_kmeans = kmeans.fit(transformed_feature)\n",
    "transformed = model_kmeans.transform(transformed_feature)\n",
    "transformed.groupBy(\"prediction\").count().orderBy(col(\"count\").desc()).show()\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ophelia",
   "language": "python",
   "name": "ophelia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
