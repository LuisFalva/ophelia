{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".output_subarea.output_text.output_stream.output_stdout > pre {\n",
       "    width:max-content;\n",
       "}\n",
       ".p-Widget.jp-RenderedText.jp-OutputArea-output > pre {\n",
       "   width:max-content;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_subarea.output_text.output_stream.output_stdout > pre {\n",
    "    width:max-content;\n",
    "}\n",
    ".p-Widget.jp-RenderedText.jp-OutputArea-output > pre {\n",
    "   width:max-content;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.functions import udf, when, rand, struct, col, lit, current_date, current_timestamp, monotonically_increasing_id\n",
    "from pyspark.sql.types import StringType, IntegerType, StructField, StructType, DoubleType, ArrayType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ophelia.spark.start import Ophelia\n",
    "from ophelia.spark.read.spark_read import Read\n",
    "from ophelia.spark.ml.feature_miner import FeatureMiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:11:08.151 Ophelia [TAPE] +---------------------------------------------------------------------+\n",
      "02:11:08.151 Ophelia [INFO] | My name is Ophelia Vendata                                          |\n",
      "02:11:08.151 Ophelia [INFO] | I am an artificial assistant for data mining & ML engine with spark |\n",
      "02:11:08.151 Ophelia [INFO] | Welcome to Ophelia spark miner engine                               |\n",
      "02:11:08.151 Ophelia [INFO] | Lib Version Ophelia.0.0.1                                           |\n",
      "02:11:08.151 Ophelia [WARN] | V for Vendata...                                                    |\n",
      "02:11:08.151 Ophelia [TAPE] +---------------------------------------------------------------------+\n",
      "02:11:08.151 Ophelia [WARN] Initializing Spark Session\n",
      "02:11:18.343 Ophelia [INFO] Spark Version: 3.0.0\n",
      "02:11:18.343 Ophelia [INFO] This Is: 'CRC Model' App\n",
      "02:11:18.344 Ophelia [INFO] Spark Context Initialized Success\n"
     ]
    }
   ],
   "source": [
    "ophelia = Ophelia(\"CRC Model\")\n",
    "sc = ophelia.Spark.build_spark_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:11:50.416 Ophelia [INFO] Read CSV File From Path: data/raw/csv/bank.csv\n",
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- deposit: string (nullable = true)\n",
      "\n",
      "+---+----------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "|age|job       |marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|deposit|\n",
      "+---+----------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "|59 |admin.    |married|secondary|no     |2343   |yes    |no  |unknown|5  |may  |1042    |1       |-1   |0       |unknown |yes    |\n",
      "|56 |admin.    |married|secondary|no     |45     |no     |no  |unknown|5  |may  |1467    |1       |-1   |0       |unknown |yes    |\n",
      "|41 |technician|married|secondary|no     |1270   |yes    |no  |unknown|5  |may  |1389    |1       |-1   |0       |unknown |yes    |\n",
      "|55 |services  |married|secondary|no     |2476   |yes    |no  |unknown|5  |may  |579     |1       |-1   |0       |unknown |yes    |\n",
      "|54 |admin.    |married|tertiary |no     |184    |no     |no  |unknown|5  |may  |673     |2       |-1   |0       |unknown |yes    |\n",
      "+---+----------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = ophelia.SparkSession\n",
    "path = \"data/raw/csv/bank.csv\"\n",
    "\n",
    "customer_data = spark.readFile(path, 'csv', header=True, infer_schema=True)\n",
    "customer_data.printSchema()\n",
    "customer_data.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:12:09.885 Ophelia [INFO] Creating Multi String Indexers\n",
      "02:12:12.687 Ophelia [INFO] Build String Indexer\n",
      "+---+----------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+---------+-------------+---------------+-------------+-------------+----------+-------------+-----------+--------------+-------------+\n",
      "|age|job       |marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|deposit|job_index|marital_index|education_index|default_index|housing_index|loan_index|contact_index|month_index|poutcome_index|deposit_index|\n",
      "+---+----------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+---------+-------------+---------------+-------------+-------------+----------+-------------+-----------+--------------+-------------+\n",
      "|59 |admin.    |married|secondary|no     |2343   |yes    |no  |unknown|5  |may  |1042    |1       |-1   |0       |unknown |yes    |3.0      |0.0          |0.0            |0.0          |1.0          |0.0       |1.0          |0.0        |0.0           |1.0          |\n",
      "|56 |admin.    |married|secondary|no     |45     |no     |no  |unknown|5  |may  |1467    |1       |-1   |0       |unknown |yes    |3.0      |0.0          |0.0            |0.0          |0.0          |0.0       |1.0          |0.0        |0.0           |1.0          |\n",
      "|41 |technician|married|secondary|no     |1270   |yes    |no  |unknown|5  |may  |1389    |1       |-1   |0       |unknown |yes    |2.0      |0.0          |0.0            |0.0          |1.0          |0.0       |1.0          |0.0        |0.0           |1.0          |\n",
      "|55 |services  |married|secondary|no     |2476   |yes    |no  |unknown|5  |may  |579     |1       |-1   |0       |unknown |yes    |4.0      |0.0          |0.0            |0.0          |1.0          |0.0       |1.0          |0.0        |0.0           |1.0          |\n",
      "|54 |admin.    |married|tertiary |no     |184    |no     |no  |unknown|5  |may  |673     |2       |-1   |0       |unknown |yes    |3.0      |0.0          |1.0            |0.0          |0.0          |0.0       |1.0          |0.0        |0.0           |1.0          |\n",
      "+---+----------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+---------+-------------+---------------+-------------+-------------+----------+-------------+-----------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string_feature = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'deposit']\n",
    "indexed_df = customer_data.toStringIndex(string_feature)\n",
    "indexed_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prince"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankd_pd = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['job', 'marital','education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'deposit']\n",
    "bank_select = bankd_pd[cols]\n",
    "bank_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mca = prince.MCA(\n",
    "    n_components=5,\n",
    "    n_iter=3,\n",
    "    copy=True,\n",
    "    check_input=True,\n",
    "    engine='auto',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_mca = mca.fit(bank_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = mca.plot_coordinates(\n",
    "    X=bank_select,\n",
    "    ax=None,\n",
    "    figsize=(8, 8),\n",
    "    show_row_points=False,\n",
    "    row_points_size=10,\n",
    "    show_row_labels=False,\n",
    "    show_column_points=True,\n",
    "    column_points_size=30,\n",
    "    show_column_labels=False,\n",
    "    legend_n_cols=1\n",
    ")\n",
    "#ax.get_figure()#.savefig('images/mca_coordinates.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mca.eigenvalues_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mca.column_coordinates(bank_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_mca = mca.row_coordinates(bank_select)\n",
    "pd_mca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indexed_feature = [f'{c}_index' for c in string_feature]\n",
    "encoded_df = indexed_df.toOHEncoder(string_feature).drop(*indexed_feature)\n",
    "encoded_df.show(5, False)\n",
    "encoded_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# 10 samples with 5 features\n",
    "train_features = np.random.rand(10,5)\n",
    "\n",
    "model = PCA(n_components=2).fit(train_features)\n",
    "X_pc = model.transform(train_features)\n",
    "\n",
    "# number of components\n",
    "n_pcs= model.components_.shape[0]\n",
    "\n",
    "# get the index of the most important feature on EACH component\n",
    "# LIST COMPREHENSION HERE\n",
    "most_important = [np.abs(model.components_[i]).argmax() for i in range(n_pcs)]\n",
    "\n",
    "initial_feature_names = ['a','b','c','d','e']\n",
    "\n",
    "# get the names\n",
    "most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "\n",
    "# LIST COMPREHENSION HERE AGAIN\n",
    "dic = {'PC{}'.format(i+1): most_important_names[i] for i in range(n_pcs)}\n",
    "\n",
    "# build the dataframe\n",
    "df = pd.DataFrame(sorted(dic.items()))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esto es el \"df\" con 10 filas y 5 columnas\n",
    "\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esto es el \"df\" reducido a solo 2 componentes, se mantienen las filas\n",
    "\n",
    "X_pc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esto es 2 componentes como filas y 5 columnas que son las features originales\n",
    "\n",
    "(model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.abs(model.components_[i]).argmax() for i in range(n_pcs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Feature Pattern C.F.P. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ophelia.spark.ml.unsupervised.FeatureExtraction import SingularValueDecomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df = SingularValueDecomposition(k=10).transform(encoded_df.drop(*string_feature))\n",
    "svd_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ophelia.spark.functions.transpose import TransposeDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df.toNarrow(['id'], ['vector_class', 'features']).where(col('vector_class') == 'pca_features').show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Customer Clusterization  K-M.C.C. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(featuresCol='pca_features', k=5, seed=12345)\n",
    "model_kmeans = kmeans.fit(svd_df)\n",
    "kmean_df = model_kmeans.transform(svd_df)\n",
    "kmean_df.groupBy(\"prediction\").count().orderBy(col(\"prediction\")).show()\n",
    "kmean_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Custering for MCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mca_schema = StructType([StructField('C_1', DoubleType(), True),\n",
    "                         StructField('C_2', DoubleType(), True),\n",
    "                         StructField('C_3', DoubleType(), True),\n",
    "                         StructField('C_4', DoubleType(), True),\n",
    "                         StructField('C_5', DoubleType(), True),])\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "mca_df = spark.createDataFrame(pd_mca, mca_schema).select(monotonically_increasing_id().alias('row_num'), \"*\")\n",
    "mca_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mca_vec = mca_df.toVecAssembler(['C_1', 'C_2', 'C_3', 'C_4', 'C_5']).select('*', col('features').alias('pca_features')).drop('features')\n",
    "mca_vec.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mca_kmeans = KMeans(featuresCol='pca_features', k=5, seed=12345)\n",
    "mca_model_kmeans = mca_kmeans.fit(mca_vec)\n",
    "mca_kmean_df = mca_model_kmeans.transform(mca_vec)\n",
    "mca_kmean_df.groupBy(\"prediction\").count().orderBy(col(\"prediction\")).show()\n",
    "mca_kmean_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_y_scatter(mca_kmean_df, xy_lims=[-2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(row):\n",
    "    row_tuple = (\n",
    "        row.id,\n",
    "        row.features, \n",
    "        row.scaled_features,\n",
    "        row.pca_features,\n",
    "        row.prediction\n",
    "    )\n",
    "    return row_tuple + tuple(row.features.toArray().tolist())\n",
    "\n",
    "fix_cols = [\n",
    "    'id',\n",
    "    'features',\n",
    "    'scaled_features',\n",
    "    'pca_features',\n",
    "    'prediction'\n",
    "]\n",
    "extract_feature = kmean_df.rdd.map(extract).toDF(fix_cols)\n",
    "no_name_cols = extract_feature.drop(*fix_cols).columns\n",
    "named_cols = encoded_df.columns\n",
    "cluster_feature = extract_feature.select(*fix_cols, *[col(c).alias(a) for c, a in zip(no_name_cols, named_cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_feature.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_k_pca(X, y, xylims):\n",
    "    \"\"\"\n",
    "    a scatter plot of the 2-dimensional space\n",
    "    \"\"\"\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    plt.rcParams['figure.figsize'] = 15, 7\n",
    "    plt.rcParams['font.size'] = 12\n",
    "    markers = 's', 'x', 'o', '.', '+'\n",
    "    colors = list(plt.rcParams['axes.prop_cycle'])\n",
    "    target = np.unique(y)\n",
    "    for idx, (t, m) in enumerate(zip(target, markers)):\n",
    "        subset = X[y == t]\n",
    "        plt.scatter(subset[:, 0], subset[:, 1], s = 50,\n",
    "                    c = colors[idx]['color'], label = t, marker = m)\n",
    "\n",
    "    plt.xlim(xylims[0], xylims[1])\n",
    "    plt.xlabel('PC 1')\n",
    "    plt.ylim(xylims[0], xylims[1])\n",
    "    plt.ylabel('PC 2')\n",
    "    plt.legend(loc = 'upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def x_y_scatter(df, xy_lims=[-10, 10]):\n",
    "    X_rdd = df.rdd.map(lambda row: row.pca_features)\n",
    "    X_pca = np.array(X_rdd.collect())\n",
    "    y_rdd = df.rdd.map(lambda row: row.prediction)\n",
    "    y = np.array(y_rdd.collect())\n",
    "    print(f\"\\nmat 'X' dim:{X_pca.shape} and vector 'y' dim:{y.shape}\\n\")\n",
    "    print(f\"clustering with K={X_pca.shape[1]}\\n\")\n",
    "    return plot_k_pca(X_pca, y, xy_lims)\n",
    "\n",
    "x_y_scatter(kmean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for k in range(5):\n",
    "    x_y_scatter(kmean_df.where(col('prediction') == k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ophelia",
   "language": "python",
   "name": "ophelia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
