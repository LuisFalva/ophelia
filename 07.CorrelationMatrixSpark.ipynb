{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".output_subarea.output_text.output_stream.output_stdout > pre {\n",
       "    width:max-content;\n",
       "}\n",
       ".p-Widget.jp-RenderedText.jp-OutputArea-output > pre {\n",
       "   width:max-content;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_subarea.output_text.output_stream.output_stdout > pre {\n",
    "    width:max-content;\n",
    "}\n",
    ".p-Widget.jp-RenderedText.jp-OutputArea-output > pre {\n",
    "   width:max-content;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, lag, first, count, max as spark_max, min as spark_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./Ophelia-0.0.1-py3-none-any.whl\n",
      "Collecting numpy==1.19.1\n",
      "  Using cached numpy-1.19.1-cp36-cp36m-macosx_10_9_x86_64.whl (15.3 MB)\n",
      "Collecting pyspark==3.0.0\n",
      "  Using cached pyspark-3.0.0.tar.gz (204.7 MB)\n",
      "Collecting py4j==0.10.9\n",
      "  Using cached py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.0.0-py2.py3-none-any.whl size=205044182 sha256=72070c9f6623fb01788b67b728d10142a733ced5f5028f1c484051d359c6fa2e\n",
      "  Stored in directory: /Users/falva/Library/Caches/pip/wheels/92/da/33/2bda9255ee3867975ac7ae7688d298a6d3f5dbd6fd57490d0f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark, numpy, Ophelia\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.9\n",
      "    Uninstalling py4j-0.10.9:\n",
      "      Successfully uninstalled py4j-0.10.9\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.0.0\n",
      "    Uninstalling pyspark-3.0.0:\n",
      "      Successfully uninstalled pyspark-3.0.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.1\n",
      "    Uninstalling numpy-1.19.1:\n",
      "      Successfully uninstalled numpy-1.19.1\n",
      "  Attempting uninstall: Ophelia\n",
      "    Found existing installation: Ophelia 0.0.1\n",
      "    Uninstalling Ophelia-0.0.1:\n",
      "      Successfully uninstalled Ophelia-0.0.1\n",
      "Successfully installed Ophelia-0.0.1 numpy-1.19.1 py4j-0.10.9 pyspark-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall Ophelia-0.0.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ophelia'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1fc6e79b2483>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mophelia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ophelia'"
     ]
    }
   ],
   "source": [
    "import ophelia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ophelia.spark.OpheliaMain import Ophelia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import install_ophelia\n",
    "#from ophelia.spark.OpheliaMain import Ophelia\n",
    "#module_path = os.path.abspath(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ophelia = Ophelia(\"Risk Classification Demo\", no_mask=False)\n",
    "sc = ophelia.Spark.build_spark_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = ophelia.SparkSession\n",
    "spark_wrapper = ophelia.Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lag\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = ['muchas']\n",
    "\n",
    "\n",
    "w = Window.partitionBy('partition_by').orderBy(col('order_by'))\n",
    "((col('hola') / lag(col('hola'), 1).over(w)) - 1).alias('hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(lambda c: (col(c) / lag(col(c), 1).over(w) - 1).alias(c), lista))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_yield_df.select([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_feature_all(df):\n",
    "    s, i, l, d, f, t, o = [], [], [], [], [], [], []\n",
    "    for k, v in df.dtypes:\n",
    "        s.append(k) if v == 'string' else \\\n",
    "            i.append(k) if v == 'int' else \\\n",
    "            l.append(k) if v == 'bigint' else \\\n",
    "            d.append(k) if v == 'double' else \\\n",
    "            f.append(k) if v == 'float' else \\\n",
    "            t.append(k) if v == 'date' or v == 'timestamp' else \\\n",
    "            o.append(k)\n",
    "    return {'string': s, 'int': i, 'long': l, 'double': d, 'float': f, 'date': t, 'other': o}\n",
    "\n",
    "def select_strings(self):\n",
    "    datatype = self.dtypes\n",
    "    select_type = ['string', 'str', 'AnyStr', 'char']\n",
    "    return [k for k, v in datatype if v in select_type]\n",
    "\n",
    "def select_integers(self):\n",
    "    datatype = self.dtypes\n",
    "    select_type = ['int', 'integer']\n",
    "    return [k for k, v in datatype if v in select_type]\n",
    "\n",
    "def select_floats(self):\n",
    "    datatype = self.dtypes\n",
    "    select_type = ['float']\n",
    "    return [k for k, v in datatype if v in select_type]\n",
    "\n",
    "def select_doubles(self):\n",
    "    datatype = self.dtypes\n",
    "    select_type = ['double']\n",
    "    return [k for k, v in datatype if v in select_type]\n",
    "\n",
    "def select_decimals(self):\n",
    "    datatype = self.dtypes\n",
    "    select_type = ['decimal']\n",
    "    return [k for k, v in datatype if v in select_type]\n",
    "\n",
    "def select_longs(self):\n",
    "    datatype = self.dtypes\n",
    "    select_type = ['long', 'bigint']\n",
    "    return [k for k, v in datatype if v in select_type]\n",
    "\n",
    "def select_dates(self):\n",
    "    datatype = self.dtypes\n",
    "    select_type = ['date', 'timestamp']\n",
    "    return [k for k, v in datatype if v in select_type]\n",
    "\n",
    "def select_categoricals(self):\n",
    "    return select_strings(self) + select_longs(self) + select_dates(self)\n",
    "\n",
    "def select_numerics(self):\n",
    "    return select_integers(self) + select_floats(self) + select_doubles(self) + select_decimals(self) + select_longs(self)\n",
    "\n",
    "def select_features(df, dtype='all'):\n",
    "    \"\"\"\n",
    "    Feature picking function helps to split variable names from spark DataFrame\n",
    "    into 'string', 'int', 'bigint', 'double', 'float', 'date' and 'other' type in separated list\n",
    "    :param df: spark DataFrame with fields to analyze\n",
    "    :return: dict\n",
    "    \"\"\"\n",
    "    \n",
    "    if dtype in ['str', 'string']:\n",
    "        return select_strings(df)\n",
    "    elif dtype in ['int', 'integer']:\n",
    "        return select_integers(df)\n",
    "    elif dtype in ['float']:\n",
    "        return select_floats(df)\n",
    "    elif dtype in ['double']:\n",
    "        return select_doubles(df)\n",
    "    elif dtype in ['decimal']:\n",
    "        return select_decimals(df)\n",
    "    elif dtype in ['long', 'bigint']:\n",
    "        return select_longs(df)\n",
    "    elif dtype in ['date', 'timestamp']:\n",
    "        return select_dates(df)\n",
    "    elif dtype in ['all', 'every', 'global', 'full']:\n",
    "        return select_feature_all(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = ['a', 'b'] + ['c', 'd']\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_integer(portfolio_yield_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_yield_window_path = 'data/staging/benchmark/close_day_price'\n",
    "portfolio_yield_df = spark.read.parquet(portfolio_yield_window_path)\n",
    "portfolio_yield_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_picking(portfolio_yield_df, 'full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ophelia.spark.functions import string_match, union_all\n",
    "from ophelia.spark.utils import regex_expr\n",
    "\n",
    "dic = {\n",
    "    'Product': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C'],\n",
    "    'Year': [2010, 2010, 2010, 2011, 2011, 2011, 2012, 2012, 2012],\n",
    "    'Revenue': [100, 200, 300, 110, 190, 320, 120, 220, 350]\n",
    "}\n",
    "dic_to_df = spark.createDataFrame(pd.DataFrame(data=dic))\n",
    "print(dic_to_df.Shape)\n",
    "dic_to_df.show()\n",
    "dic_to_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log\n",
    "from typing import List, AnyStr, Any\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "\n",
    "def single_string_indexer(single_col: AnyStr) -> StringIndexer:\n",
    "    \"\"\"\n",
    "    Single string indexer method creates 'StringIndexer' pyspark.ml object\n",
    "    :param single_col: str column name from str datatype column to index\n",
    "    :return: StringIndexer\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return StringIndexer(inputCol=single_col, outputCol=\"{COL}_index\".format(COL=single_col))\n",
    "    except TypeError as te:\n",
    "        raise TypeError(f\"An error occurred while calling single_string_indexer() method: {te}\")\n",
    "\n",
    "def multi_string_indexer(multi_col: List[AnyStr]) -> List[StringIndexer]:\n",
    "    \"\"\"\n",
    "    Multi string indexer method creates a list of 'StringIndexer'\n",
    "    pyspark.ml object by each given column\n",
    "    :param multi_col: list for column string names to index\n",
    "    :return: list of multi StringIndexer\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return [StringIndexer(\n",
    "            inputCol=column,\n",
    "            outputCol=\"{COLS}_index\".format(COLS=column)) for column in multi_col]\n",
    "    except TypeError as te:\n",
    "        raise TypeError(f\"An error occurred while calling multi_string_indexer() method: {te}\")\n",
    "\n",
    "\n",
    "def build_string_indexer(df: DataFrame, col_name: Any) -> DataFrame:\n",
    "    \"\"\"\n",
    "    String Indexer builder wrapper for single and multi string indexing\n",
    "    :param df: pyspark DataFrame to transform\n",
    "    :param col_name: any column(s) name(s)\n",
    "    :return: transformed spark DataFrame\n",
    "    \"\"\"\n",
    "    if isinstance(col_name, list):\n",
    "        pipe_ml = Pipeline(stages=[*multi_string_indexer(multi_col=col_name)])\n",
    "    else:\n",
    "        print(\"mañana\")\n",
    "        pipe_ml = Pipeline(stages=[single_string_indexer(single_col=col_name)])\n",
    "    fit_model = pipe_ml.fit(dataset=df)\n",
    "    return fit_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_index = ophelia.MLMiner.build_string_indexer(dic_to_df, ['Product', 'Revenue', 'Year'])\n",
    "str_index.show(5, False)\n",
    "str_index.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ophelia.MLMiner.build_one_hot_encoder(str_index, col_name=['Product', 'Revenue']).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ophelia.ophelib.ml.unsupervised.FeatureExtraction import SingularValueDecomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection = portfolio_yield_df.drop('close_timestamp', 'close_date', 'close_year')\n",
    "\n",
    "svd_df = SingularValueDecomposition(k=10).transform(feature_selection)\n",
    "svd_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tape_message = f\"| Compute SVD With K={10}, d={71}, n={3000} |\"\n",
    "print(\"+\"+\"-\"*(len(tape_message)-2)+\"+\")\n",
    "print(tape_message)\n",
    "print(\"+\"+\"-\"*(len(tape_message)-2)+\"+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tape_message = f\"| Components Over {95}% Of The Variance: k={10} |\"\n",
    "print(\"+\"+\"-\"*(len(tape_message)-2)+\"+\")\n",
    "print(tape_message)\n",
    "print(\"+\"+\"-\"*(len(tape_message)-2)+\"+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_index.select('*', col('Year').cast('bigint')).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Shape* SparkWrapper:\n",
    "### The shape wrapper is added to the Spark DataFrame class in order to have the must commonly used method in Pandas and Numpy type objects, this is pretty useful when you want to track the dimension of the Spark DataFrame at some spaecific transformation stage and get an insight of what your rows and columns number are gathering into different dimensions.\n",
    "> Important note: *shape* method is called as the traditional **.shape** of Pandas an Numpy objects.\n",
    "\n",
    "### It returns: \n",
    "- Tuple with: (total row number, total column number), *such as (n, m) matrix dimension*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *pctChange* from SparkWrapper:\n",
    "### The pct_change wrapper is added to the Spark DataFrame class in order to have the must commonly used method in Pandas objects, this is for getting the relative percentage change between one observation to another sorted by some sortable date-type column and lagged by some laggable numeric-type column. \n",
    "> Important note: you can call *pct_change* method as the traditional **.pct_change** way for Pandas dataframe objects or you can rather specify the parameters of the function. So if any parameter is specified then the method will infere which column to sort and which column to lag in order to get the **relative percentage change**.\n",
    "\n",
    "### It returns: \n",
    "- Tuple with: (total row number, total column number), *such as (n, m) matrix dimension*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One of the many options that we can use this *.pctChange* method is with no parameter specified thus it will infere which column to sort and which column to lag in order to get the **relative percentage change**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.pctChange().show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another option is configuring all recept parameters from the function, the following are:\n",
    "- **periods**; this parameter will control the offset of the lag periods since the default value is 1 this will always return a lag-1 information DataFrame\n",
    "- **partition_by**; the partition parameter will fixed the partition column over the DataFrame e.g. _\"bank_segment\", \"assurance_product_type\"_\n",
    "- **order_by**; order by parameter will be the specific column to order the sequential observations, e.g. _\"balance_date\", \"trade_close_date\", \"contract_date\"_\n",
    "- **pct_cols**; percentage change col (pct_cols) will be the spacific column to lag-over giving back the relative change between one element to other, e.g. *$(x_{t} \\div{x_{t-1}})$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this case we will specify only the **periods** parameter to yield a lag of -2 days over the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.pctChange(periods=2).na.fill(0).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With parameters **partition_by, order_by & pct_cols**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.pctChange(partition_by=\"Product\", order_by=\"Year\", pct_cols=\"Revenue\").na.fill(0).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.pctChange(partition_by=\"Product\", order_by=\"Year\", pct_cols=[\"Year\", \"Revenue\"]).na.fill(0).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_change_df = dic_to_df.pctChange(partition_by=\"Product\", order_by=\"Year\", pct_cols=\"Revenue\").na.fill(0)\n",
    "print(pct_change_df.Shape)\n",
    "pct_change_df.show(5, False)\n",
    "pct_change_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Matrix* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#+-------+----+-------+\n",
    "#|Product|Year|Revenue|\n",
    "#+-------+----+-------+\n",
    "#|      A|2010|    100|\n",
    "#|      B|2010|    200|\n",
    "#|      C|2010|    300|\n",
    "#|      A|2011|    110|\n",
    "#|      B|2011|    190|\n",
    "#|      C|2011|    320|\n",
    "#|      A|2012|    120|\n",
    "#|      B|2012|    220|\n",
    "#|      C|2012|    350|\n",
    "#+-------+----+-------+\n",
    "\n",
    "agg_dict_test = {'Revenue': 'max'}\n",
    "to_matrix_df = dic_to_df.toWide(group_by=\"Product\", pivot_col=\"Year\", agg_dict=agg_dict_test).orderBy(\"Product_Year\")\n",
    "print(to_matrix_df.Shape)\n",
    "to_matrix_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_replace(rows):\n",
    "    for r in range(len(rows)):\n",
    "        if rows[r] == \"Null\":\n",
    "            rows[r] = rows[r-1]\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_replace([1, 2, 3, 'Null', 4, 5, 'Null', 'Null', 'Null', 6, 7, 8, 'Null', 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import AnyStr, Dict\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lit, explode, struct, array, round as spark_round\n",
    "from pyspark.sql.functions import (count, first, sum as spark_sum, min as spark_min,\n",
    "                                   max as spark_max, mean, stddev, variance)\n",
    "\n",
    "def SparkMethods():\n",
    "    return {'sum': spark_sum, 'min': spark_min, 'max': spark_max, 'mean': mean,\n",
    "            'stddev': stddev, 'var': variance, 'first': first, 'count': count}\n",
    "\n",
    "def wide_format(self: DataFrame, group_by: AnyStr, pivot_col: AnyStr, agg_dict: Dict[AnyStr, AnyStr],\n",
    "                rnd: int = 4, rep: int = 20):\n",
    "    \"\"\"\n",
    "    Wide format method will reshape from narrow multidimensional\n",
    "    table to wide tabular format table for feature wide table\n",
    "    :param self: Spark DataFrame inheritance object\n",
    "    :param group_by: str, name for group DataFrame\n",
    "    :param pivot_col: str, name for wide pivot column\n",
    "    :param agg_dict: dict, with wide Spark function\n",
    "    :param rnd: int, for round wide value\n",
    "    :param rep: int, repartition threshold for wide partition optimization\n",
    "    :return: wide Spark DataFrame\n",
    "    \"\"\"\n",
    "    agg_list = []\n",
    "    keys, values = list(agg_dict.keys())[0], list(agg_dict.values())[0]\n",
    "    for k in agg_dict:\n",
    "        for i in range(len(agg_dict[k].split(','))):\n",
    "            strip_string = agg_dict[k].replace(' ', '').split(',')\n",
    "            agg_list.append(spark_round(SparkMethods()[strip_string[i]](k), rnd).alias(f'{strip_string[i]}_{k}'))\n",
    "    group_by_expr = col(group_by).alias(f'{group_by}_{pivot_col}')\n",
    "\n",
    "    if len(list(agg_dict)) == 1:\n",
    "        pivot_df = self.groupBy(group_by_expr).pivot(pivot_col).agg(*agg_list).repartition(rep).na.fill(0)\n",
    "        renamed_cols = [col(c).alias(f\"{c}_{keys}_{values}\") for c in pivot_df.columns[1:]]\n",
    "        return pivot_df.select(f'{group_by}_{pivot_col}', *renamed_cols)\n",
    "\n",
    "    return self.groupBy(group_by_expr).pivot(pivot_col).agg(*agg_list).repartition(rep).na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dict_test = {'Product': 'first'}\n",
    "wide_format(dic_to_df, group_by=\"Revenue\", pivot_col=\"Product\", agg_dict=agg_dict_test).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Panel* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_matrix_df.toPanel(pivot_col='Product_Year', new_col='mean_Revenue').show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Cartesian* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_matrix_df.cartRDD('Product_Year').take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *CorrMat* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "to_matrix_df.corrMatrix().show()\n",
    "end = time.time()\n",
    "print('elapsed:', int(end - start), 'segundos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_matrix_df.corrMatrix(offset=0.9).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *corrStat* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.corrStat('Year', 'Product', agg_dict_test).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *uniqueRow* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.uniqueRow('Product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.uniqueRow('Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *vecAssembler* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_matrix_df.vecAssembler(to_matrix_df.columns[1:]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Join Small* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_matrix_df.joinSmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml.linalg import Vectors\n",
    "#from pyspark.ml.stat import ChiSquareTest\n",
    "#dataset = [[0, Vectors.dense([0, 0, 1])],\n",
    "#           [0, Vectors.dense([1, 0, 1])],\n",
    "#           [1, Vectors.dense([2, 1, 1])],\n",
    "#           [1, Vectors.dense([3, 1, 1])]]\n",
    "#dataset = spark.createDataFrame(dataset, [\"label\", \"features\"])\n",
    "#dataset.show(5, False)\n",
    "#chiSqResult = ChiSquareTest.test(dataset, 'features', 'label')\n",
    "#chiSqResult.show(5, False)\n",
    "#chiSqResult.select(\"degreesOfFreedom\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test con más datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv = spark.read.csv('data/raw/csv/bank.csv', header=True, inferSchema=True)\n",
    "bank_csv.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 127106153880.0\n",
    "\n",
    "array = [((i+2)/4)*i for i in range(1, 1000000)]\n",
    "array[713037]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryHelperSearch(array, target, left_p, right_p):\n",
    "    if left_p > right_p:\n",
    "        return -1\n",
    "    mid_point = (left_p + right_p) // 2\n",
    "    potential_match = array[mid_point]\n",
    "    if target == potential_match:\n",
    "        return mid_point\n",
    "    elif target < potential_match:\n",
    "        return binaryHelperSearch(array, target, left_p, mid_point - 1)\n",
    "    else:\n",
    "        return binaryHelperSearch(array, target, mid_point + 1, right_p)\n",
    "\n",
    "def binarySearch(array, target):\n",
    "    return binaryHelperSearch(array, target, 0, len(array) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarySearch(array, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def __rand_int_suffix(col, n_rows):\n",
    "    random_suffix = str(randint(1, n_rows))\n",
    "    return col + '_' + random_suffix\n",
    "\n",
    "def skewness_reducer(col_to_rand, n_rows):\n",
    "    udf_rand_suffix = udf(__rand_int_suffix, StringType())\n",
    "    return udf_rand_suffix(col_to_rand, lit(n_rows)).alias('rand_id')\n",
    "\n",
    "def join_skewness(self, df, on, how):\n",
    "    remove_skewness_df = df.select('*', skewness_reducer(on, df.))\n",
    "    remove_skewness_self\n",
    "    return self.join(df, on, how)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.select('*', skewness_reducer('education', bank_csv.Shape[0])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Sort Columns Asc* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.sortColAsc().show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FreqItems from Pyspark Vs freq_items from SparkWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.freqItems(['job', 'marital', 'education'], support=0.5).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.sampleBy('marital', fractions={'married':0.5, 'single':0.5, 'divorced':0.5}).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.sample(withReplacement=True, fraction=0.5).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Sample N* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.sampleN(20).show(30, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from pyspark.sql.functions import create_map, lit\n",
    "\n",
    "map_values = {\n",
    "    'A': '0',\n",
    "    'B': '1',\n",
    "    'C': '2'\n",
    "}\n",
    "def map_item(self, origin_col, map_col, map_val):\n",
    "    map_expr = create_map([lit(x) for x in chain(*map_val.items())])\n",
    "    return self.select('*', (map_expr[self[origin_col]]).alias(map_col))\n",
    "\n",
    "map_item(dic_to_df, 'Product', 'bin_product', map_values).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_values = {\n",
    "    'A': '0',\n",
    "    'B': '1',\n",
    "    'C': '2'\n",
    "}\n",
    "\n",
    "dic_to_df.mapItem('Product', 'bin_product', map_values).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Select Regex, Select Contains & Regex Expr* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.selectRegex(regex_expr(['day', 'ous', 'pr'])).show(5, False)\n",
    "bank_csv.selectContains(['day', 'ous', 'pr']).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dict_ = {\n",
    "    'balance': 'mean, count, sum, stddev, var, min',\n",
    "    'housing': 'count, sum',\n",
    "    'loan': 'count',\n",
    "    'age': 'mean'\n",
    "}\n",
    "embedded_rep_test = bank_csv.toMatrix('education', 'deposit', agg_dict_)\n",
    "embedded_rep_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Cosstab* de Pyspark Vs Matrix de SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.where(string_match('contact == telephone')).crosstab('education', 'campaign').orderBy('education_campaign').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agg_dict_ = {\n",
    "    'balance': 'count'\n",
    "}\n",
    "bank_csv.where(string_match('contact == telephone')).toMatrix('education', 'campaign', test_agg_dict_).orderBy('education_campaign').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *crossPct* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_agg_dict_ = {\n",
    "    'balance': 'mean',\n",
    "    'age': 'mean',\n",
    "    'loan': 'count'\n",
    "}\n",
    "test = bank_csv.crossPct('education', 'deposit', _agg_dict_, cols='all')\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tra = test.toPanel('education_deposit', ['label', 'value'])\n",
    "tra.show(50, False)\n",
    "tra.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Select StartsWith & EndsWith* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.selectStartswith('m').show(5, False)\n",
    "bank_csv.selectStartswith(['m', 'd']).show(5, False)\n",
    "\n",
    "bank_csv.selectEndswith('y').show(5, False)\n",
    "bank_csv.selectEndswith(['y', 'h']).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    bank_csv.sampleN(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *ForEach Col* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_3_dict = {\n",
    "    'balance': 'mean',\n",
    "    'housing': 'count',\n",
    "    'age': 'mean'\n",
    "}\n",
    "bank_csv.foreachCol('education', 'deposit', test_3_dict, 'sum').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1247.3143 + 2232.0278"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *ResumeDF & tabularTable* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, functions as f\n",
    "\n",
    "def resume_dataframe(self, group_by=None, new_col=None):\n",
    "    cols_types = [k for k, v in self.dtypes if v != 'string']\n",
    "    if group_by is None:\n",
    "        try:\n",
    "            agg_df = self.agg(*[f.sum(c).alias(c) for c in cols_types])\n",
    "            return agg_df.withColumn(new_col, f.lit('+++ total')).select(new_col, *cols_types)\n",
    "        except Exception as e:\n",
    "            raise AssertionError(f\"empty expression found. {e}\")\n",
    "    return self.groupBy(group_by).agg(*[f.sum(c).alias(c) for c in cols_types])\n",
    "DataFrame.resumeDF = resume_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __expression(cols_list, expr):\n",
    "    expr_dict = {\n",
    "        'sum': '+'.join(cols_list),\n",
    "        'sub': '-'.join(cols_list),\n",
    "        'mul': '*'.join(cols_list),\n",
    "        'div': '/'.join(cols_list),\n",
    "    }\n",
    "    return expr_dict[expr]\n",
    "\n",
    "def foreach_col(self, group_by, pivot_col, agg_dict, oper):\n",
    "    func = []\n",
    "    regex_keys = list(agg_dict.keys())\n",
    "    regex_values = list(agg_dict.values())\n",
    "    df = self.toMatrix(group_by, pivot_col, agg_dict)\n",
    "    for i in range(len(regex_keys)):\n",
    "        cols_list = df.selectRegex(regex_expr(regex_keys[i])).columns\n",
    "        expression = f.expr(__expression(cols_list, oper))\n",
    "        func.append(expression.alias(f'{regex_keys[i]}_{regex_values[i]}_{oper}'))\n",
    "    return df.select('*', *func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tab_table(self, group_by, pivot_col, agg_dict, oper='sum'):\n",
    "    sum_by_col_df = foreach_col(self, group_by, pivot_col, agg_dict, oper)\n",
    "    return sum_by_col_df.union(resume_dataframe(sum_by_col_df, new_col=self.columns[0]))\n",
    "DataFrame.tabularTable = tab_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.resumeDF(group_by='education').show()\n",
    "bank_csv.resumeDF(new_col='education').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_4_dict = {\n",
    "    'balance': 'mean, sum',\n",
    "    'age': 'mean',\n",
    "    'duration': 'mean'\n",
    "}\n",
    "bank_csv.tabularTable('education', 'deposit', test_4_dict).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Empty Scan* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.emptyScan().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Union All* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('before union:', bank_csv.count())\n",
    "u = union_all([bank_csv, bank_csv, bank_csv, bank_csv, bank_csv])\n",
    "print('after union:', u.count())\n",
    "u.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.rollingDown('balance', 'month', method='mean', window=20).show(25, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.rollingDown('marital', 'month', method='mean', window=5).show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df_orders.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_orders.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from functools import reduce\n",
    "#\n",
    "#\n",
    "#list_comp = [test.groupBy(i).count().sort(f.col(\"count\").desc()).limit(1).select(f.lit(i).alias(\"col\"), f.col(i).alias(\"mode\")) for i in test.columns]\n",
    "#mode = reduce(lambda a, b: a.union(b), list_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mode.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplicación de df's cuadradas\n",
    "# multiplicación de df x df_vector\n",
    "# multiplicación de escalar x df y df_vector\n",
    "# todos usaran el metodo transpose para hacer las multiplicaciones columnares siempre\n",
    "# método split train, test que calcule el tamaño de la muestra y genere de eso el dataset de train y test\n",
    "# muestreo estratificado aleatorio\n",
    "# toMatrix generando la mediana por cortes [0.5, 0.25, 0.75]\n",
    "# toMatrix generando la moda\n",
    "# separador de strings en columnas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ophelia",
   "language": "python",
   "name": "ophelia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
