{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from itertools import chain\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import date_format, to_date, col, year, month, dayofmonth, sum as spark_sum, when, create_map, lit, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 10000000)\n",
    "pd.set_option('display.max_rows', 10000000)\n",
    "pd.set_option('display.width', 10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Spark Session for pseudo-distributed computing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Exploratory_Analysis').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CSV daily price Funds file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_path_file = 'data-resources/data.csv\n",
    "portfolio_data = spark.read.format(\"csv\").options(header=\"true\").load(portfolio_path_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change impure schema portfolio input data.\n",
    "### Defining portfolio dataframe data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_portfolio = [date_format(\n",
    "    to_date(col(portfolio_data.columns[0]), 'dd/MM/yyyy'),\n",
    "    'yyyy-MM-dd').cast('date').alias('operation_date')] + [col(x).cast('float') for x in portfolio_data.columns[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering operation dates without nulls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_data_ns = portfolio_data.where(col(portfolio_data.columns[0]).isNotNull())\\\n",
    "                                        .select(schema_portfolio)\n",
    "\n",
    "portfolio_data_ns.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_field_mod1 = ['operation_date']\n",
    "writing_path_mod1 = '/data/core/fince/data/portfolioOptimization/price_wharehouse_transform/'\n",
    "print('\\nWriting parquets ...\\n')\n",
    "portfolio_data_ns.repartition(1).write.mode('overwrite').parquet(writing_path_mod1, partitionBy=partition_field_mod1)\n",
    "\n",
    "%time\n",
    "print('\\nSUCCESS \\nPARQUET DATA SAVED!')\n",
    "print('\\nNew root path table data:', writing_path_mod1+'operation_date=yyy-MM-dd', '\\nparquet chunks portitioned by:', partition_field_mod1)\n",
    "\n",
    "portfolio_path_parquet = '/data/core/fince/data/portfolioOptimization/price_wharehouse_transform/'\n",
    "portfolio_df = spark.read.parquet(portfolio_path_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year parameters input array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_param_1, year_param_2 = 2016, 2019\n",
    "year_array = list(range(year_param_1, year_param_2+1))\n",
    "print('Year filter array parameters:', year_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_dates = portfolio_df.select('*',\n",
    "                                      year(\"operation_date\").alias('year'), \n",
    "                                      month(\"operation_date\").alias('month'), \n",
    "                                      dayofmonth(\"operation_date\").alias('day'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring dataframe portfolio funds data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tickers_agg = [spark_sum(when(col(x).isNotNull(), 1).otherwise(0)).alias('count_' + str(x)) for x in portfolio_dates.columns[1:-3]]\n",
    "portfolio_dates_agg = portfolio_dates.groupBy('year')\\\n",
    "                                     .agg(*count_tickers_agg)\\\n",
    "                                     .orderBy('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_dates_agg.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_by_year = [spark_sum(when(col(x) > 0, 1).otherwise(0)).alias('ticker_' + str(x[6:])) for x in portfolio_dates_agg.columns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_year_count = portfolio_dates_agg.where(col('year').isin(*year_array)).select(*count_by_year)\n",
    "portfolio_year_count.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_and_values = create_map(list(chain.from_iterable([[lit(c), col(c)] for c in portfolio_year_count.columns[:-1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_T = portfolio_year_count.select(*['ticker_operation_date'], explode(field_and_values))\\\n",
    "                                  .withColumnRenamed('key', 'ticker_fund')\\\n",
    "                                  .withColumnRenamed('value', 'total_years_price')\\\n",
    "                                  .drop('ticker_operation_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_T.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_T.groupBy('total_years_price').count().orderBy('total_years_price').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
