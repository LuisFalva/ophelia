{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import reduce\n",
    "from itertools import chain\n",
    "from scipy.stats import stats\n",
    "from scipy.stats import rankdata\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from pyspark.mllib.linalg.distributed import IndexedRowMatrix, IndexedRow\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import create_map, col, to_date, date_format, year, month, dayofmonth, when, lit, lag, array, explode, struct, udf, first\n",
    "from pyspark.sql.functions import sum as spark_sum, avg as spark_avg, count, stddev as spark_stddev\n",
    "from pyspark.sql.types import FloatType, StructField, StructType, DateType, IntegerType, ArrayType\n",
    "from pyspark.sql import SparkSession, Window, DataFrame\n",
    "from pylab import *\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 10000000)\n",
    "pd.set_option('display.max_rows', 10000000)\n",
    "pd.set_option('display.width', 10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Spark Session for pseudo-distributed computing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Portfolio_Optimization').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CSV daily price Funds file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_path_file = 'data.csv'\n",
    "portfolio_data = spark.read.format(\"csv\").options(header=\"true\").load(portfolio_path_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change impure schema portfolio input data.\n",
    "### Defining portfolio dataframe data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_portfolio = [date_format(\n",
    "    to_date(col(portfolio_data.columns[0]), 'dd/MM/yyyy'),\n",
    "    'yyyy-MM-dd').cast('date').alias('operation_date')] + [col(x).cast('float') for x in portfolio_data.columns[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering operation dates without nulls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_data_ns = portfolio_data.where(col(portfolio_data.columns[0]).isNotNull())\\\n",
    "                                        .select(schema_portfolio)\n",
    "\n",
    "portfolio_data_ns.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_field_mod1 = ['operation_date']\n",
    "writing_path_mod1 = '/data/core/fince/data/portfolioOptimization/price_wharehouse_transform/'\n",
    "print('\\nWriting parquets ...\\n')\n",
    "portfolio_data_ns.repartition(1).write.mode('overwrite').parquet(writing_path_mod1, partitionBy=partition_field_mod1)\n",
    "\n",
    "%time\n",
    "print('\\nSUCCESS \\nPARQUET DATA SAVED!')\n",
    "print('\\nNew root path table data:', writing_path_mod1+'operation_date=yyy-MM-dd', '\\nparquet chunks portitioned by:', partition_field_mod1)\n",
    "\n",
    "portfolio_path_parquet = '/data/core/fince/data/portfolioOptimization/price_wharehouse_transform/'\n",
    "portfolio_df = spark.read.parquet(portfolio_path_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Year parameters input array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_param_1, year_param_2 = 2016, 2019\n",
    "year_array = list(range(year_param_1, year_param_2+1))\n",
    "print('Year filter array parameters:', year_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_dates = portfolio_df.select('*',\n",
    "                                      year(\"operation_date\").alias('year'), \n",
    "                                      month(\"operation_date\").alias('month'), \n",
    "                                      dayofmonth(\"operation_date\").alias('day'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data, analytic base table structuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates_index(dates_list):\n",
    "    \"\"\"\n",
    "    Dates parser function, transform a list of dates in a dictionary\n",
    "    :param dates_list: list with date values\n",
    "    :return: parser udf for sequence of dates\n",
    "    \"\"\"\n",
    "    if not isinstance(dates_list, list):\n",
    "        raise PythagorasUtilsException('Invalid param')\n",
    "\n",
    "    if len(dates_list) <= 0:\n",
    "        raise PythagorasUtilsException('Empty param')\n",
    "\n",
    "    dates_dict = {date: index for index, date in enumerate(dates_list)}\n",
    "    result = udf(lambda x: dates_dict[x], IntegerType())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_dates_list = sorted([x.operation_date for x in portfolio_dates.select('operation_date').distinct().collect()])\n",
    "print(\"unique dates list:\",len(operation_dates_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_index_udf = dates_index(operation_dates_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugging_portfolio = portfolio_dates.where(col('year').isin(year_array)).select('*', (date_index_udf(col('operation_date'))).alias('date_id'))\n",
    "debugging_portfolio.orderBy(col('operation_date')).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_cols = debugging_portfolio.columns[1:-5]\n",
    "count_by_col = [spark_sum(col(x)).alias(str(x)) for x in long_cols]\n",
    "aggregate_columns = debugging_portfolio.select(*count_by_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing none type data:\n",
    "null_counts = aggregate_columns.select([count(when(col(c).isNull(), c)).alias(c) for c in aggregate_columns.columns]).collect()[0].asDict()\n",
    "drop_cols = [k for k, v in null_counts.items() if v > 0]\n",
    "removed_errors = debugging_portfolio.drop(*drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing NaN & fit vectors with no more than 10 NaN's (days):\n",
    "missing_counter = removed_errors.select([count(when(col(c).isNull(), c)).alias(c) for c in removed_errors.columns]).collect()[0].asDict()\n",
    "drop_rude_missing = [k for k, v in missing_counter.items() if v > 10]\n",
    "remove_rude_missing = removed_errors.drop(*drop_rude_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_fields = remove_rude_missing.agg(*(spark_avg(c).alias(c) for c in remove_rude_missing.columns if c not in ['operation_date']))\n",
    "purifying_portfolio = remove_rude_missing.na.fill(numerical_fields.first().asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.orderBy(\"operation_date\")\n",
    "yield_cols = purifying_portfolio.columns[:-5]\n",
    "yield_portfolio = (reduce(lambda r_df, col_name: r_df.withColumn(col_name, (lag(r_df[col_name]).over(w) / r_df[col_name])-1), yield_cols, purifying_portfolio))\\\n",
    "                                                     .where(col(yield_cols[0]).isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Portfolio's Yield dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_field_mod2 = ['operation_date']\n",
    "writing_path_mod2 = '/data/core/fince/data/portfolioOptimization/portfolio_yield_window/'\n",
    "\n",
    "print('\\nWriting parquets ...')\n",
    "yield_portfolio.repartition(5).write.mode('overwrite').parquet(writing_path_mod2, partitionBy=partition_field_mod2)\n",
    "\n",
    "%time\n",
    "print('\\nSUCCESS \\nPARQUET DATA SAVED!')\n",
    "print('\\nNew root path tabla data:', writing_path_mod2 + 'operation_date=yyy-MM-dd', '\\nparquet chunks portitioned by:', partition_field_mod2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading persisted Portfolio Yields dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_yield_window_path = '/data/core/fince/data/portfolioOptimization/portfolio_yield_window/'\n",
    "portfolio_yield_df = spark.read.parquet(portfolio_yield_window_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_yield_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [portfolio_yield_df.select(lit(fund).alias('fund_name'), col(fund).alias('fund_yield')) for fund in portfolio_yield_df.columns[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll_df(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_yield_T = unionAll_df(*dataframes).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Portfolio's Yield Transpose dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writing_path_mod3 = '/data/core/fince/data/portfolioOptimization/portfolio_yield_transpose/'\n",
    "\n",
    "print('\\nWriting parquets ...')\n",
    "portfolio_yield_T.repartition(1).write.mode('overwrite').parquet(writing_path_mod3)\n",
    "\n",
    "%time\n",
    "print('\\nSUCCESS \\nPARQUET DATA SAVED!')\n",
    "print('\\nNew root path tabla data:', writing_path_mod3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading persisted Portfolio Yields Transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_yield_T_path = '/data/core/fince/data/portfolioOptimization/portfolio_yield_transpose/'\n",
    "portfolio_yield_T_df = spark.read.parquet(portfolio_yield_T_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_yield_T_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe_ratio_df = portfolio_yield_T_df.groupBy(\"fund_name\")\\\n",
    "                                      .agg(spark_avg('fund_yield'), spark_stddev('fund_yield'))\\\n",
    "                                      .select(\"*\", (col(\"avg(fund_yield)\") / col(\"stddev_samp(fund_yield)\")).alias(\"sharpe_ratio\"))\\\n",
    "                                      .orderBy(col(\"sharpe_ratio\").desc())\\\n",
    "                                      .drop(\"avg(fund_yield)\", \"stddev_samp(fund_yield)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharpe ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe_ratio_df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_array = portfolio_yield_df.columns[:-1]\n",
    "monthly_return = np.array(portfolio_yield_df.select(*field_array).collect())\n",
    "print('test with', len(field_array), 'funds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('monthly_return matrix:\\n', monthly_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Value Decomposition analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_return_rdd = sc.parallelize(monthly_return.tolist()).zipWithIndex()\n",
    "\n",
    "# Obtaining model parameters:\n",
    "n = monthly_return_rdd.count()\n",
    "p = len(monthly_return_rdd.take(1)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_dense_vector = udf(lambda x: Vectors.dense(x), VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_return_df = spark.createDataFrame(monthly_return_rdd).toDF('features', 'id')\n",
    "monthly_return_df.select('features').collect()\n",
    "#monthly_return_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_return_df = monthly_return_df.withColumn(\"features\", udf_dense_vector(\"features\"))\n",
    "monthly_return_df.select('features').collect()#.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdScaler = StandardScaler(withMean=True, withStd=True, inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "model = stdScaler.fit(monthly_return_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_return_std_df = model.transform(monthly_return_df).drop(\"features\").withColumnRenamed(\"scaled_features\",\"features\")\n",
    "monthly_return_std_df.select('features').collect()#.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index Row Matrix\n",
    "monthly_return_irm = IndexedRowMatrix(monthly_return_std_df.rdd.map(lambda x: IndexedRow(x[0], x[1].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD = monthly_return_irm.computeSVD(p, True)\n",
    "U = SVD.U\n",
    "S = SVD.s.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_vals = S**2/(n-1)\n",
    "eigvals = np.flipud(np.sort(eigen_vals))\n",
    "cumsum = eigvals.cumsum()\n",
    "total_variance_explained = cumsum/eigvals.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nvector of eigenvalues:\\n', eigen_vals)\n",
    "print('\\nvector of eigvals:\\n', eigvals)\n",
    "print('\\nvector of cumsum:\\n', cumsum)\n",
    "print('\\nvector of total_variance_explained:\\n', total_variance_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.argmax(total_variance_explained>0.95)+1\n",
    "V = SVD.V\n",
    "U = U.rows.map(lambda x: (x.index, x.vector[0:K]*S[0:K]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "princ_comps = np.array(list(map(lambda x:x[1], sorted(U.collect(), key = lambda x:x[0]))))\n",
    "print(princ_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(princ_comps, princ_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setosa = princ_comps[iris_target==0]\n",
    "#versicolor = princ_comps[iris_target==1]\n",
    "#verginica = princ_comps[iris_target==2]\n",
    "#plt.scatter(princ_comps, princ_comps)\n",
    "#plt.scatter(princ_comps, princ_comps, c=\"b\",label=\"hola\")\n",
    "#plt.scatter(versicolor[:,0], versicolor[:,1], c=\"g\",label=\"versicolor\")\n",
    "#plt.scatter(verginica[:,0], verginica[:,1], c=\"r\",label=\"verginica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = monthly_return.shape[0]\n",
    "N = monthly_return.shape[1]\n",
    "T1 = 11\n",
    "start_month = T1 + 1\n",
    "T2 = size - start_month\n",
    "end_month = size\n",
    "covmatr = np.zeros((N, N))\n",
    "w_RP = np.zeros((T2, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = monthly_return.T\n",
    "w_EW = np.zeros((T2, N))\n",
    "onen = np.full((1, N), 1/N)\n",
    "r_ew  = np .zeros((T2, N))\n",
    "r_rp = np.zeros((T2, 1))\n",
    "retEW = np.zeros((T2, 1))\n",
    "retRP = np.zeros((T2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating optimization parameters...\\n')\n",
    "for y in range(start_month, end_month):\n",
    "    w_EW[:] = onen\n",
    "    r_ew[y - start_month] = np.dot(monthly_return[y,:] , 1/N)\n",
    "    retEW[y - start_month] = sum(r_ew[y-start_month])\n",
    "%time\n",
    "print('\\nDONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating Marginal Risk Contribution variables...\\n')\n",
    "for w in range(start_month, end_month):\n",
    "    covmatr = np.cov(ret[:,w-T1:w])    \n",
    "%time\n",
    "print('\\nDONE!')\n",
    "print('\\nvariance & covariance matrix:')\n",
    "print(covmatr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator function for Risk Contribution variables\n",
    "- mrc aka: marginal risk contribution\n",
    "- rc aka: risk contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RC(weight, covmatr):\n",
    "    weight = np.array(weight)\n",
    "    variance = weight.T @ covmatr @ weight\n",
    "    sigma = variance ** .5\n",
    "    mrc = 1/sigma * (covmatr @ weight)\n",
    "    rc = weight * mrc\n",
    "    rc = rc/rc.sum()\n",
    "    return rc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator function for RiskParity objective variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RiskParity_objective(x):\n",
    "    variance = x.T @ covmatr @ x\n",
    "    sigma = variance ** .5\n",
    "    mrc = 1/sigma * (covmatr @ x)\n",
    "    rc = x * mrc\n",
    "    a = np.reshape(rc, (len(rc),1))\n",
    "    risk_diffs = a - a.T\n",
    "    sum_risk_diffs_squared = np.sum(np.square(np.ravel(risk_diffs)))\n",
    "    return sum_risk_diffs_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_sum_constraint(x):\n",
    "    return np.sum(x) - 1.0\n",
    "        \n",
    "def weight_longonly(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function object for instance on Minimization scipy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RiskParity(covmatr):\n",
    "    x0 = np.repeat(1/covmatr.shape[1], covmatr.shape[1])\n",
    "    constraints = ({'type': 'eq', 'fun': weight_sum_constraint},\n",
    "                   {'type': 'ineq', 'fun' : weight_longonly})\n",
    "    options = {'ftol' : 1e-20, 'maxiter': 999}\n",
    "    result = minimize(fun = RiskParity_objective,\n",
    "                      x0 = x0,\n",
    "                      constraints = constraints,\n",
    "                      options = options)\n",
    "    return result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating optimized return matrices...')\n",
    "for w in range(start_month, end_month):\n",
    "    w_RP[w - start_month] = RiskParity(covmatr)\n",
    "    r_rp[w - start_month] = np.dot(monthly_return[w,:], w_RP[w - start_month,:])\n",
    "    retRP[w - start_month] = sum(r_rp[w - start_month])\n",
    "%time\n",
    "print('\\nDONE!')\n",
    "print('\\nw_RP matrix:')\n",
    "print(w_RP)\n",
    "print('\\nretRP matrix:')\n",
    "print(retRP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El eje x será el mes, el eje y será el activo (fondo), y z será el peso activo del portafolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = np.amax(w_RP)\n",
    "mn = np.amin(w_RP)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection = '3d')\n",
    "\n",
    "X = np.arange (0, T2, 1)\n",
    "Y = np.arange( 0, N, 1)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = np.transpose(w_RP)\n",
    "\n",
    "surf = ax.plot_surface(X, Y, Z, cmap = cm.Reds_r, linewidth = 0)\n",
    "\n",
    "ax.set_zlim(mn-.02, mx+.05)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
