{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import reduce\n",
    "from itertools import chain\n",
    "from scipy.stats import stats\n",
    "from scipy.stats import rankdata\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from pyspark.mllib.linalg.distributed import IndexedRowMatrix, IndexedRow\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import create_map, col, to_date, date_format, year, month, dayofmonth, when, lit, lag, array, explode, struct, udf, first\n",
    "from pyspark.sql.functions import sum as spark_sum, avg as spark_avg, count, stddev as spark_stddev\n",
    "from pyspark.sql.types import FloatType, StructField, StructType, DateType, IntegerType, ArrayType\n",
    "from pyspark.sql import SparkSession, Window, DataFrame\n",
    "from pylab import *\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 10000000)\n",
    "pd.set_option('display.max_rows', 10000000)\n",
    "pd.set_option('display.width', 10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Spark Session for pseudo-distributed computing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://127.0.0.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Portfolio_Optimization</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Portfolio_Optimization>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Portfolio_Optimization').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CSV daily price Funds file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_path_file = 'data.csv'\n",
    "portfolio_data = spark.read.format(\"csv\").options(header=\"true\").load(portfolio_path_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change impure schema portfolio input data.\n",
    "### Defining portfolio dataframe data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_portfolio = [date_format(\n",
    "    to_date(col(portfolio_data.columns[0]), 'dd/MM/yyyy'),\n",
    "    'yyyy-MM-dd').cast('date').alias('operation_date')] + [col(x).cast('float') for x in portfolio_data.columns[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering operation dates without nulls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_data_ns = portfolio_data.where(col(portfolio_data.columns[0]).isNotNull())\\\n",
    "                                        .select(schema_portfolio)\n",
    "\n",
    "portfolio_data_ns.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_field_mod1 = ['operation_date']\n",
    "writing_path_mod1 = '/data/core/fince/data/portfolioOptimization/price_wharehouse_transform/'\n",
    "print('\\nWriting parquets ...\\n')\n",
    "portfolio_data_ns.repartition(1).write.mode('overwrite').parquet(writing_path_mod1, partitionBy=partition_field_mod1)\n",
    "\n",
    "%time\n",
    "print('\\nSUCCESS \\nPARQUET DATA SAVED!')\n",
    "print('\\nNew root path table data:', writing_path_mod1+'operation_date=yyy-MM-dd', '\\nparquet chunks portitioned by:', partition_field_mod1)\n",
    "\n",
    "portfolio_path_parquet = '/data/core/fince/data/portfolioOptimization/price_wharehouse_transform/'\n",
    "portfolio_df = spark.read.parquet(portfolio_path_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Year parameters input array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_param_1, year_param_2 = 2016, 2019\n",
    "year_array = list(range(year_param_1, year_param_2+1))\n",
    "print('Year filter array parameters:', year_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_dates = portfolio_df.select('*',\n",
    "                                      year(\"operation_date\").alias('year'), \n",
    "                                      month(\"operation_date\").alias('month'), \n",
    "                                      dayofmonth(\"operation_date\").alias('day'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data, analytic base table structuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates_index(dates_list):\n",
    "    \"\"\"\n",
    "    Dates parser function, transform a list of dates in a dictionary\n",
    "    :param dates_list: list with date values\n",
    "    :return: parser udf for sequence of dates\n",
    "    \"\"\"\n",
    "    if not isinstance(dates_list, list):\n",
    "        raise PythagorasUtilsException('Invalid param')\n",
    "\n",
    "    if len(dates_list) <= 0:\n",
    "        raise PythagorasUtilsException('Empty param')\n",
    "\n",
    "    dates_dict = {date: index for index, date in enumerate(dates_list)}\n",
    "    result = udf(lambda x: dates_dict[x], IntegerType())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_dates_list = sorted([x.operation_date for x in portfolio_dates.select('operation_date').distinct().collect()])\n",
    "print(\"unique dates list:\",len(operation_dates_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_index_udf = dates_index(operation_dates_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugging_portfolio = portfolio_dates.where(col('year').isin(year_array)).select('*', (date_index_udf(col('operation_date'))).alias('date_id'))\n",
    "debugging_portfolio.orderBy(col('operation_date')).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_cols = debugging_portfolio.columns[1:-5]\n",
    "count_by_col = [spark_sum(col(x)).alias(str(x)) for x in long_cols]\n",
    "aggregate_columns = debugging_portfolio.select(*count_by_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing none type data:\n",
    "null_counts = aggregate_columns.select([count(when(col(c).isNull(), c)).alias(c) for c in aggregate_columns.columns]).collect()[0].asDict()\n",
    "drop_cols = [k for k, v in null_counts.items() if v > 0]\n",
    "removed_errors = debugging_portfolio.drop(*drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing NaN & fit vectors with no more than 10 NaN's (days):\n",
    "missing_counter = removed_errors.select([count(when(col(c).isNull(), c)).alias(c) for c in removed_errors.columns]).collect()[0].asDict()\n",
    "drop_rude_missing = [k for k, v in missing_counter.items() if v > 10]\n",
    "remove_rude_missing = removed_errors.drop(*drop_rude_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_fields = remove_rude_missing.agg(*(spark_avg(c).alias(c) for c in remove_rude_missing.columns if c not in ['operation_date']))\n",
    "purifying_portfolio = remove_rude_missing.na.fill(numerical_fields.first().asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.orderBy(\"operation_date\")\n",
    "yield_cols = purifying_portfolio.columns[:-5]\n",
    "yield_portfolio = (reduce(lambda r_df, col_name: r_df.withColumn(col_name, (lag(r_df[col_name]).over(w) / r_df[col_name])-1), yield_cols, purifying_portfolio))\\\n",
    "                                                     .where(col(yield_cols[0]).isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Portfolio's Yield dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_field_mod2 = ['operation_date']\n",
    "writing_path_mod2 = '/data/core/fince/data/portfolioOptimization/portfolio_yield_window/'\n",
    "\n",
    "print('\\nWriting parquets ...')\n",
    "yield_portfolio.repartition(5).write.mode('overwrite').parquet(writing_path_mod2, partitionBy=partition_field_mod2)\n",
    "\n",
    "%time\n",
    "print('\\nSUCCESS \\nPARQUET DATA SAVED!')\n",
    "print('\\nNew root path tabla data:', writing_path_mod2 + 'operation_date=yyy-MM-dd', '\\nparquet chunks portitioned by:', partition_field_mod2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading persisted Portfolio Yields dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_yield_window_path = '/data/core/fince/data/portfolioOptimization/portfolio_yield_window/'\n",
    "portfolio_yield_df = spark.read.parquet(portfolio_yield_window_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_yield_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [portfolio_yield_df.select(lit(fund).alias('fund_name'), col(fund).alias('fund_yield')) for fund in portfolio_yield_df.columns[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll_df(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_yield_T = unionAll_df(*dataframes).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Portfolio's Yield Transpose dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writing_path_mod3 = '/data/core/fince/data/portfolioOptimization/portfolio_yield_transpose/'\n",
    "\n",
    "print('\\nWriting parquets ...')\n",
    "portfolio_yield_T.repartition(1).write.mode('overwrite').parquet(writing_path_mod3)\n",
    "\n",
    "%time\n",
    "print('\\nSUCCESS \\nPARQUET DATA SAVED!')\n",
    "print('\\nNew root path tabla data:', writing_path_mod3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading persisted Portfolio Yields Transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_yield_T_path = '/data/core/fince/data/portfolioOptimization/portfolio_yield_transpose/'\n",
    "portfolio_yield_T_df = spark.read.parquet(portfolio_yield_T_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|fund_name|          fund_yield|\n",
      "+---------+--------------------+\n",
      "|  SCOTIAG|-4.03531517030653...|\n",
      "|  SCOTIAG|-1.08301687504974...|\n",
      "|  SCOTIAG|-1.03740924003892...|\n",
      "|  SCOTIAG|-1.90468599718274...|\n",
      "|  SCOTIAG|-1.93812434923335E-4|\n",
      "|  SCOTIAG|-1.99102454164434...|\n",
      "|  SCOTIAG|-1.94726544552992...|\n",
      "|  SCOTIAG|-2.04587845092696...|\n",
      "|  SCOTIAG|-1.52785173599068...|\n",
      "|  SCOTIAG|-1.99152936178115...|\n",
      "|  SCOTIAG|-1.60985792116763...|\n",
      "|  SCOTIAG|-1.68461374847495...|\n",
      "|  SCOTIAG|-1.88301221698328...|\n",
      "|  SCOTIAG|-1.93291433550069...|\n",
      "|  SCOTIAG|-2.06810648491706...|\n",
      "|  SCOTIAG|-2.15583108282402...|\n",
      "|  SCOTIAG|-1.61182011837457...|\n",
      "|  SCOTIAG|-2.96001069655105...|\n",
      "|  SCOTIAG|-1.16392617629856...|\n",
      "|  SCOTIAG|-1.87465484195237...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "portfolio_yield_T_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe_ratio_df = portfolio_yield_T_df.groupBy(\"fund_name\")\\\n",
    "                                      .agg(spark_avg('fund_yield'), spark_stddev('fund_yield'))\\\n",
    "                                      .select(\"*\", (col(\"avg(fund_yield)\") / col(\"stddev_samp(fund_yield)\")).alias(\"sharpe_ratio\"))\\\n",
    "                                      .orderBy(col(\"sharpe_ratio\").desc())\\\n",
    "                                      .drop(\"avg(fund_yield)\", \"stddev_samp(fund_yield)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharpe ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "| fund_name|        sharpe_ratio|\n",
      "+----------+--------------------+\n",
      "|    GBMMOD| 0.03137959880780523|\n",
      "|    GBMCRE| 0.03124715775689063|\n",
      "|   NAFINDX| 0.01844368305409419|\n",
      "|   HSBCBOL|0.015666999130266483|\n",
      "|   INVEXMX|0.013004038169870356|\n",
      "|   ACTIPAT|0.012462306905213617|\n",
      "|   ACTIVAR|0.011726284859371962|\n",
      "|   ACTINMO|0.010918150288106438|\n",
      "|   BMERIND|0.010006322375368616|\n",
      "|    VECTPA|0.009086193748315314|\n",
      "|    BLKIPC|0.008840245406882136|\n",
      "|    BLKPAT|0.008052367375532735|\n",
      "|    SURIPC|0.008036704146507328|\n",
      "|   SCOTIPC| 0.00799137535845908|\n",
      "|   VECTIND|0.007295953729364737|\n",
      "|    SURPAT|0.006253600896206242|\n",
      "|   PRINRVA|0.003672604815147...|\n",
      "|    NTESEL|6.952067806518164E-4|\n",
      "|      MAYA|1.502042229259964E-4|\n",
      "|STERDOW270|-0.00157575205813...|\n",
      "|STERDOW281|-0.00157575205813...|\n",
      "|SVIVE20338|-0.00188773265509...|\n",
      "|SVIVE20390|-0.00188773265509...|\n",
      "|   ST&ERUS|-0.00229508790117...|\n",
      "|HSBCDOL201|-0.00551912186143...|\n",
      "|HSBCDOL189|-0.00551912186143...|\n",
      "|SVIVE35383|-0.00615665465199...|\n",
      "|SVIVE35322|-0.00615665465199...|\n",
      "|   AXESEDM|-0.00988724242649921|\n",
      "|   BBVAE50|-0.01030172543023...|\n",
      "|INVEXCO185|-0.01131602104207...|\n",
      "|INVEXCO197|-0.01131602104207...|\n",
      "|   BLKFIBR|-0.01162094654594...|\n",
      "|VECTCOB188|-0.01229635133961992|\n",
      "|VECTCOB200|-0.01229635133961992|\n",
      "|BBVADOL183|-0.01248375765367...|\n",
      "|BBVADOL195|-0.01248375765367...|\n",
      "|   SBANKDL|-0.01332244074956...|\n",
      "|ELITE/M342|-0.01504913634308...|\n",
      "|ELITE/M326|-0.01504913634308...|\n",
      "|   SCOTDOL|-0.01675271151327208|\n",
      "|SVIVE50310|-0.01731546052518...|\n",
      "|SVIVE50376|-0.01731546052518...|\n",
      "|   ACTICOB|-0.01808833539775036|\n",
      "| NTEUSA280|-0.02095545013232...|\n",
      "| NTEUSA269|-0.02095545013232...|\n",
      "|   INVEXTK|-0.02097477319067...|\n",
      "|   BLKDOLS|-0.02138736542561...|\n",
      "|    SURUSD|-0.02371342026382...|\n",
      "|   SCOTEUR|-0.02748665600840...|\n",
      "|   VALUEF2|-0.02854064687105149|\n",
      "|   TEMGBIA|-0.02863803738835007|\n",
      "|   ELITE/C|-0.02908279227337...|\n",
      "|   AXESGLO|-0.02909683129877...|\n",
      "|    NTEGLA|-0.02924417060955...|\n",
      "|GBMPMOD314|-0.02975361054762...|\n",
      "|GBMPMOD344|-0.02975361054762...|\n",
      "|   STEREAL| -0.0302702562429538|\n",
      "|    GBMTRV|-0.03059786241102...|\n",
      "|   SURGLOB|-0.03164432859530238|\n",
      "|FRANUSA278|-0.03275806107110105|\n",
      "|FRANUSA267|-0.03275806107110105|\n",
      "|    PRGLOB|-0.03590093526395236|\n",
      "|     SUPER|-0.03738202190602461|\n",
      "| FONSER161|-0.03740914308265436|\n",
      "| FONSER130|-0.03740914308265436|\n",
      "|    HSBCF4|-0.03757850251905...|\n",
      "|   SCOT-FX|-0.03778943404420...|\n",
      "|   SVIVE60|-0.03797129483988079|\n",
      "|VECTSIC262|-0.03813458415099966|\n",
      "|VECTSIC282|-0.03813458415099966|\n",
      "|   STERGOB|-0.03823550716158274|\n",
      "|   STER10P|-0.03847896681505...|\n",
      "|   ST&ER1P|-0.04224282625539819|\n",
      "|     PYMES|-0.04319587005681026|\n",
      "|   GOLD5MA|-0.04326301554426207|\n",
      "|   ST&ER1X|-0.04354085257827...|\n",
      "|   BLKINT1|-0.04566771884098...|\n",
      "|BLKUSEQ276|-0.04655418304582...|\n",
      "|BLKUSEQ265|-0.04655418304582...|\n",
      "|DIVER/A340|-0.04719736655663532|\n",
      "|DIVER/A324|-0.04719736655663532|\n",
      "| SURAGR353|-0.04767478605529...|\n",
      "| SURAGR337|-0.04767478605529...|\n",
      "|FRANOPR266|-0.04805952796550379|\n",
      "|FRANOPR277|-0.04805952796550379|\n",
      "|   GBMPICT|-0.05066621946040704|\n",
      "|   NAVIGTR|-0.05067898189910263|\n",
      "|   HSBC-80|-0.05140826030564...|\n",
      "|   SCOTUSA|-0.05202184213492604|\n",
      "|ACTI500272|-0.05322497043094075|\n",
      "|ACTI500261|-0.05322497043094075|\n",
      "|   HSBC-70|-0.05535501820367...|\n",
      "|PRINLS3331|-0.05611022047638752|\n",
      "|PRINLS3358|-0.05611022047638752|\n",
      "|   VALUEF7|-0.05861102340303083|\n",
      "|GOLD4MA345|-0.06126975457081919|\n",
      "|GOLD4MA329|-0.06126975457081919|\n",
      "|   SCOTQNT|-0.06231807188832087|\n",
      "| HSBCF3347|-0.06462865763661345|\n",
      "+----------+--------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sharpe_ratio_df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_array = portfolio_yield_df.columns[:-1]\n",
    "monthly_return = np.array(portfolio_yield_df.select(*field_array).collect())\n",
    "print('test with', len(field_array), 'funds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('monthly_return matrix:\\n', monthly_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Value Decomposition analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_return_rdd = sc.parallelize(monthly_return.tolist()).zipWithIndex()\n",
    "\n",
    "# Obtaining model parameters:\n",
    "n = monthly_return_rdd.count()\n",
    "p = len(monthly_return_rdd.take(1)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_dense_vector = udf(lambda x: Vectors.dense(x), VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_return_df = spark.createDataFrame(monthly_return_rdd).toDF('features', 'id')\n",
    "monthly_return_df.select('features').collect()\n",
    "#monthly_return_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_return_df = monthly_return_df.withColumn(\"features\", udf_dense_vector(\"features\"))\n",
    "monthly_return_df.select('features').collect()#.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdScaler = StandardScaler(withMean=True, withStd=True, inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "model = stdScaler.fit(monthly_return_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_return_std_df = model.transform(monthly_return_df).drop(\"features\").withColumnRenamed(\"scaled_features\",\"features\")\n",
    "monthly_return_std_df.select('features').collect()#.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index Row Matrix\n",
    "monthly_return_irm = IndexedRowMatrix(monthly_return_std_df.rdd.map(lambda x: IndexedRow(x[0], x[1].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD = monthly_return_irm.computeSVD(p, True)\n",
    "U = SVD.U\n",
    "S = SVD.s.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_vals = S**2/(n-1)\n",
    "eigvals = np.flipud(np.sort(eigen_vals))\n",
    "cumsum = eigvals.cumsum()\n",
    "total_variance_explained = cumsum/eigvals.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nvector of eigenvalues:\\n', eigen_vals)\n",
    "print('\\nvector of eigvals:\\n', eigvals)\n",
    "print('\\nvector of cumsum:\\n', cumsum)\n",
    "print('\\nvector of total_variance_explained:\\n', total_variance_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.argmax(total_variance_explained>0.95)+1\n",
    "V = SVD.V\n",
    "U = U.rows.map(lambda x: (x.index, x.vector[0:K]*S[0:K]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "princ_comps = np.array(list(map(lambda x:x[1], sorted(U.collect(), key = lambda x:x[0]))))\n",
    "print(princ_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(princ_comps, princ_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setosa = princ_comps[iris_target==0]\n",
    "#versicolor = princ_comps[iris_target==1]\n",
    "#verginica = princ_comps[iris_target==2]\n",
    "#plt.scatter(princ_comps, princ_comps)\n",
    "#plt.scatter(princ_comps, princ_comps, c=\"b\",label=\"hola\")\n",
    "#plt.scatter(versicolor[:,0], versicolor[:,1], c=\"g\",label=\"versicolor\")\n",
    "#plt.scatter(verginica[:,0], verginica[:,1], c=\"r\",label=\"verginica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = monthly_return.shape[0]\n",
    "N = monthly_return.shape[1]\n",
    "T1 = 11\n",
    "start_month = T1 + 1\n",
    "T2 = size - start_month\n",
    "end_month = size\n",
    "covmatr = np.zeros((N, N))\n",
    "w_RP = np.zeros((T2, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = monthly_return.T\n",
    "w_EW = np.zeros((T2, N))\n",
    "onen = np.full((1, N), 1/N)\n",
    "r_ew  = np .zeros((T2, N))\n",
    "r_rp = np.zeros((T2, 1))\n",
    "retEW = np.zeros((T2, 1))\n",
    "retRP = np.zeros((T2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating optimization parameters...\\n')\n",
    "for y in range(start_month, end_month):\n",
    "    w_EW[:] = onen\n",
    "    r_ew[y - start_month] = np.dot(monthly_return[y,:] , 1/N)\n",
    "    retEW[y - start_month] = sum(r_ew[y-start_month])\n",
    "%time\n",
    "print('\\nDONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating Marginal Risk Contribution variables...\\n')\n",
    "for w in range(start_month, end_month):\n",
    "    covmatr = np.cov(ret[:,w-T1:w])    \n",
    "%time\n",
    "print('\\nDONE!')\n",
    "print('\\nvariance & covariance matrix:')\n",
    "print(covmatr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator function for Risk Contribution variables\n",
    "- mrc aka: marginal risk contribution\n",
    "- rc aka: risk contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RC(weight, covmatr):\n",
    "    weight = np.array(weight)\n",
    "    variance = weight.T @ covmatr @ weight\n",
    "    sigma = variance ** .5\n",
    "    mrc = 1/sigma * (covmatr @ weight)\n",
    "    rc = weight * mrc\n",
    "    rc = rc/rc.sum()\n",
    "    return rc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator function for RiskParity objective variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RiskParity_objective(x):\n",
    "    variance = x.T @ covmatr @ x\n",
    "    sigma = variance ** .5\n",
    "    mrc = 1/sigma * (covmatr @ x)\n",
    "    rc = x * mrc\n",
    "    a = np.reshape(rc, (len(rc),1))\n",
    "    risk_diffs = a - a.T\n",
    "    sum_risk_diffs_squared = np.sum(np.square(np.ravel(risk_diffs)))\n",
    "    return sum_risk_diffs_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_sum_constraint(x):\n",
    "    return np.sum(x) - 1.0\n",
    "        \n",
    "def weight_longonly(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function object for instance on Minimization scipy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RiskParity(covmatr):\n",
    "    x0 = np.repeat(1/covmatr.shape[1], covmatr.shape[1])\n",
    "    constraints = ({'type': 'eq', 'fun': weight_sum_constraint},\n",
    "                   {'type': 'ineq', 'fun' : weight_longonly})\n",
    "    options = {'ftol' : 1e-20, 'maxiter': 999}\n",
    "    result = minimize(fun = RiskParity_objective,\n",
    "                      x0 = x0,\n",
    "                      constraints = constraints,\n",
    "                      options = options)\n",
    "    return result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating optimized return matrices...')\n",
    "for w in range(start_month, end_month):\n",
    "    w_RP[w - start_month] = RiskParity(covmatr)\n",
    "    r_rp[w - start_month] = np.dot(monthly_return[w,:], w_RP[w - start_month,:])\n",
    "    retRP[w - start_month] = sum(r_rp[w - start_month])\n",
    "%time\n",
    "print('\\nDONE!')\n",
    "print('\\nw_RP matrix:')\n",
    "print(w_RP)\n",
    "print('\\nretRP matrix:')\n",
    "print(retRP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El eje x será el mes, el eje y será el activo (fondo), y z será el peso activo del portafolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = np.amax(w_RP)\n",
    "mn = np.amin(w_RP)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection = '3d')\n",
    "\n",
    "X = np.arange (0, T2, 1)\n",
    "Y = np.arange( 0, N, 1)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = np.transpose(w_RP)\n",
    "\n",
    "surf = ax.plot_surface(X, Y, Z, cmap = cm.Reds_r, linewidth = 0)\n",
    "\n",
    "ax.set_zlim(mn-.02, mx+.05)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
