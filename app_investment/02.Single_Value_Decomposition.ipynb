{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.mllib.linalg.distributed import IndexedRowMatrix, IndexedRow, RowMatrix\n",
    "from pyspark.ml.feature import StandardScaler, PCA\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 10000000)\n",
    "pd.set_option('display.max_rows', 10000000)\n",
    "pd.set_option('display.width', 10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Spark Session for pseudo-distributed computing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Single_Value_Decomposition_Portfolio').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading persisted Portfolio Yields dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_yield_window_path = '/data/core/fince/data/portfolioOptimization/portfolio_yield_window/'\n",
    "portfolio_yield_df = spark.read.parquet(portfolio_yield_window_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_array = portfolio_yield_df.columns[:-5]\n",
    "monthly_return = np.array(portfolio_yield_df.select(*field_array).collect())\n",
    "print('test with', len(field_array), 'funds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('monthly_return matrix:\\n', monthly_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El cálculo se realiza utilizando la descomposición de valores singulares (Singular Value Decomposition, SVD). La SVD de cualquier matriz $mxn$ se calcula como:\n",
    "\n",
    "$$A = U \\sum V^T$$\n",
    "\n",
    "### Donde $U$ es una matriz ortogonal $m×m$ cuyas columnas son los vectores propios (eigenvectores) de  $AA^T$ , $V$ es una matriz ortogonal $n×n$ cuyas columnas son los eigenvectores de  $A^T A$ , y $\\sum$ es una matriz diagonal $m×n$ y sus valores son cero excepto a lo largo de la diagonal.\n",
    "\n",
    "### Al aplicar PCA, tenemos que centrar nuestros datos, es decir, tenemos que restar la media de la columna. Luego, según la naturaleza de nuestros datos, es posible que necesitemos estandarizar nuestros datos (hacer que cada característica tenga una varianza unitaria y una media cero). Si las columnas están en diferentes escalas, como el año, la temperatura, la concentración de dióxido de carbono, por ejemplo, tenemos que estandarizar los datos. Si los datos están en la misma unidad, por otro lado, la estandarización puede provocar la pérdida de información importante. En el primer caso, cuando las columnas están en la misma unidad y en una escala similar, usamos la matriz de covarianza para SVD pero cuando las unidades son diferentes ya que estandarizamos los datos, usamos la matriz de correlación.\n",
    "\n",
    "### Los componentes principales (PC) son el producto matricial de los datos originales y la matriz $V$, que es igual al producto de las matrices $U$ y $\\sum$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Value Decomposition analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_return_rdd = sc.parallelize(monthly_return.tolist()).zipWithIndex()\n",
    "\n",
    "# Obtaining model parameters:\n",
    "n = monthly_return_rdd.count()\n",
    "p = len(monthly_return_rdd.take(1)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_dense_vector = udf(lambda x: Vectors.dense(x), VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_return_df = spark.createDataFrame(monthly_return_rdd).toDF('features', 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_return_df = monthly_return_df.withColumn(\"features\", udf_dense_vector(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdScaler = StandardScaler(withMean=True, withStd=True, inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "model = stdScaler.fit(monthly_return_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_return_std_df = model.transform(monthly_return_df).drop(\"features\")\n",
    "monthly_return_std_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 11\n",
    "\n",
    "pca = PCA(k=k, inputCol = stdScaler.getOutputCol(), outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(monthly_return_std_df)\n",
    "transformed_feature = model.transform(monthly_return_std_df)\n",
    "np.round(100.00*model.explainedVariance.toArray(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = np.round(100.00*model.pc.toArray(),4)\n",
    "df_pc = pd.DataFrame(pcs, columns = ['PC_'+str(i) for i in range(1, k+1)], index = field_array)\n",
    "df_pc.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index Row Matrix\n",
    "monthly_return_irm = IndexedRowMatrix(monthly_return_std_df.rdd.map(lambda x: IndexedRow(x[0], x[1].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD = monthly_return_irm.computeSVD(p, True)\n",
    "U = SVD.U\n",
    "S = SVD.s.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_vals = S**2/(n-1)\n",
    "eigvals = np.flipud(np.sort(eigen_vals))\n",
    "cumsum = eigvals.cumsum()\n",
    "total_variance_explained = cumsum/eigvals.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.argmax(total_variance_explained > 0.95)+1\n",
    "V = SVD.V\n",
    "U = U.rows.map(lambda x: (x.index, x.vector[0:K]*S[0:K]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "princ_comps = np.array(list(map(lambda x:x[1], sorted(U.collect(), key = lambda x:x[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "plt.scatter(princ_comps[:, 1], princ_comps[:, 0], alpha=0.7)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(total_variance_explained, eigvals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
