{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import date_format, to_date, col, year, month, dayofmonth, when, lag, udf, collect_list\n",
    "from pyspark.sql.functions import sum as spark_sum, count as spark_count, avg as spark_avg\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import SparkSession, Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 10000000)\n",
    "pd.set_option('display.max_rows', 10000000)\n",
    "pd.set_option('display.width', 10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Spark Session for pseudo-distributed computing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Transform_Data').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CSV daily price Funds file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_path_file = 'data-resources/data.csv'\n",
    "portfolio_data = spark.read.format(\"csv\").options(header=\"true\").load(portfolio_path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_d = \"data-resources/data_structure_sample.csv\"\n",
    "test_df = spark.read.format(\"csv\").options(header=\"true\").load(test_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change impure schema portfolio input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining portfolio dataframe data:\n",
    "schema_portfolio = [date_format(\n",
    "    to_date(col(portfolio_data.columns[0]), 'dd/MM/yyyy'),\n",
    "    'yyyy-MM-dd').cast('date').alias('operation_date')] + [col(x).cast('float') for x in portfolio_data.columns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering operation dates without nulls:\n",
    "portfolio_data_ns = portfolio_data.where(col(portfolio_data.columns[0]).isNotNull())\\\n",
    "                                  .select(schema_portfolio)\n",
    "\n",
    "portfolio_data_ns.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partition_field_mod1 = ['operation_date']\n",
    "#writing_path_mod1 = '/data/core/fince/data/portfolioOptimization/price_wharehouse_transform/'\n",
    "#print('\\nWriting parquets ...\\n')\n",
    "#portfolio_data_ns.repartition(1).write.mode('overwrite').parquet(writing_path_mod1, partitionBy=partition_field_mod1)\n",
    "\n",
    "#%time\n",
    "#print('\\nSUCCESS \\nPARQUET DATA SAVED!')\n",
    "#print('\\nNew root path table data:', writing_path_mod1+'operation_date=yyy-MM-dd', '\\nparquet chunks portitioned by:', partition_field_mod1)\n",
    "\n",
    "portfolio_path_parquet = '/data/core/fince/data/portfolioOptimization/price_wharehouse_transform/'\n",
    "portfolio_df = spark.read.parquet(portfolio_path_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Year parameters input array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_param_1, year_param_2 = 2016, 2019\n",
    "year_array = list(range(year_param_1, year_param_2+1))\n",
    "print('Year filter array parameters:', year_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_dates = portfolio_df.select('*',\n",
    "                                      year(\"operation_date\").alias('year'), \n",
    "                                      month(\"operation_date\").alias('month'), \n",
    "                                      dayofmonth(\"operation_date\").alias('day'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data, analytic base table structuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates_index(dates_list):\n",
    "    \"\"\"\n",
    "    Dates parser function, transform a list of dates in a dictionary\n",
    "    :param dates_list: list with date values\n",
    "    :return: parser udf for sequence of dates\n",
    "    \"\"\"\n",
    "    if not isinstance(dates_list, list):\n",
    "        raise PythagorasUtilsException('Invalid param')\n",
    "\n",
    "    if len(dates_list) <= 0:\n",
    "        raise PythagorasUtilsException('Empty param')\n",
    "\n",
    "    dates_dict = {date: index for index, date in enumerate(dates_list)}\n",
    "    result = udf(lambda x: dates_dict[x], IntegerType())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_dates_list = sorted([x.operation_date for x in portfolio_dates.select('operation_date').distinct().collect()])\n",
    "print(\"unique dates list:\",len(operation_dates_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_index_udf = dates_index(operation_dates_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugging_portfolio = portfolio_dates.where(col('year').isin(year_array)).select('*', (date_index_udf(col('operation_date'))).alias('date_id'))\n",
    "debugging_portfolio.orderBy(col('operation_date')).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_cols = debugging_portfolio.columns[:-5]\n",
    "count_by_col = [spark_sum(col(x)).alias(str(x)) for x in long_cols]\n",
    "aggregate_columns = debugging_portfolio.select(*count_by_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_columns.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing none type data:\n",
    "null_counts = aggregate_columns.select([spark_count(when(col(c).isNull(), c)).alias(c) for c in aggregate_columns.columns]).collect()[0].asDict()\n",
    "drop_cols = [k for k, v in null_counts.items() if v > 0]\n",
    "removed_errors = debugging_portfolio.drop(*drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_errors.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing NaN & fit vectors with no more than 10 NaN's (days):\n",
    "missing_counter = removed_errors.select([spark_count(when(col(c).isNull(), c)).alias(c) for c in removed_errors.columns]).collect()[0].asDict()\n",
    "drop_rude_missing = [k for k, v in missing_counter.items() if v >= 10]\n",
    "remove_rude_missing = removed_errors.drop(*drop_rude_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_rude_missing.orderBy(\"operation_date\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_fields = remove_rude_missing.agg(*(spark_avg(c).alias(c) for c in remove_rude_missing.columns if c not in ['operation_date']))\n",
    "purifying_portfolio = remove_rude_missing.na.fill(numerical_fields.first().asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.orderBy(\"operation_date\")\n",
    "yield_cols = purifying_portfolio.columns[:-5]\n",
    "yield_portfolio = (reduce(lambda r_df, col_name: r_df.withColumn(col_name, r_df[col_name] \n",
    "                                                              / (lag(r_df[col_name]).over(w))-1), yield_cols, purifying_portfolio))\\\n",
    "                                                     .where(col(yield_cols[0]).isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_portfolio.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_portfolio.orderBy(\"operation_date\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_portfolio_df = yield_portfolio.select(*yield_portfolio.columns[:-4])\n",
    "yield_portfolio_df.orderBy(\"operation_date\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_portfolio_df.describe(\"SCOTIAG\",\"AXESCP\",\"BMERGOB\",\"BMRGOB25\",\"VALUEF4\",\"BLKDIA7\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Portfolio's Yield dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_field_mod2 = ['operation_date']\n",
    "writing_path_mod2 = '/data/core/fince/data/portfolioOptimization/portfolio_yield_window/'\n",
    "\n",
    "print('\\nWriting parquets ...')\n",
    "yield_portfolio_df.coalesce(1).write.mode('overwrite').parquet(writing_path_mod2, partitionBy=partition_field_mod2)\n",
    "\n",
    "%time\n",
    "print('\\nSUCCESS \\nPARQUET DATA SAVED!')\n",
    "print('\\nNew root path tabla data:', writing_path_mod2 + 'operation_date=yyy-MM-dd', '\\nparquet chunks portitioned by:', partition_field_mod2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
