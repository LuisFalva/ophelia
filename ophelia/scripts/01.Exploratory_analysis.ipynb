{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import chain\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import date_format, to_date, col, year, month, dayofmonth, sum as spark_sum, when, create_map, lit, explode, udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 10000000)\n",
    "pd.set_option('display.max_rows', 10000000)\n",
    "pd.set_option('display.width', 10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Spark Session for pseudo-distributed computing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Exploratory_Analysis').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CSV daily price Funds file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_path_file = 'data-resources/data.csv'\n",
    "portfolio_data = spark.read.format(\"csv\").options(header=\"true\").load(portfolio_path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change impure schema portfolio input data.\n",
    "### Defining portfolio dataframe data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_portfolio = [date_format(\n",
    "    to_date(col(portfolio_data.columns[0]), 'dd/MM/yyyy'),\n",
    "    'yyyy-MM-dd').cast('date').alias('operation_date')] + [col(x).cast('float') for x in portfolio_data.columns[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering operation dates without nulls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_data_ns = portfolio_data.where(col(portfolio_data.columns[0]).isNotNull())\\\n",
    "                                  .select(schema_portfolio)\n",
    "\n",
    "portfolio_data_ns.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partition_field_mod1 = ['operation_date']\n",
    "#writing_path_mod1 = '/data/core/fince/data/portfolioOptimization/price_wharehouse_transform/'\n",
    "#print('\\nWriting parquets ...\\n')\n",
    "#portfolio_data_ns.repartition(1).write.mode('overwrite').parquet(writing_path_mod1, partitionBy=partition_field_mod1)\n",
    "\n",
    "#%time\n",
    "#print('\\nSUCCESS \\nPARQUET DATA SAVED!')\n",
    "#print('\\nNew root path table data:', writing_path_mod1+'operation_date=yyy-MM-dd', '\\nparquet chunks portitioned by:', partition_field_mod1)\n",
    "\n",
    "portfolio_path_parquet = '/data/core/fince/data/portfolioOptimization/price_wharehouse_transform/'\n",
    "portfolio_df = spark.read.parquet(portfolio_path_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year parameters input array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_param_1, year_param_2 = 2016, 2019\n",
    "year_array = list(range(year_param_1, year_param_2+1))\n",
    "print('Year filter array parameters:', year_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_dates = portfolio_df.select('*', year(\"operation_date\").alias('year'), \n",
    "                                          month(\"operation_date\").alias('month'), \n",
    "                                     dayofmonth(\"operation_date\").alias('day'))\\\n",
    "                              .orderBy(\"operation_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_dates.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring dataframe portfolio funds data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tickers_agg = [spark_sum(when(col(x).isNotNull(), 1).otherwise(0)).alias('count_' + str(x)) for x in portfolio_dates.columns[1:-3]]\n",
    "portfolio_dates_agg = portfolio_dates.groupBy('year')\\\n",
    "                                     .agg(*count_tickers_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_by_year = [spark_sum(when(col(x) > 0, 1).otherwise(0)).alias(str(x[6:])) for x in portfolio_dates_agg.columns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_year_count = portfolio_dates_agg.where(col('year').isin(*year_array)).select(*count_by_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_and_values = create_map(list(chain.from_iterable([[lit(c), col(c)] for c in portfolio_year_count.columns[:-1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_T = portfolio_year_count.select(*['operation_date'], explode(field_and_values))\\\n",
    "                                  .withColumnRenamed('key', 'ticker_fund')\\\n",
    "                                  .withColumnRenamed('value', 'total_years_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_T.groupBy('total_years_price').count().orderBy('total_years_price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_fields = portfolio_T.where(col(\"total_years_price\") >= 4).select(\"ticker_fund\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_set = historical_fields.collect()\n",
    "new_array = []\n",
    "for x in range(len(df_set)):\n",
    "    new_array.append(df_set[x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "X = portfolio_dates.select(new_array).toPandas()\n",
    "X.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "cor = X.corr()\n",
    "mask = np.tril(cor)\n",
    "sns.heatmap(cor, vmin=-1, vmax=1, mask=mask, cmap= 'coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_series(series, w=15, h=5):\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(w=w, h=h)\n",
    "    plt.plot(series)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"GBMGUBL\"\n",
    "spark_collection_1 = portfolio_dates.select(feature).where(col(feature).isNotNull()).collect()\n",
    "collection_1 = []\n",
    "for row in range(len(spark_collection_1)):\n",
    "    collection_1.append(spark_collection_1[row][0])\n",
    "    \n",
    "print(\"\\nseries:\", feature)\n",
    "plot_series(collection_1, 20, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logarithm_scale(real_number):\n",
    "    \"\"\"\n",
    "    Logarithm Scale method transforms to natural logarithm value.\n",
    "    :param real_number: float type input value\n",
    "    :return: FloatType value\n",
    "    \"\"\"\n",
    "    try:\n",
    "        log_number = math.log(float(real_number))\n",
    "    except Exception:\n",
    "        log_number = float(0)\n",
    "\n",
    "    return log_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logarithmic_scale_udf = udf(logarithm_scale, DoubleType())\n",
    "scaled_feature = feature_analysis.select(*[logarithmic_scale_udf(col(c)).alias(\"log_\"+c) for c in feature_analysis.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_feature = \"log_TASA\"\n",
    "spark_collection_2 = scaled_feature.select(log_feature).collect()\n",
    "collection_2 = []\n",
    "for row in range(len(spark_collection_2)):\n",
    "    collection_2.append(spark_collection_2[row][0])\n",
    "\n",
    "print(\"\\nseries:\", log_feature)\n",
    "plot_series(collection_2, 20, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "X = scaled_feature.toPandas()\n",
    "X.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_mark(observation, min_val, class_length, bins):\n",
    "    \"\"\"\n",
    "    Computes the corresponding bin to a certain data observation given the data set minimum, size, bins\n",
    "    and the class length\n",
    "    :param observation: float, value of which is required to obtain your bin number\n",
    "    :param min_val: float, minimum value observed in the rdd\n",
    "    :param class_length: float, length of each sub interval\n",
    "    :param bins: int, number of sub intervals\n",
    "    :return: int, bin corresponding to the given observation\n",
    "    \"\"\"\n",
    "    interval = int((observation - min_val) / class_length)\n",
    "    if interval >= bins:\n",
    "        return bins - 1\n",
    "    else:\n",
    "        return interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_rdd_continuous(data_set_rdd, min_val, class_length, bins, n):\n",
    "    \"\"\"\n",
    "    Generates the frequency table rdd from certain continuous column rdd\n",
    "\n",
    "    :param data_set_rdd: rdd, rdd of the continuous column of which the histogram will be computed\n",
    "    :param min_val: float, minimum value observed in the rdd\n",
    "    :param class_length: float, length of each sub interval\n",
    "    :param bins: int, number of sub intervals\n",
    "    :param n: int, table length\n",
    "    :return: rdd, rdd containing the frequencies for each class of the histogram\n",
    "    \"\"\"\n",
    "    frequency_rdd = data_set_rdd \\\n",
    "        .map(lambda x: (class_mark(x, min_val, class_length, bins), 1)) \\\n",
    "        .reduceByKey(lambda x, y: x + y) \\\n",
    "        .map(lambda y: (y[0]+1, min_val+class_length*y[0], min_val+class_length*(y[0]+1), y[1], y[1]/n))\n",
    "    return frequency_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_table_continuous(data_set_df, column, bins=None, suffix=''):\n",
    "    \"\"\"\n",
    "    Computes the histogram frequency table from a column with continuous values for a table Dataframe\n",
    "\n",
    "    :param data_set_df: Dataframe, table of which it is required to calculate the frequency histogram of some of\n",
    "    its columns\n",
    "    :param column: string, column with continuous values which is required to calculate its histogram\n",
    "    :param bins: int, number of sub intervals\n",
    "    :param suffix: string, assign the suffix to each column of the frequency table\n",
    "    :return: Dataframe with the histogram frequency table\n",
    "    \"\"\"\n",
    "    freq_schema = ['bin', 'lower_limit'+suffix, 'upper_limit'+suffix, 'fa_'+column+suffix, 'f_'+column+suffix]\n",
    "    window_freq = Window.orderBy('bin').rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "    cumulative_rel_freq = spark_sum('f_'+column+suffix)\\\n",
    "        .over(window_freq)\\\n",
    "        .alias('F_cumulative_'+column+suffix)\n",
    "\n",
    "    cumulative_abs_freq = spark_sum('fa_'+column+suffix)\\\n",
    "        .over(window_freq)\\\n",
    "        .alias('Fa_cumulative_'+column+suffix)\n",
    "\n",
    "    data_set_rdd = data_set_df.select(column).rdd.map(lambda row: (row[0]))\n",
    "    n = data_set_rdd.count()\n",
    "\n",
    "    if bins is None:\n",
    "        bins = 1 + int(3.322 * np.log(n))\n",
    "\n",
    "    maximum = data_set_rdd.max()\n",
    "    minimum = data_set_rdd.min()\n",
    "    class_length = (maximum - minimum) / bins\n",
    "\n",
    "    frequency_table_df = frequency_rdd_continuous(data_set_rdd, minimum, class_length, bins, n)\\\n",
    "        .toDF(freq_schema)\\\n",
    "        .select('*', cumulative_abs_freq, cumulative_rel_freq)\n",
    "    return frequency_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_analysis = portfolio_dates.select(new_array).where(col(\"TASA\").isNotNull())\n",
    "frequency_table = frequency_table_continuous(data_set_df=feature_analysis, column=\"TASA\")\n",
    "frequency_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_plot = \"F_cumulative_TASA\"\n",
    "to_pandas_df = frequency_table.toPandas()\n",
    "to_pandas_df.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_frequency_table = frequency_table_continuous(data_set_df=scaled_feature, column=\"log_TASA\")\n",
    "log_frequency_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_plot = \"F_cumulative_log_TASA\"\n",
    "to_pandas_df = log_frequency_table.toPandas()\n",
    "to_pandas_df.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
