{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, count as spark_count, avg as spark_avg\n",
    "\n",
    "pd.set_option('display.max_columns', 10000000)\n",
    "pd.set_option('display.max_rows', 10000000)\n",
    "pd.set_option('display.width', 10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iniciamos Sesi√≥n con Ophelia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enquire.vendetta import Ophelia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ophelia = Ophelia()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CSV daily price Funds file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_portfolio_data(path_file, source, date_col, withSchema=True):\n",
    "    spark = ophelia.spark\n",
    "    portfolio_path_file = path_file\n",
    "    portfolio_data = ophelia.mazterize.read_file(path_file, source, spark)\n",
    "    if withSchema == True:\n",
    "        return ophelia.mazterize.schema_define(portfolio_data, date_col)\n",
    "    return portfolio_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change impure schema portfolio input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_df = read_portfolio_data(\n",
    "    path_file=\"data-resources/raw/csv/data.csv\",\n",
    "    source=\"csv\",\n",
    "    date_col=\"operation_date\",\n",
    ")\n",
    "\n",
    "portfolio_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Year parameters input array:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data, analytic base table structuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_date_window(df, from_year, to_year, col_date):\n",
    "    year_array = ophelia.arrays.year_array(from_year, to_year)\n",
    "    split_dates = ophelia.dataframe.split_date_columns(df, col_date)\n",
    "    operation_dates_list = ophelia.arrays.sorted_date_list(df, col_date)\n",
    "    date_index_udf = ophelia.arrays.dates_index(operation_dates_list)\n",
    "    portfolio_dates = split_dates.where(col(col_date+\"_year\").isin(year_array))\\\n",
    "                                 .select('*', (date_index_udf(col(col_date))).alias(col_date[:9]+\"_id\"))\n",
    "    return portfolio_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_window_df = portfolio_date_window(\n",
    "    df=portfolio_df, \n",
    "    from_year=\"2016\", \n",
    "    to_year=\"2019\", \n",
    "    col_date=\"operation_date\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitoring_empty_vector(df, feature_type):\n",
    "    float_cols = ophelia.arrays.feature_picking(df)[str(feature_type)]\n",
    "    count_by_col = [spark_count(col(x)).alias(str(x)) for x in float_cols]\n",
    "    aggregate_columns = df.select(*count_by_col)\n",
    "    return aggregate_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_null(panel, missing_days, N):\n",
    "    null_count = panel.select([col(c).alias(c) for c in panel.columns]).collect()[0].asDict()\n",
    "    clean_null_list = [k for k, v in null_count.items() if v < abs(missing_days - N)]\n",
    "    return clean_null_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_empty_vector(df, feature_type, missing_days=10):\n",
    "    sample_count = df.count()\n",
    "    empty_panel = monitoring_empty_vector(df, feature_type)\n",
    "    clean_null_list = debug_null(empty_panel, missing_days, sample_count)\n",
    "    debug_vector = df.drop(*clean_null_list)\n",
    "    return debug_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_none_df = debug_empty_vector(portfolio_window_df, feature_type=\"float\")\n",
    "remove_none_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_impute(df):\n",
    "    float_cols = ophelia.arrays.feature_picking(df)[\"float\"]\n",
    "    numerical_fields = df.agg(*(spark_avg(c).alias(c) for c in df.columns if c in float_cols))\n",
    "    portfolio_base_table = df.na.fill(numerical_fields.collect()[0].asDict())\n",
    "    return portfolio_base_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_base_table = mean_impute(remove_none_df)\n",
    "portfolio_base_table.orderBy(col(\"operation_date\").desc()).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_price_lag(df, on=\"operation_index\", how=\"left\"):\n",
    "    portfolio_indexed = ophelia.rdd.row_indexing(df)\n",
    "    lag_portfolio_df = ophelia.dataframe.lag_min_max_data(df)\n",
    "    lag_portfolio_indexed = ophelia.rdd.row_indexing(lag_portfolio_df)\n",
    "    join_indexed = portfolio_indexed.join(lag_portfolio_indexed, on=on, how=how)\n",
    "    return join_indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_price_lag = join_price_lag(portfolio_base_table)\n",
    "join_price_lag.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_yield(df, fix_cols):\n",
    "    counter_count = 1\n",
    "    float_cols = ophelia.arrays.feature_picking(portfolio_base_table)[\"float\"]\n",
    "    price_yield = df.select(*fix_cols, \n",
    "                            *[((col(c) / col(\"{0}_lag\".format(c)) - counter_count)).alias(\"{0}_yield\".format(c)) for c in float_cols]).na.fill(0)\n",
    "    return price_yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_cols = [\n",
    "    \"operation_index\", \n",
    "    \"operation_id\", \n",
    "    \"operation_date\", \n",
    "    \"operation_date_year\",\n",
    "    \"operation_date_month\",\n",
    "    \"operation_date_day\"\n",
    "]\n",
    "portfolio_yield_df = price_yield(df=join_price_lag, fix_cols=fix_cols).orderBy(col(\"operation_index\"))\n",
    "portfolio_yield_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Portfolio's Yield dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_path = ophelia.mazterize.write_parquet(\n",
    "    dataframe=portfolio_yield_df, \n",
    "    name_directory=\"portfolio_yield_window\", \n",
    "    partition_field=\"operation_date\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yield(parquet_path, source=\"parquet\"):\n",
    "    spark = ophelia.spark\n",
    "    yield_df = ophelia.mazterize.read_file(path_source=parquet_path, source=source, spark_session=spark)\n",
    "    return yield_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_porfolio_df = read_yield(parquet_path=price_path)\n",
    "yield_porfolio_df.orderBy(col(\"operation_date\").desc()).limit(5).toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
