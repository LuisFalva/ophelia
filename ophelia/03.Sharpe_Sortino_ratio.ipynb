{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import style\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from functools import reduce\n",
    "from pyspark.sql.window import Window \n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import monotonically_increasing_id, col, lit, isnull, when, udf, row_number,\\\n",
    "                                  avg as spark_avg, stddev as spark_stddev, sqrt, abs as spark_abs\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 10000000)\n",
    "pd.set_option('display.max_rows', 10000000)\n",
    "pd.set_option('display.width', 10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Spark Session for pseudo-distributed computing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Sharpe&Sortino_ratio').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading persisted Portfolio Yields dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_yield_window_path = 'data/master/ophelia/data/OpheliaData/portfolio_yield_window/'\n",
    "portfolio_yield_df = spark.read.parquet(portfolio_yield_window_path)\n",
    "portfolio_yield_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll_df(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [\n",
    "    portfolio_yield_df.select(lit(fund).alias('fund_name'), \n",
    "                              col(fund).alias('fund_yield')) for fund in portfolio_yield_df.columns[5:-1]\n",
    "]\n",
    "portfolio_yield_T = unionAll_df(*dataframes).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_yield_T.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Portfolio's Yield Transpose dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writing_path_mod3 = 'data/master/ophelia/data/OpheliaData/portfolio_yield_transpose/'\n",
    "\n",
    "print('\\nWriting parquets ...')\n",
    "portfolio_yield_T.repartition(1).write.mode('overwrite').parquet(writing_path_mod3)\n",
    "\n",
    "%time\n",
    "print('\\nSUCCESS \\nPARQUET DATA SAVED!')\n",
    "print('\\nNew root path tabla data:', writing_path_mod3)\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading persisted Portfolio Yields Transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_yield_T_path = '/data/core/fince/data/portfolioOptimization/portfolio_yield_transpose/'\n",
    "portfolio_yield_T_df = spark.read.parquet(portfolio_yield_T_path)\n",
    "len(portfolio_yield_T_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRESHOLD = float(0.0)\n",
    "CASE = \"BMERGOB\"\n",
    "\n",
    "negative_fund_yield = portfolio_yield_T_df.where(col(\"fund_yield\") < TRESHOLD)\n",
    "negative_fund_yield.where(col(\"fund_name\") == CASE).show(5)\n",
    "negative_fund_yield.where(col(\"fund_name\") == CASE)\\\n",
    "                   .describe(\"fund_yield\")\\\n",
    "                   .where((col(\"summary\") == \"min\")\n",
    "                        | (col(\"summary\") == \"max\")\n",
    "                        | (col(\"summary\") == \"mean\")\n",
    "                        | (col(\"summary\") == \"stddev\")).show()\n",
    "print(\"after filtering negative yields we've got following parameters:\")\n",
    "print(\"{stddev:8.547E-5 , min: -1.872, max: -4.646}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_yield_df = negative_fund_yield.groupBy(\"fund_name\")\\\n",
    "                                   .agg(spark_abs(spark_avg(col('fund_yield'))).alias(\"downside_mean_yield\"),\n",
    "                                        when(isnull(spark_stddev(col('fund_yield'))), 0).otherwise(\n",
    "                                            spark_stddev(col('fund_yield'))).alias(\"downside_stddev_yield\"))\n",
    "                                                                \n",
    "print(\"mean yield df:\")\n",
    "mean_yield_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sortino ratio:\n",
    "\n",
    "## **The Formula for the Sortino Ratio Is:**\n",
    "## Sortino Ratio = $\\frac{ R_p - r_f }{ \\sigma_d }$ \n",
    "## **Where:**\n",
    "### *R_p = Actual or expected portfolio return*\n",
    "### *r_f = Risk-free rate*\n",
    "### *sigma_d = Standard deviation of the downside*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_free_rate = 0\n",
    "sortino_df = mean_yield_df.select(\"*\", \n",
    "                                  ((col(\"downside_mean_yield\") - lit(risk_free_rate)) / col(\"downside_stddev_yield\")).alias(\"sortino_ratio\"))\\\n",
    "                          .na.fill(0)\n",
    "sortino_df.orderBy(col(\"sortino_ratio\")).show(5)\n",
    "sortino_df.where(col(\"fund_name\") == CASE).show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharpe ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe_df = portfolio_yield_T_df.groupBy(\"fund_name\")\\\n",
    "                                .agg(spark_avg('fund_yield').alias(\"mean_yield\"), spark_stddev('fund_yield').alias(\"stddev_yield\"))\\\n",
    "                                .select(\"*\", ((col(\"mean_yield\") - lit(risk_free_rate)) / col(\"stddev_yield\")).alias(\"sharpe_ratio\"))\n",
    "sharpe_df.orderBy(col(\"sharpe_ratio\").desc()).show(5)\n",
    "sharpe_df.where(col(\"fund_name\") == CASE).show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joined Both Ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.orderBy(\"fund_name\") \n",
    "joined_ratios_df = sortino_df.join(sharpe_df, on=\"fund_name\", how=\"left\").select(\"*\", row_number().over(w).alias(\"id\"))\n",
    "joined_ratios_df.printSchema()\n",
    "joined_ratios_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_ratios_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = joined_ratios_df.columns[1:-1]\n",
    "\n",
    "def pca_sklearn(spark_df, col_features, scree_plot=False):\n",
    "    data = spark_df.select(col_features).toPandas()\n",
    "    scaled_data = preprocessing.scale(data.T)\n",
    "    pca = PCA()\n",
    "    pca.fit(scaled_data)\n",
    "    pca_data = pca.transform(scaled_data)\n",
    "    per_var = np.round(pca.explained_variance_* 100, decimals=1)\n",
    "    labels = [\"PC\" + str(x) for x in range(1, len(per_var)+1)]\n",
    "    if scree_plot == True:\n",
    "        plt.bar(x=range(1, len(per_var)+1), height=per_var, tick_label=labels)\n",
    "        plt.ylabel(\"Percentage of Explained Variance\")\n",
    "        plt.xlabel(\"Principal Component\")\n",
    "        plt.show()\n",
    "    return pca_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_sklearn(spark_df=joined_ratios_df, col_features=f, scree_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_spark(numpy_array, labels_col):\n",
    "    pca_rdd = sc.parallelize(numpy_array)\n",
    "    return pca_rdd.map(lambda x: x.tolist()).toDF(labels_col)\n",
    "\n",
    "def numpy_pandas(numpy_array, labels_col):\n",
    "    pca_rdd = sc.parallelize(numpy_array)\n",
    "    return pca_rdd.map(lambda x: x.tolist()).toDF(labels_col).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = numpy_pandas(numpy_array=pca_data, labels_col=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca_df.PC1, pca_df.PC2)\n",
    "plt.title(\"My PCA Graph\")\n",
    "plt.xlabel(\"PC1 - {0}%\".format(per_var[0]))\n",
    "plt.ylabel(\"PC2 - {0}%\".format(per_var[1]))\n",
    "for sample in pca_df.index:\n",
    "    plt.annotate(sample, (pca_df.PC1.loc[sample], pca_df.PC2.loc[sample]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centils grouping test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_collection(df, col_collect):\n",
    "    row_collection = df.select(col_collect).collect()\n",
    "    list_collection = []\n",
    "    for row in range(len(row_collection)):\n",
    "        list_collection.append(row_collection[row][0])\n",
    "    return sorted(list_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volatile_yield_list = column_collection(df=joined_ratios_df, col_collect=\"stddev_yield\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array(volatile_yield_list)\n",
    "p = np.percentile(a, 50)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "33, 66, 100, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "33+33+33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means model for Clustering Sharpe and Sortino Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vector, VectorUDT, Vectors\n",
    "from pyspark.mllib.linalg.distributed import IndexedRowMatrix, IndexedRow\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But first! PCA dimension reduction analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After PCA dimension reduction analysis, the 93% of de variance is explained by sortino and sharpe ratio (we don't need more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = [\"sortino_ratio\", \"sharpe_ratio\"]\n",
    "vector_assembler = VectorAssembler(inputCols=features_list, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_assembler_df = vector_assembler.transform(joined_ratios_df)\n",
    "vector_assembler_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = joined_ratios_df.count()\n",
    "print(\"n:\", n)\n",
    "p = len(features_list)\n",
    "print(\"p:\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_change = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "vector_dense_df = vector_assembler_df.withColumn(\"features\", udf_change(\"features\"))\n",
    "vector_dense_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_model = standard_scaler.fit(vector_dense_df.select(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_feature_df = scaler_model.transform(vector_dense_df)\n",
    "scaled_feature_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_feature_df.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picking_columns = scaled_feature_df.select(\"id\", \"scaled_features\")\n",
    "feature_df = IndexedRowMatrix(picking_columns.rdd.map(lambda x: IndexedRow(x[0], x[1].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD = feature_df.computeSVD(p, True)\n",
    "U = SVD.U\n",
    "S = SVD.s.toArray()\n",
    "eigen_vals = S**2/(n-1)\n",
    "eigvals = np.flipud(np.sort(eigen_vals))\n",
    "cumsum = eigvals.cumsum()\n",
    "total_variance_explained = cumsum/eigvals.sum()\n",
    "K = np.argmax(total_variance_explained>0.95)+1\n",
    "V = SVD.V\n",
    "U = U.rows.map(lambda x: (x.index, x.vector[0:K]*S[0:K]))\n",
    "princ_comps = np.array(list(map(lambda x:x[1], sorted(U.collect(), key = lambda x:x[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_variance_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means with selected feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_5 = KMeans(featuresCol=\"features\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kmeans = kmeans_5.fit(scaled_feature_df)\n",
    "final_fund_class = model_kmeans.transform(scaled_feature_df).select(\"id\", \"fund_name\", \"sortino_ratio\", \"sharpe_ratio\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fund_class.groupBy(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fund_class.where(col(\"prediction\") == 3).show(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fund_class.orderBy(col(\"prediction\")).show(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fund_pd = final_fund_class.toPandas()\n",
    "plt.scatter(final_fund_pd.sortino_ratio, final_fund_pd.sharpe_ratio, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(2, 9):\n",
    "    kmean = KMeans(featuresCol=\"features\", k=k)\n",
    "    model = kmean.fit(scaled_feature_df)\n",
    "    squared_error = model.computeCost(scaled_feature_df)\n",
    "    print(\"with k={}\".format(k))\n",
    "    print(\"within set sum of squared errors = \", str(squared_error))\n",
    "    print(\"---\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se queda con 5 grupos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
