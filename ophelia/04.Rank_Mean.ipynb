{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, array, explode, struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Rank_Mean').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## por temas de replicación, siempre es mejor convertir el archivo a leer en formato csv\n",
    "\n",
    "monthly_data = pd.read_csv(\"data-resources/monthly_fund_price/monthly_data.csv\")\n",
    "daily_data = pd.read_csv(\"data-resources/daily_fund_price/daily_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## seleccionamos la primera columna y lo convertimos a vector, esta columna representa el indice que estamos analizando, \n",
    "## 'MXWDU_Index', la idea es medir esta columna con el resto, ya que las demás son acciones que forman parte de ése índice.\n",
    "\n",
    "benchmark_month = monthly_data.loc[0:monthly_data.shape[0], monthly_data.columns[1]]\n",
    "benchmark_day = daily_data.loc[0:daily_data.shape[0], daily_data.columns[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculamos el porcentaje de cambio de un día contra otro.\n",
    "## fórmula: (t+1 / t)-1\n",
    "## pseudo-código: (precio_hoy / precio_ayer)-1\n",
    "\n",
    "pct_benchmark_month = benchmark_month.pct_change(1)\n",
    "pct_benchmark_day = benchmark_day.pct_change(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## el vector de \"percentage change\" se convierte a un arreglo numpy de dimensión (147,)\n",
    "## NOTA: en los arreglos (objetos) de tipo numpy.array preservan los valores, tanto por fila, como por columna,\n",
    "##       el órden de los elementos, cómo un 'índice implícito'\n",
    "\n",
    "pct_benchmark_month_array = np.array(pct_benchmark_month)\n",
    "pct_benchmark_day_array = np.array(pct_benchmark_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lo mismo hacemos, pero para el resto de variables equity,\n",
    "## seleccionamos la primera columna y lo convertimos a vector, esta columna representa el indice que estamos analizando, \n",
    "## 'MXWDU_Index', la idea es medir esta columna con el resto, ya que las demás son acciones que forman parte de ese índice.\n",
    "\n",
    "investment_universe_month = monthly_data.loc[0:monthly_data.shape[0],monthly_data.columns[2:monthly_data.shape[1]]]\n",
    "investment_universe_day = daily_data.loc[0:daily_data.shape[0],daily_data.columns[2:daily_data.shape[1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (t+1 / t)-1\n",
    "## (precio_hoy / precio_ayer)-1\n",
    "\n",
    "pct_investment_month = investment_universe_month.pct_change(1)\n",
    "pct_investment_day = investment_universe_day.pct_change(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_investment_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## el vector de percentage change se convierte a un arreglo numpy de dimensión (147,70)\n",
    "\n",
    "pct_investment_month_array = np.array(pct_investment_month)\n",
    "pct_investment_day_array = np.array(pct_investment_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creamos arreglos numpy con dimensiones X+1 = 148, rellenas de ceros, para ser imputados con nuevos vectores\n",
    "\n",
    "up_month = np.zeros((pct_benchmark_month_array.shape[0]+1, 1))\n",
    "down_month = np.zeros((pct_benchmark_month_array.shape[0]+1, 1))\n",
    "up_move = np.zeros((pct_benchmark_month_array.shape[0]+1, pct_investment_month_array.shape[1]))\n",
    "down_move = np.zeros((pct_benchmark_month_array.shape[0]+1, pct_investment_month_array.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rellenamos las matrices de ceros con valores que aprueben las condiciones, \n",
    "## se realiza una comparación dentro de los arreglos de porcentajes de cambio, \n",
    "## sí alguno de esos porcentajes es superior a 0, entonces entra a los arreglos \n",
    "## de movimientos positivos (incrementos), pero sí alguno es menor que 0, entonces\n",
    "## el porcentaje se almacena en los arreglos de movimientos negativos (decrementos).\n",
    "\n",
    "## Básicamente, se separan los porcentajes de cambio en dos matrices: \n",
    "## matriz de positivos cuando el porcentaje es > 0 \n",
    "## matriz de negativos cuando el porcentaje es <= 0\n",
    "\n",
    "size_benchmark_matrix = pct_benchmark_month_array.shape[0]\n",
    "for i in range (1, size_benchmark_matrix):\n",
    "    if pct_benchmark_month_array[i] > 0:\n",
    "        up_month[i] = pct_benchmark_month_array[i]\n",
    "        up_move[i] = pct_investment_month_array[i, 0:pct_investment_month_array.shape[1]]\n",
    "    else:\n",
    "        down_month[i] = pct_benchmark_month_array[i]\n",
    "        down_move[i] = pct_investment_month_array[i, 0:pct_investment_month_array.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculamos los vectores 'peor más alto' y 'mejor más alto'\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "greater_worse = down_move / down_month\n",
    "greater_better = (up_move / up_month) * float(-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ambos vectores los convertimos a pandas dataframes, y solo nos quedamos con los vectores que tengan valores != np.nan\n",
    "## una de las ventajas de los pandas dataframes es que mantienen un ídince único por row, esto lo hace poder separarse, y juntarse\n",
    "## en cuantos sub-conjuntos se requieran y siempre se podrá mantener un órden.\n",
    "\n",
    "greater_worse_df = pd.DataFrame(data=greater_worse).dropna()\n",
    "greater_better_df = pd.DataFrame(data=greater_better).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculamos ahora, la mediana acumulada con los pandas dataframes que construimos, \n",
    "## con un periodo mínimo (método expanding) de al menos 1 observación dada.\n",
    "\n",
    "median_down = greater_worse_df.expanding().median()\n",
    "median_up = greater_better_df.expanding().median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se transponen ambos pandas df por la columna periodos, columna que almacena números no consecutivos desde 1 hasta 147\n",
    "\n",
    "down_transpose = median_down.T\n",
    "up_transpose = median_up.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se rankean los resultados (top 10) entre las fechas cierre (periodo) y se vuelve a transponer la tabla ranked_down\n",
    "\n",
    "ranked_down = down_transpose.rank()\n",
    "transpose_ranked_down = ranked_down.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se rankean los resultados (top 10) entre las fechas cierre (periodo) y se vuelve a transponer la tabla ranked_up\n",
    "\n",
    "ranked_up = up_transpose.rank()\n",
    "transpose_ranked_up = ranked_up.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se añade variable 'label' con la idea de que al juntar ambos dataframes se puedan distinguir los 'worse' de los 'better'\n",
    "## y se unen ambos dataframes con la etiqueta creada, se usó el método 'insert' por lo que no se deberá correr de nuevo, una\n",
    "## vez ejecutado ya que fallará por duplicidad de columnas.\n",
    "\n",
    "worse_better_df = pd.concat([transpose_ranked_up, transpose_ranked_down]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se crea un índice 'closing_id' para cada registro, éste corre de [1:N] \n",
    "## con la idea de etiquetar el id del mes de registro de cierre,\n",
    "## de la misma forma que lo anterior, NO se deberá ejecutar de nuevo; una vez hecho.\n",
    "\n",
    "worse_better_df['closing_id'] = range(1, len(worse_better_df) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se transponen ambos dataframes, de antes tener una dimensión (68, 70), es decir; \n",
    "## 68 registros i.e. 'Rows' (variables)\n",
    "## 70 columnas fijas (a menos que sea añadido otro asset desde el csv inicial)\n",
    "\n",
    "## a tener una dimensión 'transpuesta' (invertída sí querés...) de (70, 68), es decir;\n",
    "## 70 registros i.e. 'Rows' fijos (a menos que sea añadido otro asset desde el csv inicial)\n",
    "## 68 columnas (variables!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## de pandas dataframes, una vez separados en dos conjuntos ['worse', 'better'],\n",
    "## creamos por separado dos spark dataframes.\n",
    "\n",
    "worse_better = spark.createDataFrame(worse_better_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se crea un generador \"shape long format\", una lista con iteraciones, esta lista trabajará con dos variables principales,\n",
    "## 1-. la variable 'equity_index', será la que contenga los 'id' de los activos\n",
    "## 2-. la variable 'median_down', será la que contenga la mediana acumulada por cada activo.\n",
    "## Se mantendrá a lo largo de la transformación 1 columna fija; 'closing_id',\n",
    "## closing_id: variable que indica el mes de cierre y reporte de precio\n",
    "\n",
    "def shape_long_format(dataframe, pivot_col):\n",
    "    \n",
    "    columns, data_type = zip(*((c, t) for (c, t) in dataframe.dtypes if c not in pivot_col))\n",
    "    assert len(set(data_type)) == 1, \"Columns not the same data type...\"\n",
    "    \n",
    "    generator_explode = explode(array([\n",
    "        struct(lit(c).alias(\"asset_id\"), col(c).alias(\"top_rank\")) for c in columns\n",
    "    ])).alias(\"column_explode\")\n",
    "\n",
    "    return dataframe.select(pivot_col + [generator_explode]).select(pivot_col + [\"column_explode.asset_id\", \"column_explode.top_rank\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se crea nuevo spark-dataframe donde solo se mostrará por partición ['closing_id'] \n",
    "## el top 10 mejores meses donde tuvo menos malos que el resto de los registros; [\"top_rank\"].\n",
    "\n",
    "asset_ranking_df = shape_long_format(worse_better, [\"closing_id\"]).where(col(\"top_rank\") <= 10).orderBy(\"closing_id\", \"top_rank\")\n",
    "asset_ranking_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A partir de aqui se escribe en formato csv para trabajar con Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aqui se re-escribe el parquet a formato csv.\n",
    "\n",
    "####################################################\n",
    "#¡¡¡¡¡¡OJO, NO CORRER A MENOS QUE SE REQUIERA!!!!!!#\n",
    "####################################################\n",
    "\n",
    "#asset_ranking_df.coalesce(1).write.mode('overwrite').option(\"header\",\"true\").csv(\"data-resources/asset_ranking_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ruta donde se encuentra el nuevo csv es: \"~/data-resources/asset_ranking_csv/part-00000-b95a05be-c938-4f1b-b4fd-5c3490966a8e-c000.csv\"\n",
    "\n",
    "mdt_path = \"data-resources/asset_ranking_csv/part-00000-b95a05be-c938-4f1b-b4fd-5c3490966a8e-c000.csv\"\n",
    "pandas_df = pd.read_csv(mdt_path)\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lo que se pretende ahora, es obtener una matriz de dimensión (146, 10), es decir, \n",
    "## tener en cada row las fechas de cierre [closing_id],\n",
    "## en cada columna (header) el número de ranking top 10 [top_rank],\n",
    "## y en cada campo, el id del activo [asset_id].\n",
    "\n",
    "pandas_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pandas_df.astype({'asset_id':'int32'})\n",
    "newselect = pandas_df[[\"closing_id\",\"asset_id\"]]\n",
    "num_of_assets = 10\n",
    "\n",
    "indexed = np.zeros((investment_universe_month.shape[0],num_of_assets))\n",
    "\n",
    "for q in range(1,investment_universe_month.shape[0]):\n",
    "    selection = newselect.loc[pandas_df[\"closing_id\"]==q]\n",
    "    newselect_transpose = selection.T\n",
    "    newpdf = newselect_transpose['asset_id':].head()\n",
    "    indexed[q] = newpdf\n",
    "\n",
    "indexed = indexed[1:investment_universe_month.shape[0]+1]\n",
    "\n",
    "\n",
    "index_row = indexed.astype(np.int64)\n",
    "newrow = [0]*num_of_assets #np.zeros((1,num_of_assets))\n",
    "index_row = np.vstack([index_row,newrow])\n",
    "portfolio = np.zeros((investment_universe_month.shape[0],num_of_assets))\n",
    "\n",
    "for r in range(0,investment_universe_month.shape[0]-1):\n",
    "    s = r+1\n",
    "    columns = index_row[s]\n",
    "    portfolio[s] = pct_investment_month_array[s,[columns]]\n",
    "    \n",
    "means = portfolio[1:investment_universe_month.shape[0]-1]\n",
    "performance = np.dot(portfolio,(1/num_of_assets))\n",
    "returns = np.zeros((investment_universe_month.shape[0]-1,1))\n",
    "equalw = np.zeros((investment_universe_month.shape[0]-1,1))\n",
    "eweights = 1/(pct_investment_month_array.shape[1])\n",
    "eweighted = np.dot(pct_investment_month_array,eweights)\n",
    "\n",
    "for x in range (1,investment_universe_month.shape[0]):\n",
    "    returns[x-1] = sum(performance[x])\n",
    "    equalw[x-1] = sum(eweighted[x])\n",
    "        \n",
    "\n",
    "beg = 100\n",
    "start = 100\n",
    "commence = 100\n",
    "bench = pct_benchmark_month_array[1:investment_universe_month.shape[0]]\n",
    "bmk = np.zeros((investment_universe_month.shape[0],))\n",
    "port = np.zeros((investment_universe_month.shape[0]-1,))\n",
    "ew = np.zeros((investment_universe_month.shape[0]-1,))\n",
    "\n",
    "\n",
    "for i in range (0,investment_universe_month.shape[0]-1):\n",
    "    bmk[i] = beg*(1+bench[i])\n",
    "    beg = bmk[i]\n",
    "    port[i] = start*(1+returns[i])\n",
    "    start = port[i]\n",
    "    ew[i] = commence*(1+equalw[i])\n",
    "    commence = ew[i]\n",
    "    \n",
    "plt.plot(bmk[0:investment_universe_month.shape[0]-1])\n",
    "plt.plot(port)\n",
    "plt.plot(ew)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
