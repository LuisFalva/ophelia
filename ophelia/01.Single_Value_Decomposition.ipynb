{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.mllib.linalg.distributed import IndexedRowMatrix, IndexedRow, RowMatrix\n",
    "from pyspark.ml.feature import StandardScaler, PCA\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', int(1e7))\n",
    "pd.set_option('display.max_rows', int(1e7))\n",
    "pd.set_option('display.width', int(1e7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Spark Session for pseudo-distributed computing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Single_Value_Decomposition_Portfolio').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading persisted Portfolio Yields dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_yield_window_path = '/data/core/fince/data/portfolioOptimization/portfolio_yield_window/'\n",
    "portfolio_yield_df = spark.read.parquet(portfolio_yield_window_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_yield_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_array = portfolio_yield_df.columns[:-5]\n",
    "monthly_return = np.array(portfolio_yield_df.select(*field_array).collect())\n",
    "print('test with p:', len(field_array), 'funds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('monthly_return matrix:\\n', monthly_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El cálculo se realiza utilizando la descomposición de valores singulares (Singular Value Decomposition, SVD). La SVD de cualquier matriz $mxn$ se calcula como:\n",
    "\n",
    "$$A = U \\sum V^T$$\n",
    "\n",
    "### Donde $U$ es una matriz ortogonal $m×m$ cuyas columnas son los vectores propios (eigenvectores) de  $AA^T$ , $V$ es una matriz ortogonal $n×n$ cuyas columnas son los eigenvectores de  $A^T A$ , y $\\sum$ es una matriz diagonal $m×n$ y sus valores son cero excepto a lo largo de la diagonal.\n",
    "\n",
    "### Al aplicar PCA, tenemos que centrar nuestros datos, es decir, tenemos que restar la media de la columna. Luego, según la naturaleza de nuestros datos, es posible que necesitemos estandarizar nuestros datos (hacer que cada característica tenga una varianza unitaria y una media cero). Si las columnas están en diferentes escalas, como el año, la temperatura, la concentración de dióxido de carbono, por ejemplo, tenemos que estandarizar los datos. Si los datos están en la misma unidad, por otro lado, la estandarización puede provocar la pérdida de información importante. En el primer caso, cuando las columnas están en la misma unidad y en una escala similar, usamos la matriz de covarianza para SVD pero cuando las unidades son diferentes ya que estandarizamos los datos, usamos la matriz de correlación.\n",
    "\n",
    "### Los componentes principales (PC) son el producto matricial de los datos originales y la matriz $V$, que es igual al producto de las matrices $U$ y $\\sum$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Value Decomposition analysis.\n",
    "\n",
    "### At the very first step we have to take two input parameters, one is called ___n___, that refers to the total count of rows in dataframe. The second refers to the total number of columns called _features_, i.e. ___p___. So that we will find this matrix with _(n, p)_ dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_return_rdd = sc.parallelize(monthly_return.tolist()).zipWithIndex()\n",
    "\n",
    "# Obtaining model parameters:\n",
    "#      number of total rows n\n",
    "#      number of total features p\n",
    "n = monthly_return_rdd.count()\n",
    "p = len(monthly_return_rdd.take(1)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do we want to confirm is that every vector $\\vec{V_i}$ of length p is a _dense vector_. This is, we want to get fully completed vectors without any null values. For that, we create a udf function (User Defined Function) with this implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_dense_vector = udf(lambda x: Vectors.dense(x), VectorUDT())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will overwrite the _features_ column with dense vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_return_df = spark.createDataFrame(monthly_return_rdd).toDF(\"features\", \"id\")\\\n",
    "                         .withColumn(\"features\", udf_dense_vector(\"features\"))\n",
    "monthly_return_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_return_df.where(col(\"id\") == 0).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's standarize this dense vectors of length __p__ with the _Standard Scaler_ method, i.e. Mean and Standard Deviation are involved for this standarization (re-scaled vectors of features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdScaler = StandardScaler(withMean=True, withStd=True, inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "model = stdScaler.fit(monthly_return_df)\n",
    "monthly_return_std_df = model.transform(monthly_return_df)\n",
    "monthly_return_std_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to compute SVD we have to transfrom spark-dataframe to a matrix object with indexed elements from scaled features, for that, we will use _IndexedRowMatrix_ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_return_irm = IndexedRowMatrix(monthly_return_std_df.rdd.map(lambda x: IndexedRow(x[0], x[1].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's compute the singular value decomposition of the IndexedRowMatrix. The given row matrix $A$ of dimension __$(m x n)$__ is decomposed into\n",
    "### _$$U s V^{T}$$ where:_\n",
    "* $U$: $(m x k)$ __*left singular vectors* is a IndexedRowMatrix whose columns are the eigenvectors of $(A X A')$__\n",
    "* $s$: __DenseVector consisting of square root of the eigenvalues *singular values* in descending order.__\n",
    "* $V$: $(n x k)$ __*right singular vectors* is a Matrix whose columns are the eigenvectors of $(A' X A)$__\n",
    "\n",
    "### This _computeSVD_ interface recieves two main arguments:\n",
    "* $k$, for $k^{th}$ int number, thus each element $k$ = {${k_{i} \\in \\Bbb R}$}\n",
    "* $U$, with _computeU_ boolean __True__, whether or not to compute $U$. If set to be __True__, then $U$ is computed by $A  V  s^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD = monthly_return_irm.computeSVD(k=p, computeU=True)\n",
    "U = SVD.U\n",
    "S = SVD.s.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_vals = S**2/(n-1)\n",
    "eigvals = np.flipud(np.sort(eigen_vals))\n",
    "cumsum = eigvals.cumsum()\n",
    "total_variance_explained = cumsum/eigvals.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.argmax(total_variance_explained > 0.95)+1\n",
    "V = SVD.V\n",
    "U = U.rows.map(lambda x: (x.index, x.vector[0:K]*S[0:K]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "princ_comps = np.array(list(map(lambda x:x[1], sorted(U.collect(), key = lambda x:x[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=K, inputCol = stdScaler.getOutputCol(), outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(monthly_return_std_df)\n",
    "transformed_feature = model.transform(monthly_return_std_df)\n",
    "np.round(100.00*model.explainedVariance.toArray(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = np.round(100.00*model.pc.toArray(), 4)\n",
    "df_pc = pd.DataFrame(pcs, columns = ['PC_'+str(i) for i in range(1, K+1)], index = field_array)\n",
    "df_pc.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "plt.scatter(princ_comps[:, 1], princ_comps[:, 0], alpha=0.7)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(eigvals, total_variance_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(k=5, seed=1)\n",
    "model_kmeans = kmeans.fit(monthly_return_std_df.select(\"features\"))\n",
    "transformed = model_kmeans.transform(monthly_return_std_df)\n",
    "transformed.groupBy(\"prediction\").count().orderBy(col(\"count\").desc()).show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
