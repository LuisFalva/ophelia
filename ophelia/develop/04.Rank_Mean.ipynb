{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, array, explode, struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Rank_Mean').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## por temas de replicación, siempre es mejor convertir el archivo a leer en formato csv\n",
    "\n",
    "monthly_data = pd.read_csv(\"data/raw/csv/monthly_data.csv\")\n",
    "daily_data = pd.read_csv(\"data/raw/csv/daily_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## seleccionamos la primera columna y lo convertimos a vector, esta columna representa el indice que estamos analizando, \n",
    "## 'MXWDU_Index', la idea es medir esta columna con el resto, ya que las demás son acciones que forman parte de ése índice.\n",
    "\n",
    "benchmark_month = monthly_data.loc[0:monthly_data.shape[0], monthly_data.columns[1]]\n",
    "benchmark_day = daily_data.loc[0:daily_data.shape[0], daily_data.columns[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculamos el porcentaje de cambio de un día contra otro.\n",
    "## fórmula: (t+1 / t)-1\n",
    "## pseudo-código: (precio_hoy / precio_ayer)-1\n",
    "\n",
    "pct_benchmark_month = benchmark_month.pct_change(1)\n",
    "pct_benchmark_day = benchmark_day.pct_change(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## el vector de \"percentage change\" se convierte a un arreglo numpy de dimensión (147,)\n",
    "## NOTA: en los arreglos (objetos) de tipo numpy.array preservan los valores, tanto por fila, como por columna,\n",
    "##       el órden de los elementos, cómo un 'índice implícito'\n",
    "\n",
    "pct_benchmark_month_array = np.array(pct_benchmark_month)\n",
    "pct_benchmark_day_array = np.array(pct_benchmark_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lo mismo hacemos, pero para el resto de variables equity,\n",
    "## seleccionamos la primera columna y lo convertimos a vector, esta columna representa el indice que estamos analizando, \n",
    "## 'MXWDU_Index', la idea es medir esta columna con el resto, ya que las demás son acciones que forman parte de ese índice.\n",
    "\n",
    "investment_universe_month = monthly_data.loc[0:monthly_data.shape[0],monthly_data.columns[2:monthly_data.shape[1]]]\n",
    "investment_universe_day = daily_data.loc[0:daily_data.shape[0],daily_data.columns[2:daily_data.shape[1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (t+1 / t)-1\n",
    "## (precio_hoy / precio_ayer)-1\n",
    "\n",
    "pct_investment_month = investment_universe_month.pct_change(1)\n",
    "pct_investment_day = investment_universe_day.pct_change(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## el vector de percentage change se convierte a un arreglo numpy de dimensión (147,70)\n",
    "\n",
    "pct_investment_month_array = np.array(pct_investment_month)\n",
    "pct_investment_day_array = np.array(pct_investment_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creamos arreglos numpy con dimensiones X+1 = 148, rellenas de ceros, para ser imputados con nuevos vectores\n",
    "\n",
    "up_month = np.zeros((pct_benchmark_month_array.shape[0]+1, 1))\n",
    "down_month = np.zeros((pct_benchmark_month_array.shape[0]+1, 1))\n",
    "up_move = np.zeros((pct_benchmark_month_array.shape[0]+1, pct_investment_month_array.shape[1]))\n",
    "down_move = np.zeros((pct_benchmark_month_array.shape[0]+1, pct_investment_month_array.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rellenamos las matrices de ceros con valores que aprueben las condiciones, \n",
    "## se realiza una comparación dentro de los arreglos de porcentajes de cambio, \n",
    "## sí alguno de esos porcentajes es superior a 0, entonces entra a los arreglos \n",
    "## de movimientos positivos (incrementos), pero sí alguno es menor que 0, entonces\n",
    "## el porcentaje se almacena en los arreglos de movimientos negativos (decrementos).\n",
    "\n",
    "## Básicamente, se separan los porcentajes de cambio en dos matrices: \n",
    "## matriz de positivos cuando el porcentaje es > 0 \n",
    "## matriz de negativos cuando el porcentaje es <= 0\n",
    "\n",
    "size_benchmark_matrix = pct_benchmark_month_array.shape[0]\n",
    "for i in range (1, size_benchmark_matrix):\n",
    "    if pct_benchmark_month_array[i] > 0:\n",
    "        up_month[i] = pct_benchmark_month_array[i]\n",
    "        up_move[i] = pct_investment_month_array[i, 0:pct_investment_month_array.shape[1]]\n",
    "    else:\n",
    "        down_month[i] = pct_benchmark_month_array[i]\n",
    "        down_move[i] = pct_investment_month_array[i, 0:pct_investment_month_array.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculamos los vectores 'peor más alto' y 'mejor más alto'\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "greater_worse = down_move / down_month\n",
    "greater_better = (up_move / up_month) * float(-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ambos vectores los convertimos a pandas dataframes, y solo nos quedamos con los vectores que tengan valores != np.nan\n",
    "## una de las ventajas de los pandas dataframes es que mantienen un ídince único por row, esto lo hace poder separarse, y juntarse\n",
    "## en cuantos sub-conjuntos se requieran y siempre se podrá mantener un órden.\n",
    "\n",
    "greater_worse_df = pd.DataFrame(data=greater_worse).dropna()\n",
    "greater_better_df = pd.DataFrame(data=greater_better).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculamos ahora, la mediana acumulada con los pandas dataframes que construimos, \n",
    "## con un periodo mínimo (método expanding) de al menos 1 observación dada.\n",
    "\n",
    "median_down = greater_worse_df.expanding().median()\n",
    "median_up = greater_better_df.expanding().median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se transponen ambos pandas df por la columna periodos, columna que almacena números no consecutivos desde 1 hasta 147\n",
    "\n",
    "down_transpose = median_down.T\n",
    "up_transpose = median_up.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se rankean los resultados (top 10) entre las fechas cierre (periodo) y se vuelve a transponer la tabla ranked_down\n",
    "\n",
    "ranked_down = down_transpose.rank()\n",
    "transpose_ranked_down = ranked_down.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se rankean los resultados (top 10) entre las fechas cierre (periodo) y se vuelve a transponer la tabla ranked_up\n",
    "\n",
    "ranked_up = up_transpose.rank()\n",
    "transpose_ranked_up = ranked_up.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se añade variable 'label' con la idea de que al juntar ambos dataframes se puedan distinguir los 'worse' de los 'better'\n",
    "## y se unen ambos dataframes con la etiqueta creada, se usó el método 'insert' por lo que no se deberá correr de nuevo, una\n",
    "## vez ejecutado ya que fallará por duplicidad de columnas.\n",
    "\n",
    "worse_better_df = pd.concat([transpose_ranked_up, transpose_ranked_down]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se crea un índice 'closing_id' para cada registro, éste corre de [1:N] \n",
    "## con la idea de etiquetar el id del mes de registro de cierre,\n",
    "## de la misma forma que lo anterior, NO se deberá ejecutar de nuevo; una vez hecho.\n",
    "\n",
    "worse_better_df['closing_id'] = range(1, len(worse_better_df) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se transponen ambos dataframes, de antes tener una dimensión (68, 70), es decir; \n",
    "## 68 registros i.e. 'Rows' (variables)\n",
    "## 70 columnas fijas (a menos que sea añadido otro asset desde el csv inicial)\n",
    "\n",
    "## a tener una dimensión 'transpuesta' (invertída sí querés...) de (70, 68), es decir;\n",
    "## 70 registros i.e. 'Rows' fijos (a menos que sea añadido otro asset desde el csv inicial)\n",
    "## 68 columnas (variables!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## de pandas dataframes, una vez separados en dos conjuntos ['worse', 'better'],\n",
    "## creamos por separado dos spark dataframes.\n",
    "\n",
    "worse_better = spark.createDataFrame(worse_better_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se crea un generador \"shape long format\", una lista con iteraciones, esta lista trabajará con dos variables principales,\n",
    "## 1-. la variable 'equity_index', será la que contenga los 'id' de los activos\n",
    "## 2-. la variable 'median_down', será la que contenga la mediana acumulada por cada activo.\n",
    "## Se mantendrá a lo largo de la transformación 1 columna fija; 'closing_id',\n",
    "## closing_id: variable que indica el mes de cierre y reporte de precio\n",
    "\n",
    "def shape_long_format(dataframe, pivot_col):\n",
    "    \n",
    "    columns, data_type = zip(*((c, t) for (c, t) in dataframe.dtypes if c not in pivot_col))\n",
    "    assert len(set(data_type)) == 1, \"Columns not the same data type...\"\n",
    "    \n",
    "    generator_explode = explode(array([\n",
    "        struct(lit(c).alias(\"asset_id\"), col(c).alias(\"top_rank\")) for c in columns\n",
    "    ])).alias(\"column_explode\")\n",
    "\n",
    "    return dataframe.select(pivot_col + [generator_explode]).select(pivot_col + [\"column_explode.asset_id\", \"column_explode.top_rank\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se crea nuevo spark-dataframe donde solo se mostrará por partición ['closing_id'] \n",
    "## el top 10 mejores meses donde tuvo menos malos que el resto de los registros; [\"top_rank\"].\n",
    "\n",
    "asset_ranking_df = shape_long_format(worse_better, [\"closing_id\"]).where(col(\"top_rank\") <= 10).orderBy(\"closing_id\", \"top_rank\")\n",
    "asset_ranking_df.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A partir de aqui se escribe en formato csv para trabajar con Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aqui se re-escribe el parquet a formato csv.\n",
    "\n",
    "######################################################\n",
    "# ¡¡¡¡¡¡OJO, NO CORRER A MENOS QUE SE REQUIERA!!!!!! #\n",
    "######################################################\n",
    "\n",
    "#asset_ranking_df.coalesce(1).write.mode('overwrite').option(\"header\",\"true\").csv(\"data-resources/asset_ranking_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ruta donde se encuentra el nuevo csv es: \"~/data-resources/asset_ranking_csv/part-00000-b95a05be-c938-4f1b-b4fd-5c3490966a8e-c000.csv\"\n",
    "\n",
    "mdt_path = \"data-resources/asset_ranking_csv/part-00000-b95a05be-c938-4f1b-b4fd-5c3490966a8e-c000.csv\"\n",
    "pandas_df = pd.read_csv(mdt_path)\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lo que se pretende ahora, es obtener una matriz de dimensión (146, 10), es decir, \n",
    "## tener en cada row las fechas de cierre [closing_id],\n",
    "## en cada columna (header) el número de ranking top 10 [top_rank],\n",
    "## y en cada campo, el id del activo [asset_id].\n",
    "\n",
    "pandas_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pandas_df.astype({'asset_id':'int32'})\n",
    "newselect = pandas_df[[\"closing_id\",\"asset_id\"]]\n",
    "num_of_assets = 10\n",
    "\n",
    "indexed = np.zeros((investment_universe_month.shape[0],num_of_assets))\n",
    "\n",
    "for q in range(1,investment_universe_month.shape[0]):\n",
    "    selection = newselect.loc[pandas_df[\"closing_id\"]==q]\n",
    "    newselect_transpose = selection.T\n",
    "    newpdf = newselect_transpose['asset_id':].head()\n",
    "    indexed[q] = newpdf\n",
    "\n",
    "indexed = indexed[1:investment_universe_month.shape[0]+1]\n",
    "\n",
    "\n",
    "index_row = indexed.astype(np.int64)\n",
    "newrow = [0]*num_of_assets #np.zeros((1,num_of_assets))\n",
    "index_row = np.vstack([index_row,newrow])\n",
    "portfolio = np.zeros((investment_universe_month.shape[0],num_of_assets))\n",
    "\n",
    "for r in range(0,investment_universe_month.shape[0]-1):\n",
    "    s = r+1\n",
    "    columns = index_row[s]\n",
    "    portfolio[s] = pct_investment_month_array[s,[columns]]\n",
    "    \n",
    "means = portfolio[1:investment_universe_month.shape[0]-1]\n",
    "performance = np.dot(portfolio,(1/num_of_assets))\n",
    "returns = np.zeros((investment_universe_month.shape[0]-1,1))\n",
    "equalw = np.zeros((investment_universe_month.shape[0]-1,1))\n",
    "eweights = 1/(pct_investment_month_array.shape[1])\n",
    "eweighted = np.dot(pct_investment_month_array,eweights)\n",
    "\n",
    "for x in range (1,investment_universe_month.shape[0]):\n",
    "    returns[x-1] = sum(performance[x])\n",
    "    equalw[x-1] = sum(eweighted[x])\n",
    "        \n",
    "\n",
    "beg = 100\n",
    "start = 100\n",
    "commence = 100\n",
    "bench = pct_benchmark_month_array[1:investment_universe_month.shape[0]]\n",
    "bmk = np.zeros((investment_universe_month.shape[0],))\n",
    "port = np.zeros((investment_universe_month.shape[0]-1,))\n",
    "ew = np.zeros((investment_universe_month.shape[0]-1,))\n",
    "\n",
    "\n",
    "for i in range (0,investment_universe_month.shape[0]-1):\n",
    "    bmk[i] = beg*(1+bench[i])\n",
    "    beg = bmk[i]\n",
    "    port[i] = start*(1+returns[i])\n",
    "    start = port[i]\n",
    "    ew[i] = commence*(1+equalw[i])\n",
    "    commence = ew[i]\n",
    "    \n",
    "plt.plot(bmk[0:investment_universe_month.shape[0]-1])\n",
    "plt.plot(port)\n",
    "plt.plot(ew)\n",
    "input_assets = index_row[index_row.shape[0]-2]\n",
    "input_assets1 = pct_investment_month_array[:, input_assets]\n",
    "dataframe = pd.DataFrame(input_assets1)\n",
    "input_assets1 = dataframe.dropna()\n",
    "mean_returns = input_assets1.mean()\n",
    "covar_matrix = input_assets1.cov()\n",
    "tickers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9,]\n",
    "def calc_portfolio_perf(weights, mean_returns, cov, rf):\n",
    "    portfolio_return = np.sum(mean_returns * weights) * 12\n",
    "    portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov, weights))) * np.sqrt(12)\n",
    "    sharpe_ratio = (portfolio_return - rf) / portfolio_std\n",
    "    return portfolio_return, portfolio_std, sharpe_ratio\n",
    "def simulate_random_portfolios(num_portfolios, mean_returns, cov, rf):\n",
    "    results_matrix = np.zeros((len(mean_returns)+3, num_portfolios))\n",
    "    for i in range(num_portfolios):\n",
    "        weights = np.random.random(len(mean_returns))\n",
    "        weights /= np.sum(weights)\n",
    "        portfolio_return, portfolio_std, sharpe_ratio = calc_portfolio_perf(weights, mean_returns, cov, rf)\n",
    "        results_matrix[0,i] = portfolio_return\n",
    "        results_matrix[1,i] = portfolio_std\n",
    "        results_matrix[2,i] = sharpe_ratio\n",
    "\n",
    "        for j in range(len(weights)):\n",
    "            results_matrix[j+3,i] = weights[j]\n",
    "\n",
    "    results_df = pd.DataFrame(results_matrix.T,columns=['ret','stdev','sharpe'] + [ticker for ticker in tickers])\n",
    "\n",
    "    return results_df\n",
    "mean_returns = input_assets1.mean()\n",
    "cov = input_assets1.cov()\n",
    "num_portfolios = 10000\n",
    "rf = 0.0\n",
    "results_frame = simulate_random_portfolios(num_portfolios, mean_returns, cov, rf)\n",
    "\n",
    "max_sharpe_port = results_frame.iloc[results_frame['sharpe'].idxmax()]\n",
    "\n",
    "min_vol_port = results_frame.iloc[results_frame['stdev'].idxmin()]\n",
    "\n",
    "max_return_port = results_frame.iloc[results_frame['ret'].idxmax()]\n",
    "\n",
    "plt.subplots(figsize=(15,10))\n",
    "plt.scatter(results_frame.stdev,results_frame.ret,c=results_frame.sharpe,cmap='RdYlBu')\n",
    "plt.xlabel('Standard Deviation')\n",
    "plt.ylabel('Returns')\n",
    "plt.colorbar()\n",
    "plt.scatter(max_sharpe_port['stdev'],max_sharpe_port['ret'],marker=(5,1,0),color='r',s=500)\n",
    "plt.scatter(min_vol_port['stdev'],min_vol_port['ret'],marker=(5,1,0),color='g',s=500)\n",
    "plt.scatter(max_return_port['stdev'],max_return_port['ret'],marker=(5,1,0),color='y',s=500)\n",
    "plt.show()\n"
    
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
