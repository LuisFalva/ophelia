{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".output_subarea.output_text.output_stream.output_stdout > pre {\n",
       "    width:max-content;\n",
       "}\n",
       ".p-Widget.jp-RenderedText.jp-OutputArea-output > pre {\n",
       "   width:max-content;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_subarea.output_text.output_stream.output_stdout > pre {\n",
    "    width:max-content;\n",
    "}\n",
    ".p-Widget.jp-RenderedText.jp-OutputArea-output > pre {\n",
    "   width:max-content;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dask.array as da\n",
    "#import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.optimize as sco\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.sql import SparkSession, Window, Row\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "from pyspark.sql.functions import lit, year, month, dayofmonth, last_day, col, array, explode, struct, udf, to_date, month, year, percent_rank, when, lag, first, avg\n",
    "from pyspark.sql.functions import sum as spark_sum, max as spark_max, min as spark_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import visualize\n",
    "from com.ophelia.OpheliaMain import Ophelia\n",
    "module_path = os.path.abspath(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from com.ophelia.wrapper import SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================================================================================\n",
      "18:38:18.251 Ophelia [INFO] ¡Hi! My name is Ophelia Vendata\n",
      "18:38:18.251 Ophelia [INFO] I am an artificial assistant for data mining & ML engine with spark\n",
      "18:38:18.251 Ophelia [INFO] Welcome to Ophelia spark miner engine\n",
      "18:38:18.251 Ophelia [INFO] Lib Version Ophelia.dev1.0\n",
      "18:38:18.251 Ophelia [WARN] V for Vendata...\n",
      "===================================================================================================\n",
      "\n",
      "18:38:18.252 Ophelia [WARN] Initializing Spark Session\n",
      "18:38:28.659 Ophelia [INFO] Spark Version: 3.0.0\n",
      "18:38:28.659 Ophelia [INFO] This Is: 'Rank Median Demo' Project\n",
      "18:38:28.660 Ophelia [INFO] Spark Context Initialized Success\n"
     ]
    }
   ],
   "source": [
    "ophelia = Ophelia(\"Rank Median Demo\")\n",
    "sc = ophelia.Spark.build_spark_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = ophelia.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## por temas de replicación, siempre es mejor convertir el archivo a leer en formato csv\n",
    "\n",
    "monthly_data_df = spark.read.parquet(\"data/master/ophelia/data/OpheliaData/analytical_base_table\")\n",
    "index_vector_csv = spark.read.csv(\"data/raw/csv/unique_dateprice_vector.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+---------+---------+---------+--------+--------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+---------+---------+---------+---------+--------+---------+---------+--------+---------+---------+--------+--------+--------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+--------+---------+---------+--------+--------+---------+---------+--------+--------+--------+---------+--------+---------+---------+---------+---------+--------+--------+--------+--------+-------+---------+---------+--------+---------+--------+----------+---------+----------+---------+--------+----------+---------+--------+--------+---------+---------+--------+--------+--------+--------+--------+---------+---------+--------+--------+--------+---------+----------+--------+--------+----------+----------+----------+----------+--------+----------+----------+----------+----------+---------+---------+--------+---------+---------+--------+--------+--------+--------+---------+----------+---------+--------+--------+---------+--------+--------+--------+--------+--------+--------+---------+--------+---------+---------+--------+---------+--------+---------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+---------+----------+----------+--------+--------+--------+---------+--------+--------+--------+--------+--------+--------+--------+---------+--------+----------+--------+--------+----------+-------+---------+----------+----------+---------+----------+--------+----------+----------+----------+----------+----------+----------+---------+----------+--------+----------+----------+----------+----------+---------+---------+---------+---------+-------+----------+----------+--------+--------+--------+-------+----------+-------+----------+----------+-------+----------+----------+-------+----------+-------+---------+----------+---------+---------+----------+---------+----------+----------+---------+------------+--------------+\n",
      "| SCOTIAG|  AXESCP|  BMERGOB| BMRGOB25|  VALUEF4|  BLKDIA7| BLKGUB1| GBMGUBL|  INVEXGU| NTEGUB13| NTEGUB15|  PRINFGU|  ST&ER1X| STERGOB| SURCETE| VECTPRE|  FONDEO| SCOTIA1|BMERLIQ28|  DINBUR1|FONSER130|    GBMF2| INBUMAX|  INVEXCP|  ACTIREN|   SUR1E|  MONEX28| BLKDIN45|VECTRF48|NTEMP+50| SBANKCP|BMERLIQ52|  BMERTES|BMRGOB254| BLKDIA55|BLKLIQ/A| BLKPLUSA|FONSER161|    PYMES|  ST&ER1P|  STER10P|   SUPER|  INVEXDX|HSBCEMP69|VECTRF71| SCOTIA2|ACTIGOB74|  AFIRPLU|AXESMP76|BLK1LIQA|HSBCMP83|INVEXMP85|NTEMP+86|PRINFMP88|PRINMAS89| VECTMD92|HSBCEMP93| ACTIMED| ALTERNA|   GBMF3|   GBMM3|HSBCCOR|    NTEDP|NTEMP+105| SURCORP|FINDE1110| SBANKMP|INVEXMP117|   DMEDIO|PRINFMP119|NTEMP+120| BONDDIA|PRINMAS125|AXESMP130|  GBMGUB| SCOTILP|  INBUREX|  PRINGLP|  SUR30E| SURBONO|  AXESLP| SCOTLPG| SCOTIMB|  NAFINTR|  PRINFTR| STEREAL|  SURUDI| FT/REAL|  VALUEF2|   VALUEF7| SCOTUDI| SCOTDOL|BBVADOL183|INVEXCO185|VECTCOB188|HSBCDOL189| SBANKDL|BBVADOL195|INVEXCO197|VECTCOB200|HSBCDOL201|  SCOT-FX|  ACTICOB| BLKDOLS|   PRGLOB|  ST&ERUS|  SURUSD| TEMGBIA| NAVIGTR| SCOTIPC|  ACTIVAR|   BMERIND|  NAFINDX|  SURIPC| VECTIND|  ACTIPAT|  BLKPAT|  BLKIPC|  GBMCRE|  GBMMOD| HSBCBOL| INVEXMX|     MAYA|  NTESEL|  PRINRVA|   SURPAT|  VECTPA|  ACTINMO| BLKFIBR|  SCOTUSA|ACTI500261|VECTSIC262|BLKUSEQ265|FRANOPR266|FRANUSA267|NTEUSA269|STERDOW270|ACTI500272|BLKUSEQ276|FRANOPR277|FRANUSA278|NTEUSA280|STERDOW281|VECTSIC282| AXESGLO| BLKINT1|  GBMTRV|  INVEXTK|  NTEGLA| SURGLOB| SCOTEUR| AXESEDM| BBVAE50| SCOTI12| GBMPCON|  PRINLS1|  SURMOD|SVIVE50310| SCOTI14| ELITE/C|GBMPMOD314| HSBCF2|HSBCF3317|PRINLS2319|SUR2026320|SURCRE321|SVIVE35322| SBANK50|DIVER/A324|ELITE/M326|GOLD4MA329|PRINLS3331|SUR2034334|SUR2042335|SURAGR337|SVIVE20338| SCOTQNT|DIVER/A340|ELITE/M342|GBMPMOD344|GOLD4MA345|  GOLD5MA|HSBCF3347|SURCRE352|SURAGR353| HSBCF4|PRINLS2357|PRINLS3358| GBMPICT| SVIVE60| SUR2018|HSBCJUB|SUR2026370|HSBC-50|SVIVE50376|SUR2034377|HSBC-60|SVIVE35383|SUR2042384|HSBC-70|SVIVE20390|HSBC-80|FINDE1400|ACTIGOB401|AXESMP402|HSBCMP405|INVEXMP407|  NAFFP28|PRINFMP410|PRINMAS411|VECTMD414|operation_id|operation_date|\n",
      "+--------+--------+---------+---------+---------+---------+--------+--------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+---------+---------+---------+---------+--------+---------+---------+--------+---------+---------+--------+--------+--------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+--------+---------+---------+--------+--------+---------+---------+--------+--------+--------+---------+--------+---------+---------+---------+---------+--------+--------+--------+--------+-------+---------+---------+--------+---------+--------+----------+---------+----------+---------+--------+----------+---------+--------+--------+---------+---------+--------+--------+--------+--------+--------+---------+---------+--------+--------+--------+---------+----------+--------+--------+----------+----------+----------+----------+--------+----------+----------+----------+----------+---------+---------+--------+---------+---------+--------+--------+--------+--------+---------+----------+---------+--------+--------+---------+--------+--------+--------+--------+--------+--------+---------+--------+---------+---------+--------+---------+--------+---------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+---------+----------+----------+--------+--------+--------+---------+--------+--------+--------+--------+--------+--------+--------+---------+--------+----------+--------+--------+----------+-------+---------+----------+----------+---------+----------+--------+----------+----------+----------+----------+----------+----------+---------+----------+--------+----------+----------+----------+----------+---------+---------+---------+---------+-------+----------+----------+--------+--------+--------+-------+----------+-------+----------+----------+-------+----------+----------+-------+----------+-------+---------+----------+---------+---------+----------+---------+----------+----------+---------+------------+--------------+\n",
      "|3.072966|2.092178| 37.45684| 33.23053| 88.22312|26.300716| 1.95828|1.706707|154.82692|11.866331|11.866331| 18.96942|18.116575|1.983867|1.901414|1.742375| 1.16055|1.771449| 38.31968|12.697705| 47.48447| 30.37494|7.454808|131.95883| 9.376384|3.514119|19.465092| 27.56146|1.539643|4.405284|7.056647| 38.31968|136.53365| 33.23053|26.300716|1.963705|13.569586| 47.48447|28.897066|129.79398| 8.988157|1.540308| 189.6237| 18.86934|1.539643|1.409345| 4.695273|218.92802|  2.1032|6.210948| 16.7168| 1.863508|4.405284|  29.4153| 18.03297|14.321876| 18.86934| 4.91374|4.125317|5.547982|7.657942|18.9883| 12.05522| 4.405284|1.886625| 2.673016|1.134562|  1.863508|267.75552|   29.4153| 4.405284|1.226102|  18.03297|   2.1032|3.493157|1.536789|28.151213|11.889734| 3.52067| 2.81948|2.438322|1.461298|1.818725|12.150584|11.908743|2.151386|1.359386|1.107471|128.97447|  97.87072|1.735279|1.920735| 20.034897| 143.80698|  8.322604|    1.5193|1.149301| 20.034897| 143.80698|  8.322604|    1.5193|27.748201|24.357655| 2.32473|18.639624| 9.123338|3.545023|3.631677|3.744682| 8.65261|101.31905|111.090034| 58.63241| 9.79584|2.480683| 14.71944|6.611951|1.599462| 8.52842|3.568791|44.20268|3.327018|43.881134|3.003416|16.124556|15.810364|1.820135|46.179626|1.419406| 9.524284|  4.323041|  0.876076|   2.63801| 10.069016|  2.982291| 1.650486|  3.882779|  4.323041|   2.63801| 10.069016|  2.982291| 1.650486|  3.882779|  0.876076|2.322321|3.233545|1.791189|  371.233|7.486327|2.705911|1.450077|2.335457|2.483382|1.706777|2.374014|20.666868|1.437404|      1.46|2.015373|9.234472|  1.377685| 1.8733|  1.61232| 23.772808|  2.193364| 1.476831|  2.420466|2.523797| 22.310843|  1.474429|  2.803377| 26.748793|  2.315325|  2.400772| 1.611859|  92.06885|1.272852| 22.310843|  1.474429|  1.377685|  2.803377|416.81702|  1.61232| 1.476831| 1.611859|1.36117| 23.772808| 26.748793|0.858121|1.798084|1.991767|1.29529|  2.193364|1.36034|      1.46|  2.315325|1.42502|  2.420466|  2.400772|1.51939|  92.06885|1.60889| 2.673016|  4.695273|   2.1032|  16.7168|  1.863508|19.075476|   29.4153|  18.03297|14.321876|        3249|    2017-06-19|\n",
      "|3.549467|2.330026|43.400417|38.498894|99.707726|30.448095|2.265777|  1.9711|175.58676|13.694536|13.694536|  21.9538|20.111618|2.276598|2.196999|2.014396|1.340169|2.055273|44.412895|14.418558|54.979317|35.144287|8.648129|147.16292|10.919004|4.068402|22.402584|31.981775|1.778233|5.099074|8.179097|44.412895|158.01695|38.498894|30.448095|2.240775|15.696513|54.979317|32.258347|145.49327|10.298179|1.782255|215.00919| 21.26473|1.778233|1.632508| 5.413126|255.64267|2.357146|7.197334|18.88594| 2.109069|5.099074|34.038563| 20.76533|16.556404| 21.26473|5.739609|4.856804|6.464071|8.922791|21.5029|14.052497| 5.099074|2.215647| 3.116824|1.304329|  2.109069|305.99896| 34.038563| 5.099074|1.404135|  20.76533| 2.357146|4.025921| 1.77225|29.779259|13.226135|4.050748|3.160909|2.725944| 1.62249|1.965529|13.934892|13.500898|2.446339|1.554853|1.275877| 131.6127|104.892235|1.971275|2.109275| 21.542103| 152.90315|   8.87286|    1.5828|1.234687| 21.542103| 152.90315|   8.87286|    1.5828|31.036797|26.468552|2.545804|20.871758| 9.758692|3.928191|4.002462|4.069326|8.086114| 92.72856| 103.43475|52.334595|9.179674|2.335146|13.244947|6.135634|1.476992|7.492207|3.096035|39.99067|3.255963| 42.16349|3.077557|15.478301|14.917057|1.725647|43.245735|1.433565|13.409715|  5.676226|  1.048234|  3.367652| 14.566511|  3.334936| 1.874713|  4.877156|  5.676226|  3.367652| 14.566511|  3.334936| 1.874713|  4.877156|  1.048234|2.639888|3.940054|2.086959|412.61597|8.330675|  3.3014|1.620676|2.392786|2.733651|1.963934|2.562686|23.742315|1.618206|      1.61|2.315294|10.35278|  1.418416|2.10095|  1.77106|  27.54454|  2.502888| 1.648673|  2.608192|2.873892| 24.291086|  1.621599|  3.105262|  30.89955|  2.622023|  2.695803| 1.764108| 98.608925|1.481554| 24.291086|  1.621599|  1.418416|  3.105262|455.84158|  1.77106| 1.648673| 1.764108|1.46677|  27.54454|  30.89955|0.964288| 2.02449|2.287158|1.45534|  2.502888|1.45025|      1.61|  2.622023|1.53291|  2.608192|  2.695803|1.62496| 98.608925|1.73526| 3.116824|  5.413126| 2.357146| 18.88594|  2.109069|21.394112| 34.038563|  20.76533|16.556404|        3776|    2019-06-27|\n",
      "|3.430472|2.271221|41.854397|37.162815| 96.83861|29.399952|2.188111| 1.90471|170.43474|13.234535|13.234535|21.201672|19.660957|  2.2033|2.123047|1.945717|1.295065|1.983017|42.866554|13.985065| 53.07464|33.968433|8.346534|143.36066|10.532464|3.930643|21.650106|30.861435|1.718209|4.922306|7.895874|42.866554| 152.6367|37.162815|29.399952|2.175866|15.160518| 53.07464|31.420595|141.54987| 9.969618|1.720882|208.66335| 20.66341|1.718209|1.573807| 5.230446|246.52875| 2.29421|6.955186|18.34266| 2.047566|4.922306|32.809284| 19.99269|15.972716| 20.66341|5.531427|4.678641|6.238922|8.616967|20.8134|13.552617| 4.922306| 2.12262| 3.000086|1.252219|  2.047566| 293.5103| 32.809284| 4.922306|1.359852|  19.99269|  2.29421|3.892355|1.673008|29.411127|12.399787|3.809762|2.942952|2.618407|1.508372|1.887009|13.423704|12.955537|2.330641|1.492547|1.208662|126.02881|  98.93845|1.825287|2.111629| 21.672516|  154.7885|   8.98327|    1.6074|1.246648| 21.672516|  154.7885|   8.98327|    1.6074|30.022045|26.565323|2.498037|20.066492| 9.731263|3.860109|3.976805| 4.03284|7.775852| 89.44317|  99.37012|50.755737|8.832318|2.240167|13.467892| 5.94868|1.433743| 8.05792|3.353256|38.73847|3.245772|41.527565|2.944178|14.964243| 14.50054|1.666596|42.094578|1.253555|11.342841|  4.993994|  0.953131|  2.966025| 12.197465|  3.096863| 1.681986|  4.402257|  4.993994|  2.966025| 12.197465|  3.096863| 1.681986|  4.402257|  0.953131|2.403927|3.514869| 1.86312|383.52353|7.679094|2.840404|1.452597|2.163886|2.485232|1.883833|2.544042|22.578445|1.521358|       1.5|2.210312|9.946236|  1.431455|2.00475|  1.67579|  25.90381|  2.379102| 1.542408|  2.443456|2.729012| 23.165976|  1.540935|  2.901586| 28.837244|  2.473673|  2.525842| 1.646505|  92.46662|1.374547| 23.165976|  1.540935|  1.431455|  2.901586| 423.9739|  1.67579| 1.542408| 1.646505| 1.3802|  25.90381| 28.837244|0.920325|1.918644|2.192515|1.41491|  2.379102|  1.401|       1.5|  2.473673|1.46386|  2.443456|  2.525842|1.54867|  92.46662|1.63664| 3.000086|  5.230446|  2.29421| 18.34266|  2.047566|20.787586| 32.809284|  19.99269|15.972716|        3654|    2019-01-08|\n",
      "|3.229475|2.169883|39.382637|34.957916| 91.96838|   27.654|2.058705|1.792861|161.61298|12.464411|12.464411|19.947027|18.777637|2.079733|1.997963|1.832411|1.219173|1.865382|40.318344|13.263115|49.946476|31.933748|7.844862|136.89429| 9.889676|3.697213|20.431322|29.008392|1.617624|4.627845|7.426217|40.318344|143.60265|34.957916|   27.654|2.055965|14.265285|49.946476|29.988983|134.91826| 9.419808|1.619764|197.91057| 19.65518|1.617624|1.483368| 4.935104|231.15742|2.187652|6.537611|17.40503| 1.944185|4.627845|30.933458|18.934467|15.052795| 19.65518|5.189287|4.367278|5.859733|8.093179|19.7456|12.720307| 4.627845|1.990259| 2.820089|1.188252|  1.944185|278.55936| 30.933458| 4.627845|1.285391| 18.934467| 2.187652|3.668149|1.600058|28.772812|12.103492|3.643553|2.872064|2.521538|1.483053|1.855516|12.759413|12.430525|2.241331|1.424382| 1.15494|125.06418| 98.005196|1.770932|2.012733| 20.835958| 149.77705|  8.687195|     1.572|1.199937| 20.835958| 149.77705|  8.687195|     1.572|28.786938|25.622587| 2.42568| 19.28932| 9.447631|3.687393|3.748631|3.924378|8.512395| 98.91665| 109.25872|56.528492|9.658842|2.442342|14.565521|6.633306|1.579163|8.399414|3.585899|42.97955|3.355936|45.577374|3.162919|16.095404|15.934086|1.839577|45.579166| 1.37228|11.771649|  5.147311|   1.03478|  3.124789| 12.813549|  3.224914| 1.721125|  4.590995|  5.147311|  3.124789| 12.813549|  3.224914| 1.721125|  4.590995|   1.03478|2.611028|3.840432|2.071645|  427.653|8.719966|3.064073|1.714477| 2.51838|2.820555|1.812325|2.466091|22.070208| 1.50973|      1.51|2.169134|9.765389|  1.416678|1.97013|  1.70129| 26.037169|  2.303781| 1.551239|  2.526909|2.752425| 23.350576|  1.561151|  3.036053| 30.111431|  2.443824|  2.547746| 1.702035|  96.79771|1.434424| 23.350576|  1.561151|  1.416678|  3.036053| 457.6769|  1.70129| 1.551239| 1.702035|1.44153| 26.037169| 30.111431|0.954775|1.857357|2.081371|1.34811|  2.303781|1.39898|      1.51|  2.443824|1.46913|  2.526909|  2.547746|1.57951|  96.79771|1.69522| 2.820089|  4.935104| 2.187652| 17.40503|  1.944185| 19.82832| 30.933458| 18.934467|15.052795|        3443|    2018-03-19|\n",
      "|3.289518| 2.20023| 40.12213|35.614075| 93.42413|28.177177|2.097473|1.826358|  164.254|12.695035|12.695035|20.324268|19.043062| 2.11677| 2.03546|1.866958|1.241918|1.900592|41.069126|13.477837| 50.88192| 32.54679|7.993721| 138.8195|10.082558|3.766534|20.798449|29.565496| 1.64772|4.714165| 7.56667|41.069126| 146.3082|35.614075|28.177177|2.091916|14.533581| 50.88192|30.416681|136.89331|  9.58423|1.649978|201.12308| 19.95976| 1.64772| 1.50818| 5.026318| 236.0007|2.219999| 6.66301|17.69849| 1.975395|4.714165| 31.50748|19.254444|15.332005| 19.95976|5.289366|4.450967| 5.97599|8.250829|20.0879|12.971182| 4.714165|2.029592| 2.871663|1.208389|  1.975395| 282.5928|  31.50748| 4.714165|1.307723| 19.254444| 2.219999|3.735061|1.619132|28.945187|12.125042| 3.68114|2.862838|2.548551|1.486212|1.857479|12.907206|12.520309|2.260973|1.437674|1.170319|124.06373|  96.00215| 1.79599|2.229241| 22.877296| 165.28043|  9.587195|    1.7312|1.326414| 22.877296| 165.28043|  9.587195|    1.7312| 31.57794|28.219912|2.658518|21.074068|10.330896|4.057953| 4.10398|4.387792|8.429311| 97.51041| 107.97455| 55.58122|9.563431|2.421883|14.311156|6.572095|1.561994|7.772203| 3.36978|42.39968|3.307408| 44.77719|3.198463|16.100882|15.799681|1.823488| 44.51625|1.350128| 13.45073|  5.770128|  1.133314|  3.488332|  14.74538|    3.5684| 1.959368|  5.023889|  5.770128|  3.488332|  14.74538|    3.5684| 1.959368|  5.023889|  1.133314|2.885191|  4.1967|2.276532|459.33603|9.396389|3.369645|1.946829|2.787408| 3.12859|1.858029|2.475152|22.915684| 1.54617|      1.54|2.240525|9.925617|  1.402024|2.01293|  1.74329| 27.116974|  2.369131| 1.591319|  2.599089|  2.8804| 23.768982|  1.592151|  3.128255| 31.627914|  2.524635|  2.642538| 1.757484| 100.14777|1.505899| 23.768982|  1.592151|  1.402024|  3.128255|473.76547|  1.74329| 1.591319| 1.757484| 1.4814| 27.116974| 31.627914|1.045251|1.878371|2.130532| 1.3688|  2.369131|1.41344|      1.54|  2.524635|1.48621|  2.599089|  2.642538|1.61289| 100.14777|1.74234| 2.871663|  5.026318| 2.219999| 17.69849|  1.975395|20.115477|  31.50748| 19.254444|15.332005|        3508|    2018-06-18|\n",
      "+--------+--------+---------+---------+---------+---------+--------+--------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+---------+---------+---------+---------+--------+---------+---------+--------+---------+---------+--------+--------+--------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+--------+---------+---------+--------+--------+---------+---------+--------+--------+--------+---------+--------+---------+---------+---------+---------+--------+--------+--------+--------+-------+---------+---------+--------+---------+--------+----------+---------+----------+---------+--------+----------+---------+--------+--------+---------+---------+--------+--------+--------+--------+--------+---------+---------+--------+--------+--------+---------+----------+--------+--------+----------+----------+----------+----------+--------+----------+----------+----------+----------+---------+---------+--------+---------+---------+--------+--------+--------+--------+---------+----------+---------+--------+--------+---------+--------+--------+--------+--------+--------+--------+---------+--------+---------+---------+--------+---------+--------+---------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+---------+----------+----------+--------+--------+--------+---------+--------+--------+--------+--------+--------+--------+--------+---------+--------+----------+--------+--------+----------+-------+---------+----------+----------+---------+----------+--------+----------+----------+----------+----------+----------+----------+---------+----------+--------+----------+----------+----------+----------+---------+---------+---------+---------+-------+----------+----------+--------+--------+--------+-------+----------+-------+----------+----------+-------+----------+----------+-------+----------+-------+---------+----------+---------+---------+----------+---------+----------+----------+---------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monthly_data_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def datetime_object(datetime_str):\n",
    "    datetime_object = datetime.strptime(datetime_str, '%d/%m/%y').date()\n",
    "    return datetime_object\n",
    "\n",
    "datetime_object_udf = udf(f=datetime_object, returnType=DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_vector_schema = index_vector_csv.select(\"MXWDU_Index\", datetime_object_udf(col(\"operation_date\")).alias('operation_date'))\n",
    "index_vector_schema.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_date_index = index_vector_schema.select(\"*\", month(col(\"operation_date\")).alias(\"month\"), year(col(\"operation_date\")).alias(\"year\"))\\\n",
    "                                      .groupBy(\"year\", \"month\").agg(spark_max(\"operation_date\").alias(\"operation_date\")).drop(\"year\", \"month\")\\\n",
    "                                      .join(index_vector_schema, on=\"operation_date\", how=\"left\")\n",
    "print(close_date_index.shape)\n",
    "close_date_index.orderBy(col(\"operation_date\").desc()).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_portfolio_df = close_date_index.join(monthly_data_df, on=\"operation_date\", how=\"left\")\n",
    "print(index_portfolio_df.shape)\n",
    "index_portfolio_df.orderBy(col(\"operation_date\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "np.random.seed(777)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolved_time_frame(df):\n",
    "    table = df.toPandas()\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for c in table.columns.values:\n",
    "        plt.plot(table.index, table[c], lw=3, alpha=0.8,label=c)\n",
    "    plt.ylabel('price in $')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolved_time_frame(index_portfolio_df.drop(\"operation_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# me ahorro este paso, ya no es necesario pasarlo a pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "monthly_data = index_portfolio_df.toPandas()\n",
    "print(\"nuestros datos:\", monthly_data.shape)\n",
    "monthly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_pd = monthly_data.drop(['operation_date', 'operation_id'], axis=1).pct_change(1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_dask = dd.from_pandas(test_pd, npartitions=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test_dask.corr().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dd.from_pandas((monthly_data.drop(['operation_date', 'operation_id'], axis=1)), npartitions=3).corr().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "operate_cols = index_portfolio_df.remove_element(['operation_date', 'operation_id'])\n",
    "pct_change_df = index_portfolio_df.pct_change(pct_cols=operate_cols, order_by=\"operation_date\").na.fill(np.nan)#.dropna(thresh=4)\n",
    "print(pct_change_df.shape)\n",
    "pct_change_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "\n",
    "#from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dd.read_parquet('data/master/ophelia/data/OpheliaData/analytical_base_table').to_dask_array().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def __spark_to_dask(self):\n",
    "#    tmp_path = os.getcwd() + '/data/tmp/'\n",
    "#    self.coalesce(1).write.mode('overwrite').parquet(tmp_path)\n",
    "#    return dd.read_parquet(tmp_path)\n",
    "\n",
    "#def add_to_series():\n",
    "#    def spark_to_series(self, column_series):\n",
    "#        dask_df = __spark_to_dask(self)\n",
    "#        series = dask_df[column_series]\n",
    "#        list_dask = series.to_delayed()\n",
    "#        full = [da.from_delayed(i, i.compute().shape, i.compute().dtype) for i in list_dask]\n",
    "#        return da.concatenate(full)\n",
    "#    DataFrame.toPandasSeries = spark_to_series\n",
    "#    \n",
    "#def add_to_numpy():\n",
    "#    def spark_to_numpy(self, column_series):\n",
    "#        add_to_series()\n",
    "#        dask_array = self.toPandasSeries(column_series)\n",
    "#        return da.from_array(dask_array.compute())\n",
    "#    DataFrame.toNumpyArray = spark_to_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_to_series()\n",
    "#add_to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_benchmark_month.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pct_benchmark_month.toPandasSeries('MXWDU_Index').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_benchmark_month.toNumpyArray('MXWDU_Index').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## seleccionamos la primera colu.ipynb_checkpoints/na y lo convertimos a vector, esta columna representa el indice que estamos analizando, \n",
    "## 'MXWDU_Index', la idea es medir esta columna con el resto, ya que las demás son acciones que forman parte de ése índice.\n",
    "\n",
    "#benchmark_month = monthly_data[\"MXWDU_Index\"]\n",
    "\n",
    "# nuestros datos\n",
    "#print(benchmark_month.shape)\n",
    "#benchmark_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Calculamos el porcentaje de cambio de un día contra otro.\n",
    "## fórmula: (t+1 / t)-1\n",
    "## pseudo-código: (precio_hoy / precio_ayer)-1\n",
    "\n",
    "#pct_benchmark_month = benchmark_month.pct_change(1)\n",
    "#pct_benchmark_month\n",
    "\n",
    "# nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## el vector de \"percentage change\" se convierte a un arreglo numpy de dimensión (147,)\n",
    "## NOTA: en los arreglos (objetos) de tipo numpy.array preservan los valores, tanto por fila, como por columna,\n",
    "##       el órden de los elementos, cómo un 'índice implícito'\n",
    "\n",
    "#pct_benchmark_month_array = np.array(pct_benchmark_month)\n",
    "#print(pct_benchmark_month_array.shape)\n",
    "#print(pct_benchmark_month_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_benchmark_month = pct_change_df.select('MXWDU_Index')\n",
    "print(pct_benchmark_month.shape)\n",
    "pct_benchmark_month.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## el vector de \"percentage change\" se convierte a un arreglo numpy de dimensión (147,)\n",
    "## NOTA: en los arreglos (objetos) de tipo numpy.array preservan los valores, tanto por fila, como por columna,\n",
    "##       el órden de los elementos, cómo un 'índice implícito'\n",
    "\n",
    "pct_benchmark_month_array = pct_benchmark_month.toNumpyArray('MXWDU_Index')\n",
    "print(pct_benchmark_month_array.shape)\n",
    "pct_benchmark_month_array.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Lo mismo hacemos, pero para el resto de variables equity,\n",
    "## seleccionamos la primera columna y lo convertimos a vector, esta columna representa el indice que estamos analizando, \n",
    "## 'MXWDU_Index', la idea es medir esta columna con el resto, ya que las demás son acciones que forman parte de ese índice.\n",
    "\n",
    "#investment_universe_month = monthly_data.drop(['operation_id', \"operation_date\", \"MXWDU_Index\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## (t+1 / t)-1\n",
    "## (precio_hoy / precio_ayer)-1\n",
    "\n",
    "#pct_investment_month = investment_universe_month.pct_change(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## el vector de percentage change se convierte a un arreglo numpy de dimensión (147,70)\n",
    "\n",
    "#pct_investment_month_array = np.array(pct_investment_month)\n",
    "##_pct_investment_month_array = np.array(_pct_investment_month)\n",
    "#\n",
    "## nuestros datos\n",
    "#print(pct_investment_month_array.shape)\n",
    "#print(pct_investment_month_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_investment_month = pct_change_df.drop('MXWDU_Index')\n",
    "print(pct_investment_month.shape)\n",
    "pct_investment_month.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## el vector de percentage change se convierte a un arreglo numpy de dimensión (147,70)\n",
    "\n",
    "#pct_investment_month_array = np.array(pct_investment_month)\n",
    "#_pct_investment_month_array = np.array(_pct_investment_month)\n",
    "\n",
    "pct_investment_month_array = pct_investment_month.toNumpyArray()\n",
    "# nuestros datos\n",
    "print(pct_investment_month_array.compute().shape)\n",
    "print(pct_investment_month_array.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creamos arreglos numpy con dimensiones X+1 = 148, rellenas de ceros, para ser imputados con nuevos vectores\n",
    "\n",
    "up_month = np.zeros((pct_benchmark_month_array.shape[0]+1, 1))\n",
    "down_month = np.zeros((pct_benchmark_month_array.shape[0]+1, 1))\n",
    "up_move = np.zeros((pct_benchmark_month_array.shape[0]+1, pct_investment_month_array.shape[1]))\n",
    "down_move = np.zeros((pct_benchmark_month_array.shape[0]+1, pct_investment_month_array.shape[1]))\n",
    "\n",
    "print(up_month.shape)\n",
    "print(down_month.shape)\n",
    "print(up_move.shape)\n",
    "print(down_move.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## rellenamos las matrices de ceros con valores que aprueben las condiciones, \n",
    "## se realiza una comparación dentro de los arreglos de porcentajes de cambio, \n",
    "## sí alguno de esos porcentajes es superior a 0, entonces entra a los arreglos \n",
    "## de movimientos positivos (incrementos), pero sí alguno es menor que 0, entonces\n",
    "## el porcentaje se almacena en los arreglos de movimientos negativos (decrementos).\n",
    "\n",
    "## Básicamente, se separan los porcentajes de cambio en dos matrices: \n",
    "## matriz de positivos cuando el porcentaje es > 0 \n",
    "## matriz de negativos cuando el porcentaje es <= 0\n",
    "\n",
    "size_benchmark_matrix = pct_benchmark_month_array.shape[0]\n",
    "for i in range (1, size_benchmark_matrix):\n",
    "    if pct_benchmark_month_array.compute()[i] > 0:\n",
    "        up_month[i] = pct_benchmark_month_array.compute()[i]\n",
    "        up_move[i] = pct_investment_month_array.compute()[i, 0:pct_investment_month_array.shape[1]]\n",
    "    else:\n",
    "        down_month[i] = pct_benchmark_month_array.compute()[i]\n",
    "        down_move[i] = pct_investment_month_array.compute()[i, 0:pct_investment_month_array.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(up_month.shape)\n",
    "print(down_month.shape)\n",
    "print(up_move.shape)\n",
    "print(down_move.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_month_dask = da.from_array(up_month)\n",
    "up_move_dask = da.from_array(up_move)\n",
    "down_month_dask = da.from_array(down_month)\n",
    "down_move_dask = da.from_array(down_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculamos los vectores 'peor más alto' y 'mejor más alto'\n",
    "\n",
    "#np.seterr(divide='ignore', invalid='ignore')\n",
    "#greater_worse = down_move / down_month\n",
    "#greater_better = (up_move / up_month) * float(-1.0)\n",
    "#\n",
    "#print(greater_worse.shape)\n",
    "#print(greater_better.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculamos los vectores 'peor más alto' y 'mejor más alto'\n",
    "\n",
    "#np.seterr(divide='ignore', invalid='ignore')\n",
    "greater_worse = down_move_dask / down_month_dask\n",
    "greater_better = (up_move_dask / up_month_dask) * float(-1.0)\n",
    "\n",
    "print(greater_worse.shape)\n",
    "print(greater_better.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greater_worse.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ddf = dd.concat([dd.from_dask_array(c) for c in [c1,c2,c3]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import h5py\n",
    "#f = h5py.File('myfile.hdf5', 'a')\n",
    "#d = f.require_dataset('/data', shape=up_month_dask.shape, dtype=up_month_dask.dtype)\n",
    "#da.store(up_month_dask, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ambos vectores los convertimos a pandas dataframes, y solo nos quedamos con los vectores que tengan valores != np.nan\n",
    "## una de las ventajas de los pandas dataframes es que mantienen un ídince único por row, esto lo hace poder separarse, y juntarse\n",
    "## en cuantos sub-conjuntos se requieran y siempre se podrá mantener un órden.\n",
    "\n",
    "greater_worse_df = greater_worse.to_dask_dataframe().dropna().compute()\n",
    "greater_better_df = greater_better.to_dask_dataframe().dropna().compute()\n",
    "\n",
    "print(greater_worse_df.shape)\n",
    "print(greater_better_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(df, f):\n",
    "    return df[f(df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#greater_better_df.mask(lambda x: x[0] < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "greater_worse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ambos vectores los convertimos a pandas dataframes, y solo nos quedamos con los vectores que tengan valores != np.nan\n",
    "## una de las ventajas de los pandas dataframes es que mantienen un ídince único por row, esto lo hace poder separarse, y juntarse\n",
    "## en cuantos sub-conjuntos se requieran y siempre se podrá mantener un órden.\n",
    "\n",
    "#greater_worse_df = dd.from_pandas(data=greater_worse).dropna()\n",
    "#greater_better_df = dd.from_pandas(data=greater_better).dropna()\n",
    "#\n",
    "#print(greater_worse_df.shape)\n",
    "#print(greater_better_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculamos ahora, la mediana acumulada con los pandas dataframes que construimos, \n",
    "## con un periodo mínimo (método expanding) de al menos 1 observación dada.\n",
    "\n",
    "median_down = greater_worse_df.expanding().median()\n",
    "median_up = greater_better_df.expanding().median()\n",
    "\n",
    "print(median_down.shape)\n",
    "print(median_up.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#greater_worse_df.expanding().cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se transponen ambos pandas df por la columna periodos, columna que almacena números no consecutivos desde 1 hasta 147\n",
    "\n",
    "down_transpose = median_down.T\n",
    "up_transpose = median_up.T\n",
    "\n",
    "print(down_transpose.shape)\n",
    "print(up_transpose.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se rankean los resultados (top 10) entre las fechas cierre (periodo) y se vuelve a transponer la tabla ranked_down\n",
    "\n",
    "ranked_down = down_transpose.rank(method='first')\n",
    "transpose_ranked_down = ranked_down.T\n",
    "\n",
    "print(ranked_down.shape)\n",
    "print(transpose_ranked_down.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se rankean los resultados (top 10) entre las fechas cierre (periodo) y se vuelve a transponer la tabla ranked_up\n",
    "\n",
    "ranked_up = up_transpose.rank(method='first')\n",
    "transpose_ranked_up = ranked_up.T\n",
    "\n",
    "print(ranked_up.shape)\n",
    "print(transpose_ranked_up.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transpose_ranked_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## se añade variable 'label' con la idea de que al juntar ambos dataframes se puedan distinguir los 'worse' de los 'better'\n",
    "## y se unen ambos dataframes con la etiqueta creada, se usó el método 'insert' por lo que no se deberá correr de nuevo, una\n",
    "## vez ejecutado ya que fallará por duplicidad de columnas.\n",
    "\n",
    "worse_better_pd = pd.concat([transpose_ranked_up, transpose_ranked_down]).sort_index()\n",
    "print(worse_better_pd.shape)\n",
    "worse_better_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## se crea un índice 'closing_id' para cada registro, éste corre de [1:N+1] \n",
    "## con la idea de etiquetar el id del mes de registro de cierre,\n",
    "## de la misma forma que lo anterior, NO se deberá ejecutar de nuevo; una vez hecho.\n",
    "\n",
    "worse_better_pd['closing_id'] = range(1, len(worse_better_pd) + 1)\n",
    "print(worse_better_pd.shape)\n",
    "worse_better_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se transponen ambos dataframes, de antes tener una dimensión (68, 70), es decir; \n",
    "## 68 registros i.e. 'Rows' (variables)\n",
    "## 70 columnas fijas (a menos que sea añadido otro asset desde el csv inicial)\n",
    "\n",
    "## a tener una dimensión 'transpuesta' (invertída sí querés...) de (70, 68), es decir;\n",
    "## 70 registros i.e. 'Rows' fijos (a menos que sea añadido otro asset desde el csv inicial)\n",
    "## 68 columnas (variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_name_list = index_portfolio_df.drop('operation_id', \"operation_date\", \"MXWDU_Index\").columns\n",
    "print(asset_name_list, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## de pandas dataframes, una vez separados en dos conjuntos ['worse', 'better'],\n",
    "## creamos por separado dos spark dataframes.\n",
    "\n",
    "worse_better_df = spark.createDataFrame(worse_better_pd)\n",
    "worse_better_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = dict(zip((worse_better_df.columns), asset_name_list))\n",
    "mapped_name_df = worse_better_df.select([col(c).alias(mapping.get(c, c)) for c in worse_better_df.columns])\n",
    "print(mapped_name_df.shape)\n",
    "mapped_name_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapped_name_df.coalesce(1).write.mode(\"overwrite\").parquet(\"data/ophelia/out/engine/RankMedian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finish Rank Median Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start TopAssetRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worse_better = spark.read.parquet(\"data/ophelia/out/engine/RankMedian/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se crea un generador \"shape long format\", una lista con iteraciones, esta lista trabajará con dos variables principales,\n",
    "## 1-. la variable 'equity_index', será la que contenga los 'id' de los activos\n",
    "## 2-. la variable 'median_down', será la que contenga la mediana acumulada por cada activo.\n",
    "## Se mantendrá a lo largo de la transformación 1 columna fija; 'closing_id',\n",
    "## closing_id: variable que indica el mes de cierre y reporte de precio\n",
    "\n",
    "def panel_format(df, pivot_col, new_columns: list = []):\n",
    "    first_col = str(new_columns[0])\n",
    "    second_col = str(new_columns[1])\n",
    "    piv_col = [pivot_col]\n",
    "    df_types = df.dtypes\n",
    "    cols, dtype = zip(*[(c, t) for (c, t) in df_types if c not in piv_col])\n",
    "    if len(set(dtype)) > 1:\n",
    "        raise ValueError(\"Columns not the same data type...\")\n",
    "    generator_explode = explode(array([\n",
    "        struct(lit(c).alias(first_col), col(c).alias(second_col)) for c in cols\n",
    "    ])).alias(\"column_explode\")\n",
    "    column_to_explode = [\"column_explode.\"+first_col, \"column_explode.\"+second_col]\n",
    "    panel_df = df.select(piv_col + [generator_explode])\\\n",
    "                 .select(piv_col + column_to_explode)\n",
    "    return panel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worse_better.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se crea nuevo spark-dataframe donde solo se mostrará por partición ['closing_id'] \n",
    "## el top 10 mejores meses donde tuvo menos malos que el resto de los registros; [\"top_rank\"].\n",
    "new_columns = [\"ticker\", \"rank\"]\n",
    "asset_ranking_df = worse_better.transpose(\"closing_id\", new_columns)\n",
    "print(asset_ranking_df.shape)\n",
    "asset_ranking_df.printSchema()\n",
    "asset_ranking_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "map_asset_id = dict(zip(asset_name_list, worse_better_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, create_map, lit\n",
    "from itertools import chain\n",
    "\n",
    "mapping_expr = create_map([lit(x) for x in chain(*map_asset_id.items())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_rank_10 = asset_ranking_df.where(col(\"rank\") <= 10).select('*', mapping_expr.getItem(col('ticker')).alias('ticker_id')).orderBy(\"closing_id\", \"rank\")\n",
    "asset_rank_10.coalesce(1).write.mode(\"overwrite\").parquet(\"data/ophelia/out/engine/LongMedianRank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_rank_10.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finish TopAssetRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start TrainPortfolioSimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_rank_10_pd = pd.read_parquet(\"data/ophelia/out/engine/LongMedianRank/\")\n",
    "asset_rank_10_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buscar el asset_id de la matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = asset_rank_10_pd.astype({'ticker_id':'int32'}) # le damos el tipo de dato int32 a la columna ticker\n",
    "newselect = asset_rank_10_pd[[\"closing_id\", \"ticker_id\"]] # seleccionamos solo estas dos columnas\n",
    "num_of_assets = 10 # numero de assets que se crearan\n",
    "\n",
    "indexed = np.zeros((investment_universe_month.shape[0], num_of_assets))\n",
    "\n",
    "for q in range(1,investment_universe_month.shape[0]):\n",
    "    selection = newselect.loc[pandas_df[\"closing_id\"]==q]\n",
    "    newselect_transpose = selection.T\n",
    "    newpdf = newselect_transpose['ticker_id':].head()\n",
    "    indexed[q] = newpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lo que se pretende ahora, es obtener una matriz de dimensión (146, 10), es decir, \n",
    "## tener en cada row las fechas de cierre [closing_id],\n",
    "## en cada columna (header) el número de ranking top 10 [top_rank],\n",
    "## y en cada campo, el id del activo [asset_id].\n",
    "\n",
    "pandas_df = asset_rank_10_pd.astype({'ticker_id':'int32'}) # le damos el tipo de dato int32 a la columna ticker\n",
    "newselect = asset_rank_10_pd[[\"closing_id\", \"ticker_id\"]] # seleccionamos solo estas dos columnas\n",
    "num_of_assets = 10 # numero de assets que se crearan\n",
    "investment_universe_month = index_portfolio_df.drop('operation_id', \"operation_date\", \"MXWDU_Index\")\n",
    "indexed = np.zeros((investment_universe_month.shape[0], num_of_assets))\n",
    "\n",
    "for q in range(1,investment_universe_month.shape[0]):\n",
    "    selection = newselect.loc[pandas_df[\"closing_id\"]==q]\n",
    "    newselect_transpose = selection.T\n",
    "    newpdf = newselect_transpose['ticker_id':].head()\n",
    "    indexed[q] = newpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_investment_month_array.compute().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_row.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investment_universe_month.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed = indexed[1:investment_universe_month.shape[0]+1]\n",
    "index_row = indexed.astype(np.int64)\n",
    "newrow = [0]*num_of_assets #np.zeros((1,num_of_assets))\n",
    "index_row = np.vstack([index_row, newrow])\n",
    "portfolio = np.zeros((investment_universe_month.shape[0], num_of_assets))\n",
    "\n",
    "for r in range(0, investment_universe_month.shape[0]-1):\n",
    "    s = r+1\n",
    "    print(s)\n",
    "    columns = index_row[s]\n",
    "    print(columns)\n",
    "    portfolio[s] = pct_investment_month_array.compute()[s,[columns]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = portfolio[1:investment_universe_month.shape[0]-1]\n",
    "performance = np.dot(portfolio,(1/num_of_assets))\n",
    "returns = np.zeros((investment_universe_month.shape[0]-1,1))\n",
    "equalw = np.zeros((investment_universe_month.shape[0]-1,1))\n",
    "eweights = 1/(pct_investment_month_array.shape[1])\n",
    "eweighted = np.dot(pct_investment_month_array,eweights)\n",
    "\n",
    "for x in range (1,investment_universe_month.shape[0]):\n",
    "    returns[x-1] = sum(performance[x])\n",
    "    equalw[x-1] = sum(eweighted[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beg = 100\n",
    "start = 100\n",
    "commence = 100\n",
    "bench = pct_benchmark_month_array[1:investment_universe_month.shape[0]]\n",
    "bmk = np.zeros((investment_universe_month.shape[0],))\n",
    "port = np.zeros((investment_universe_month.shape[0]-1,))\n",
    "ew = np.zeros((investment_universe_month.shape[0]-1,))\n",
    "\n",
    "for i in range (0,investment_universe_month.shape[0]-1):\n",
    "    bmk[i] = beg*(1+bench[i])\n",
    "    beg = bmk[i]\n",
    "    port[i] = start*(1+returns[i])\n",
    "    start = port[i]\n",
    "    ew[i] = commence*(1+equalw[i])\n",
    "    commence = ew[i]\n",
    "\n",
    "plt.plot(bmk[0:investment_universe_month.shape[0]-1])\n",
    "plt.plot(port)\n",
    "plt.plot(ew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_row[index_row.shape[0]-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_assets = index_row[index_row.shape[0]-2]\n",
    "input_assets1 = pct_investment_month_array[:, input_assets]\n",
    "dataframe = pd.DataFrame(input_assets1)\n",
    "input_assets1 = dataframe.dropna()\n",
    "mean_returns = input_assets1.mean()\n",
    "covar_matrix = input_assets1.cov()\n",
    "tickers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "def calc_portfolio_perf(weights, mean_returns, cov, rf):\n",
    "    portfolio_return = np.sum(mean_returns * weights) * 12\n",
    "    portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov, weights))) * np.sqrt(12)\n",
    "    sharpe_ratio = (portfolio_return - rf) / portfolio_std\n",
    "    return portfolio_return, portfolio_std, sharpe_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_random_portfolios(num_portfolios, mean_returns, cov, rf):\n",
    "    results_matrix = np.zeros((len(mean_returns)+3, num_portfolios))\n",
    "    for i in range(num_portfolios):\n",
    "        weights = np.random.random(len(mean_returns))\n",
    "        weights /= np.sum(weights)\n",
    "        portfolio_return, portfolio_std, sharpe_ratio = calc_portfolio_perf(weights, mean_returns, cov, rf)\n",
    "        results_matrix[0,i] = portfolio_return\n",
    "        results_matrix[1,i] = portfolio_std\n",
    "        results_matrix[2,i] = sharpe_ratio\n",
    "\n",
    "        for j in range(len(weights)):\n",
    "            results_matrix[j+3,i] = weights[j]\n",
    "\n",
    "    results_df = pd.DataFrame(results_matrix.T, columns=['ret','stdev','sharpe'] + [ticker for ticker in tickers])\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: revisar la sobrecarga de memoria en los stages marcados\n",
    "\n",
    "mean_returns = input_assets1.mean()\n",
    "cov = input_assets1.cov()\n",
    "num_portfolios = 210000\n",
    "rf = 0.0\n",
    "\n",
    "results_frame = simulate_random_portfolios(num_portfolios, mean_returns, cov, rf)\n",
    "\n",
    "max_sharpe_port = results_frame.iloc[results_frame['sharpe'].idxmax()]\n",
    "\n",
    "min_vol_port = results_frame.iloc[results_frame['stdev'].idxmin()]\n",
    "\n",
    "max_return_port = results_frame.iloc[results_frame['ret'].idxmax()]\n",
    "\n",
    "plt.subplots(figsize=(15,10))\n",
    "plt.scatter(results_frame.stdev, results_frame.ret, c=results_frame.sharpe, cmap='RdYlBu')\n",
    "plt.xlabel('Standard Deviation')\n",
    "plt.ylabel('Returns')\n",
    "plt.colorbar()\n",
    "plt.scatter(max_sharpe_port['stdev'], max_sharpe_port['ret'], marker=(5,1,0), color='r', s=500)\n",
    "plt.scatter(min_vol_port['stdev'], min_vol_port['ret'], marker=(5,1,0), color='g', s=500)\n",
    "plt.scatter(max_return_port['stdev'], max_return_port['ret'], marker=(5,1,0), color='y', s=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_columns = {\n",
    "    0: 'fund_w_zero',\n",
    "    1: 'fund_w_one',\n",
    "    2: 'fund_w_two',\n",
    "    3: 'fund_w_three',\n",
    "    4: 'fund_w_four',\n",
    "    5: 'fund_w_five',\n",
    "    6: 'fund_w_six',\n",
    "    7: 'fund_w_seven',\n",
    "    8: 'fund_w_eight',\n",
    "    9: 'fund_w_nine'\n",
    "}\n",
    "results_frame_renamed = results_frame.rename(columns=renamed_columns)\n",
    "results_frame_renamed.to_csv('data/ophelia/out/model/tmp/TrainSimulation.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_series_to_dict(pd_series):\n",
    "    to_python_dict = dict(pd_series)\n",
    "    return {str(k): float(v) for k, v in to_python_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_map_rename(dic, new_key, old_key):\n",
    "    for key in range(len(old_key)):\n",
    "        dic[new_key[key]] = dic.pop(old_key[key])\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_metadata_model(dic_sample, json_name):\n",
    "    dict_to_json = spark.read.json(sc.parallelize([dic_sample]))\n",
    "    dict_to_json.coalesce(1).write.mode(\"overwrite\").json(\"data/ophelia/out/model/model_info/\"+str(json_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_return_port_sample = pd_series_to_dict(results_frame_renamed.iloc[results_frame_renamed['ret'].idxmax()])\n",
    "persist_metadata_model(max_return_port_sample, \"maxReturnTrainWeights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sharpe_port_sample = pd_series_to_dict(results_frame_renamed.iloc[results_frame_renamed['sharpe'].idxmax()])\n",
    "persist_metadata_model(max_sharpe_port_sample, \"maxSharpeTrainWeights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_vol_port_sample = pd_series_to_dict(results_frame_renamed.iloc[results_frame_renamed['stdev'].idxmax()])\n",
    "persist_metadata_model(min_vol_port_sample, \"minVolatileTrainWeights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finish TrainPortfolioSimulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start EfficientFrontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def json_read(json_path):\n",
    "    with open(json_path) as json_file:\n",
    "        return(json.load(json_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_p = 'data/ophelia/out/model/model_info/minVolatileTrainWeights/part-00000-7579777d-ef23-4098-9b2b-5c6a44cf1dfe-c000.json'\n",
    "min_vol_port_sample = json_read(json_p)\n",
    "min_vol_port_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_p = 'data/ophelia/out/model/model_info/minVolatileTrainWeights/part-00000-6a7870c6-f591-4274-92c9-d1fa36fd0813-c000.json'\n",
    "#min_vol_port_sample = json_read(json_p)\n",
    "#min_vol_port_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_spark_df = spark.read.csv(\"data/ophelia/out/model/tmp/TrainSimulation.csv\", header='true', inferSchema=True)\n",
    "results_spark_df.show(5, False)\n",
    "results_spark_df.describe().show(10, False)\n",
    "results_spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, current_date, current_timestamp\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "results_index = results_spark_df.select(\"*\", \n",
    "                                        (monotonically_increasing_id() + 1000).alias(\"portfolio_id\"),\n",
    "                                        current_date().alias(\"information_date\"),\n",
    "                                        current_timestamp().alias(\"model_date\"))\n",
    "results_index.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con x portafolios\n",
    "\n",
    "filter_min_sharpe_df = results_index.where(col(\"sharpe\") >= min_vol_port_sample['sharpe'])\\\n",
    "                                    .select(\"*\", percent_rank().over(Window.orderBy(results_index['sharpe'])).alias(\"sharpe_centile\"))\n",
    "filter_min_sharpe_df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for quantile discretization we use two different techniques:\n",
    "- QuantileDiscretizer from pysspark ml\n",
    "- Percentile Bucketizer non library used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bucketer = QuantileDiscretizer(\n",
    "    numBuckets=5,\n",
    "    inputCol=\"sharpe\",\n",
    "    outputCol='{}_bucket'.format('sharpe')).fit(filter_min_sharpe_df).transform(filter_min_sharpe_df)\n",
    "df_bucketer.groupBy('sharpe_bucket').agg(avg(\"sharpe\"), avg(\"ret\")).orderBy(col(\"sharpe_bucket\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "percentile_bucketizer = filter_min_sharpe_df.select(\"*\", \n",
    "                                             when(col(\"sharpe_centile\") < 0.2, lit(5.0)).otherwise(\n",
    "                                                 when((col(\"sharpe_centile\") >= 0.2) & (col(\"sharpe_centile\") < 0.4), lit(4.0)).otherwise(\n",
    "                                                     when((col(\"sharpe_centile\") >= 0.4) & (col(\"sharpe_centile\") < 0.6), lit(3.0)).otherwise(\n",
    "                                                         when((col(\"sharpe_centile\") >= 0.6) & (col(\"sharpe_centile\") < 0.8), lit(2.0)).otherwise(\n",
    "                                                             when((col(\"sharpe_centile\") >= 0.8) & (col(\"sharpe_centile\") <= 1.0), lit(1.0)))))).alias(\"sharpe_bucket\"))\n",
    "percentile_bucketizer.groupBy(\"sharpe_bucket\").agg(avg(\"sharpe\"), avg(\"ret\")).orderBy(\"sharpe_bucket\").show()\n",
    "percentile_bucketizer.show(10, False)\n",
    "percentile_bucketizer.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# el más rapido\n",
    "percentile_bucketizer.coalesce(1).write.mode(\"overwrite\").parquet('data/ophelia/out/model/TrainPortfolio/', partitionBy=\"information_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_restuls_frame = pd.read_parquet(\"data/ophelia/out/model/TrainPortfolio/\")\n",
    "\n",
    "vol_arr = pd_restuls_frame.stdev\n",
    "ret_arr = pd_restuls_frame.ret\n",
    "sharpe_arr = pd_restuls_frame.sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_returns = input_assets1.mean()\n",
    "cov = input_assets1.cov()\n",
    "\n",
    "def get_ret_vol_sr(weights):\n",
    "    weights = np.array(weights)\n",
    "    ret = np.sum(mean_returns * weights) * 12\n",
    "    vol = np.sqrt(np.dot(weights.T, np.dot(cov, weights))) * np.sqrt(12)\n",
    "    sr = ret/vol\n",
    "    return np.array([ret, vol, sr])\n",
    "\n",
    "def neg_sharpe(weights):\n",
    "    # the number 2 is the sharpe ratio index from the get_ret_vol_sr\n",
    "    return get_ret_vol_sr(weights)[2] * -1\n",
    "\n",
    "def check_sum(weights):\n",
    "    #return 0 if sum of the weights is 1\n",
    "    return np.sum(weights)-1\n",
    "\n",
    "cons = ({'type':'eq', 'fun':check_sum})\n",
    "bound = (0.0, .1)\n",
    "num_assets = 10\n",
    "bounds = tuple(bound for asset in range(num_assets))\n",
    "init_guess = [.1,.1,.1,.1,.1,.1,.1,.1,.1,.1]\n",
    "opt_results = sco.minimize(neg_sharpe, init_guess, method='SLSQP', bounds=bounds, constraints=cons)\n",
    "print(\"opt_results:\", opt_results)\n",
    "\n",
    "get_ret_vol_sr(opt_results.x)\n",
    "\n",
    "range_vol_min = min(vol_arr)\n",
    "range_vol_max = max(vol_arr)\n",
    "range_ret_min = min(ret_arr)\n",
    "range_ret_max = max(ret_arr)\n",
    "range_sharpe_min = min(sharpe_arr)\n",
    "range_sharpe_max = max(sharpe_arr)\n",
    "\n",
    "frontier_y = np.linspace(range_ret_min, range_ret_max, 200)\n",
    "frontier_x = []\n",
    "\n",
    "def minimize_volatility(weights):\n",
    "    return get_ret_vol_sr(weights)[1]\n",
    "\n",
    "for possible_return in frontier_y:\n",
    "    cons = ({'type':'eq', 'fun': check_sum},\n",
    "            {'type':'eq', 'fun': lambda w: get_ret_vol_sr(w)[0] - possible_return})\n",
    "    result = sco.minimize(minimize_volatility, init_guess, method='SLSQP', bounds=bounds, constraints=cons)\n",
    "    frontier_x.append(result['fun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(vol_arr, ret_arr, c=sharpe_arr, cmap='RdYlBu')\n",
    "plt.colorbar(label='Sharpe Ratio')\n",
    "plt.xlabel('Volatility')\n",
    "plt.ylabel('Return')\n",
    "plt.plot(frontier_x, frontier_y, 'r--', linewidth=3)\n",
    "# plt.savefig('cover.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ophelia",
   "language": "python",
   "name": "ophelia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
