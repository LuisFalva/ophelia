{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualize\n",
    "from com.ophelia.OpheliaVendata import OpheliaVendata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ophelia = OpheliaVendata('Risk Recommend Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, max as spark_max, row_number, avg, count\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import OpSpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = ophelia.opSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_train_df = spark.read.parquet(\"data/ophelia/out/model/TrainPortfolio\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(portfolio_train_df.shape)\n",
    "portfolio_train_df.show(5, False)\n",
    "portfolio_train_df.printSchema()\n",
    "portfolio_train_df.describe(\"sharpe\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_max_sharpe = portfolio_train_df.join(\n",
    "    portfolio_train_df.groupBy('risk_bucket').agg(spark_max('sharpe').alias('max_sharpe')),\n",
    "    on=[col('sharpe') == col('max_sharpe')],\n",
    "    how='left_semi').orderBy('risk_bucket')\n",
    "filter_max_sharpe.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_class_df = spark.read.parquet(\"data/ophelia/out/model/RiskClassifier\")\n",
    "print(risk_class_df.shape)\n",
    "risk_class_df.show(5, False)\n",
    "risk_class_df.groupBy('risk_label', 'risk_label_id').count().orderBy(col('risk_label_id')).show(5, False)\n",
    "risk_class_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_class_col_prune = [\"information_date\", \"model_date\"]\n",
    "portfolio_train_col_prune = [\"information_date\", \"model_date\"]\n",
    "join_over_col = [col(\"risk_label_id\")==col(\"risk_bucket\")]\n",
    "join_col_prune = [\"risk_bucket\", \"risk_label_id\"]\n",
    "\n",
    "portfolio_base_table = risk_class_df.drop(*risk_class_col_prune)\\\n",
    "                                    .join(filter_max_sharpe.drop(*portfolio_train_col_prune), on=join_over_col, how=\"inner\")\\\n",
    "                                    #.drop(*join_col_prune)\n",
    "portfolio_base_table.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(portfolio_base_table.shape)\n",
    "portfolio_base_table.printSchema()\n",
    "portfolio_base_table.groupBy('risk_bucket', 'risk_label_id', 'risk_label', 'portfolio_id')\\\n",
    "                    .agg(count('risk_label'), avg('sharpe'), avg('stdev'), \n",
    "                         avg('ret')).orderBy('avg(sharpe)').show()\n",
    "portfolio_base_table.groupBy(\"portfolio_id\").count().show()\n",
    "portfolio_base_table.orderBy('customer_id').show(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_assembled = VectorAssembler(inputCols=['ret', 'stdev', 'sharpe'], outputCol='features').transform(portfolio_base_table)\n",
    "vec_assembled.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans().setK(5).setSeed(1)\n",
    "model_kmeans = kmeans.fit(vec_assembled)\n",
    "transformed = model_kmeans.transform(vec_assembled)\n",
    "transformed.groupBy(\"prediction\")\\\n",
    "           .agg(count('risk_label'), count('prediction'), avg('sharpe'), avg('stdev'), avg('ret')).orderBy('avg(sharpe)').show()\n",
    "transformed.groupBy(\"prediction\", 'risk_label')\\\n",
    "           .agg(count('risk_label'), count('prediction'), avg('sharpe'), avg('stdev'), avg('ret')).orderBy('avg(sharpe)').show()\n",
    "transformed.groupBy(\"prediction\", 'risk_label', 'risk_label_id', 'risk_bucket', 'portfolio_id')\\\n",
    "           .agg(count('risk_label'), count('prediction'), avg('sharpe'), avg('stdev'), avg('ret')).orderBy('avg(sharpe)').show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cambiar el index del benchmark\n",
    "- hacer back-testing con datos actuales\n",
    "- simular portafolios con constraint min 10% por fondo\n",
    "- revisar que porcentaje de equity tiene cada portafolio\n",
    "- pendiente definir constraint de equity por portafolio perfil\n",
    "- nuevo cuestionario de perfilamiento\n",
    "- pendiente definir benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now we can see cluster to verify that cluster splitting was difined correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(transformed)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model_kmeans.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_zero = transformed.where(col('prediction') == '0').toNumpyArray()\n",
    "cluster_one = transformed.where(col('prediction') == '1')\n",
    "cluster_two = transformed.where(col('prediction') == '2')\n",
    "cluster_three = transformed.where(col('prediction') == '3')\n",
    "cluster_four = transformed.where(col('prediction') == '4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter1 = dict(\n",
    "    mode = \"markers\",\n",
    "    name = \"Cluster 1\",\n",
    "    type = \"scatter3d\",    \n",
    "    x = cluster_zero.as_matrix()[:,0], y = cluster_zero.as_matrix()[:,1], z = cluster_zero.as_matrix()[:,2],\n",
    "    marker = dict( size=2, color='green')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "class Kmeans:\n",
    "    '''Implementing Kmeans algorithm.'''\n",
    "\n",
    "    def __init__(self, n_clusters, max_iter=100, random_state=123):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def initializ_centroids(self, X):\n",
    "        np.random.RandomState(self.random_state)\n",
    "        random_idx = np.random.permutation(X.shape[0])\n",
    "        centroids = X[random_idx[:self.n_clusters]]\n",
    "        return centroids\n",
    "\n",
    "    def compute_centroids(self, X, labels):\n",
    "        centroids = np.zeros((self.n_clusters, X.shape[1]))\n",
    "        for k in range(self.n_clusters):\n",
    "            centroids[k, :] = np.mean(X[labels == k, :], axis=0)\n",
    "        return centroids\n",
    "\n",
    "    def compute_distance(self, X, centroids):\n",
    "        distance = np.zeros((X.shape[0], self.n_clusters))\n",
    "        for k in range(self.n_clusters):\n",
    "            row_norm = norm(X - centroids[k, :], axis=1)\n",
    "            distance[:, k] = np.square(row_norm)\n",
    "        return distance\n",
    "\n",
    "    def find_closest_cluster(self, distance):\n",
    "        return np.argmin(distance, axis=1)\n",
    "\n",
    "    def compute_sse(self, X, labels, centroids):\n",
    "        distance = np.zeros(X.shape[0])\n",
    "        for k in range(self.n_clusters):\n",
    "            distance[labels == k] = norm(X[labels == k] - centroids[k], axis=1)\n",
    "        return np.sum(np.square(distance))\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.centroids = self.initializ_centroids(X)\n",
    "        for i in range(self.max_iter):\n",
    "            old_centroids = self.centroids\n",
    "            distance = self.compute_distance(X, old_centroids)\n",
    "            self.labels = self.find_closest_cluster(distance)\n",
    "            self.centroids = self.compute_centroids(X, self.labels)\n",
    "            if np.all(old_centroids == self.centroids):\n",
    "                break\n",
    "        self.error = self.compute_sse(X, self.labels, self.centroids)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        distance = self.compute_distance(X, old_centroids)\n",
    "        return self.find_closest_cluster(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df = portfolio_train_df.select('ret', 'stdev', 'sharpe').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import pandas as pd\n",
    "from sklearn.datasets.samples_generator import (make_blobs,\n",
    "                                                make_circles,\n",
    "                                                make_moons)\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "X_std = StandardScaler().fit_transform(df)\n",
    "\n",
    "# Run local implementation of kmeans\n",
    "km = Kmeans(n_clusters=2, max_iter=100)\n",
    "km.fit(X_std)\n",
    "centroids = km.centroids\n",
    "\n",
    "# Plot the clustered data\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "plt.scatter(X_std[km.labels == 0, 0], X_std[km.labels == 0, 1],\n",
    "            c='green', label='cluster 1')\n",
    "plt.scatter(X_std[km.labels == 1, 0], X_std[km.labels == 1, 1],\n",
    "            c='blue', label='cluster 2')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=300,\n",
    "            c='r', label='centroid')\n",
    "plt.legend()\n",
    "plt.xlim([-5, 5])\n",
    "plt.ylim([-5, 5])\n",
    "plt.xlabel('Eruption time in mins')\n",
    "plt.ylabel('Waiting time to next eruption')\n",
    "plt.title('Visualization of clustered data', fontweight='bold')\n",
    "ax.set_aspect('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = []\n",
    "list_k = list(range(1, 10))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(X_std)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pd.DataFrame(centroids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ophelia",
   "language": "python",
   "name": "ophelia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
