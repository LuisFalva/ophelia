{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".output_subarea.output_text.output_stream.output_stdout > pre {\n",
       "    width:max-content;\n",
       "}\n",
       ".p-Widget.jp-RenderedText.jp-OutputArea-output > pre {\n",
       "   width:max-content;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_subarea.output_text.output_stream.output_stdout > pre {\n",
    "    width:max-content;\n",
    "}\n",
    ".p-Widget.jp-RenderedText.jp-OutputArea-output > pre {\n",
    "   width:max-content;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, lag, first, count, max as spark_max, min as spark_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import visualize\n",
    "from com.ophelia.OpheliaMain import Ophelia\n",
    "module_path = os.path.abspath(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================================================================================\n",
      "21:00:51.464 Ophelia [INFO] Â¡Hi! My name is Ophelia Vendata\n",
      "21:00:51.465 Ophelia [INFO] I am an artificial assistant for data mining & ML engine with spark\n",
      "21:00:51.465 Ophelia [INFO] Welcome to Ophelia spark miner engine\n",
      "21:00:51.465 Ophelia [INFO] Lib Version Ophelia.dev1.0\n",
      "21:00:51.465 Ophelia [WARN] V for Vendata...\n",
      "===================================================================================================\n",
      "\n",
      "21:00:51.465 Ophelia [WARN] Initializing Spark Session\n",
      "21:00:51.490 Ophelia [INFO] Spark Version: 3.0.0\n",
      "21:00:51.490 Ophelia [INFO] This Is: 'Risk Classification Demo' Project\n",
      "21:00:51.490 Ophelia [INFO] Spark Context Initialized Success\n"
     ]
    }
   ],
   "source": [
    "ophelia = Ophelia(\"Risk Classification Demo\")\n",
    "sc = ophelia.Spark.build_spark_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = ophelia.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 3)\n",
      "+-------+----+-------+\n",
      "|Product|Year|Revenue|\n",
      "+-------+----+-------+\n",
      "|      A|2010|    100|\n",
      "|      B|2010|    200|\n",
      "|      C|2010|    300|\n",
      "|      A|2011|    110|\n",
      "|      B|2011|    190|\n",
      "|      C|2011|    320|\n",
      "|      A|2012|    120|\n",
      "|      B|2012|    220|\n",
      "|      C|2012|    350|\n",
      "+-------+----+-------+\n",
      "\n",
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Revenue: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from com.ophelia.wrapper import SparkWrapper, string_match, union_all\n",
    "from com.ophelia.utils import regex_expr\n",
    "\n",
    "dic = {\n",
    "    'Product': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C'],\n",
    "    'Year': [2010, 2010, 2010, 2011, 2011, 2011, 2012, 2012, 2012],\n",
    "    'Revenue': [100, 200, 300, 110, 190, 320, 120, 220, 350]\n",
    "}\n",
    "dic_to_df = spark.createDataFrame(pd.DataFrame(data=dic))\n",
    "print(dic_to_df.Shape)\n",
    "dic_to_df.show()\n",
    "dic_to_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Shape* SparkWrapper:\n",
    "### The shape wrapper is added to the Spark DataFrame class in order to have the must commonly used method in Pandas and Numpy type objects, this is pretty useful when you want to track the dimension of the Spark DataFrame at some spaecific transformation stage and get an insight of what your rows and columns number are gathering into different dimensions.\n",
    "> Important note: *shape* method is called as the traditional **.shape** of Pandas an Numpy objects.\n",
    "\n",
    "### It returns: \n",
    "- Tuple with: (total row number, total column number), *such as (n, m) matrix dimension*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_to_df.Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *pctChange* from SparkWrapper:\n",
    "### The pct_change wrapper is added to the Spark DataFrame class in order to have the must commonly used method in Pandas objects, this is for getting the relative percentage change between one observation to another sorted by some sortable date-type column and lagged by some laggable numeric-type column. \n",
    "> Important note: you can call *pct_change* method as the traditional **.pct_change** way for Pandas dataframe objects or you can rather specify the parameters of the function. So if any parameter is specified then the method will infere which column to sort and which column to lag in order to get the **relative percentage change**.\n",
    "\n",
    "### It returns: \n",
    "- Tuple with: (total row number, total column number), *such as (n, m) matrix dimension*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One of the many options that we can use this *.pctChange* method is with no parameter specified thus it will infere which column to sort and which column to lag in order to get the **relative percentage change**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|Revenue            |\n",
      "+-------------------+\n",
      "|null               |\n",
      "|1.0                |\n",
      "|0.5                |\n",
      "|-0.6333333333333333|\n",
      "|0.7272727272727273 |\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dic_to_df.pctChange().show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another option is configuring all recept parameters from the function, the following are:\n",
    "- **periods**; this parameter will control the offset of the lag periods since the default value is 1 this will always return a lag-1 information DataFrame\n",
    "- **partition_by**; the partition parameter will fixed the partition column over the DataFrame e.g. _\"bank_segment\", \"assurance_product_type\"_\n",
    "- **order_by**; order by parameter will be the specific column to order the sequential observations, e.g. _\"balance_date\", \"trade_close_date\", \"contract_date\"_\n",
    "- **pct_cols**; percentage change col (pct_cols) will be the spacific column to lag-over giving back the relative change between one element to other, e.g. *$(x_{t} \\div{x_{t-1}})$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this case we will specify only the **periods** parameter to yield a lag of -2 days over the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.pctChange(periods=2).na.fill(0).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With parameters **partition_by, order_by & pct_cols**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.pctChange(partition_by=\"Product\", order_by=\"Year\", pct_cols=\"Revenue\").na.fill(0).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.pctChange(partition_by=\"Product\", order_by=\"Year\", pct_cols=[\"Year\", \"Revenue\"]).na.fill(0).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_change_df = dic_to_df.pctChange(partition_by=\"Product\", order_by=\"Year\", pct_cols=\"Revenue\").na.fill(0)\n",
    "print(pct_change_df.Shape)\n",
    "pct_change_df.show(5, False)\n",
    "pct_change_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Matrix* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4)\n",
      "+------------+----------------+----------------+----------------+\n",
      "|Product_Year|2010_Revenue_max|2011_Revenue_max|2012_Revenue_max|\n",
      "+------------+----------------+----------------+----------------+\n",
      "|A           |100             |110             |120             |\n",
      "|B           |200             |190             |220             |\n",
      "|C           |300             |320             |350             |\n",
      "+------------+----------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#+-------+----+-------+\n",
    "#|Product|Year|Revenue|\n",
    "#+-------+----+-------+\n",
    "#|      A|2010|    100|\n",
    "#|      B|2010|    200|\n",
    "#|      C|2010|    300|\n",
    "#|      A|2011|    110|\n",
    "#|      B|2011|    190|\n",
    "#|      C|2011|    320|\n",
    "#|      A|2012|    120|\n",
    "#|      B|2012|    220|\n",
    "#|      C|2012|    350|\n",
    "#+-------+----+-------+\n",
    "\n",
    "agg_dict_test = {'Revenue': 'max'}\n",
    "to_matrix_df = dic_to_df.toMatrix(group_by=\"Product\", pivot_col=\"Year\", agg_dict=agg_dict_test).orderBy(\"Product_Year\")\n",
    "print(to_matrix_df.Shape)\n",
    "to_matrix_df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Panel* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_matrix_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-accd625ce2d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mto_matrix_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPanel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpivot_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Product_Year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_Revenue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'to_matrix_df' is not defined"
     ]
    }
   ],
   "source": [
    "to_matrix_df.toPanel(pivot_col='Product_Year', new_col='mean_Revenue').show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Cartesian* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_matrix_df.cartRDD('Product_Year').take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *CorrMat* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "to_matrix_df.corrMatrix().show()\n",
    "end = time.time()\n",
    "print('elapsed:', int(end - start), 'segundos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_matrix_df.corrMatrix(offset=0.9).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *corrStat* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.corrStat('Year', 'Product', agg_dict_test).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *uniqueRow* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.uniqueRow('Product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.uniqueRow('Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *vecAssembler* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_matrix_df.vecAssembler(to_matrix_df.columns[1:]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Join Small* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_matrix_df.joinSmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.ml.linalg import Vectors\n",
    "#from pyspark.ml.stat import ChiSquareTest\n",
    "#dataset = [[0, Vectors.dense([0, 0, 1])],\n",
    "#           [0, Vectors.dense([1, 0, 1])],\n",
    "#           [1, Vectors.dense([2, 1, 1])],\n",
    "#           [1, Vectors.dense([3, 1, 1])]]\n",
    "#dataset = spark.createDataFrame(dataset, [\"label\", \"features\"])\n",
    "#dataset.show(5, False)\n",
    "#chiSqResult = ChiSquareTest.test(dataset, 'features', 'label')\n",
    "#chiSqResult.show(5, False)\n",
    "#chiSqResult.select(\"degreesOfFreedom\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test con mÃ¡s datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "|age|job       |marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|deposit|\n",
      "+---+----------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "|59 |admin.    |married|secondary|no     |2343   |yes    |no  |unknown|5  |may  |1042    |1       |-1   |0       |unknown |yes    |\n",
      "|56 |admin.    |married|secondary|no     |45     |no     |no  |unknown|5  |may  |1467    |1       |-1   |0       |unknown |yes    |\n",
      "|41 |technician|married|secondary|no     |1270   |yes    |no  |unknown|5  |may  |1389    |1       |-1   |0       |unknown |yes    |\n",
      "|55 |services  |married|secondary|no     |2476   |yes    |no  |unknown|5  |may  |579     |1       |-1   |0       |unknown |yes    |\n",
      "|54 |admin.    |married|tertiary |no     |184    |no     |no  |unknown|5  |may  |673     |2       |-1   |0       |unknown |yes    |\n",
      "+---+----------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_csv = spark.read.csv('data/raw/csv/bank.csv', header=True, inferSchema=True)\n",
    "bank_csv.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127106153880.0"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 127106153880.0\n",
    "\n",
    "array = [((i+2)/4)*i for i in range(1, 1000000)]\n",
    "array[713037]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryHelperSearch(array, target, left_p, right_p):\n",
    "    if left_p > right_p:\n",
    "        return -1\n",
    "    mid_point = (left_p + right_p) // 2\n",
    "    potential_match = array[mid_point]\n",
    "    if target == potential_match:\n",
    "        return mid_point\n",
    "    elif target < potential_match:\n",
    "        return binaryHelperSearch(array, target, left_p, mid_point - 1)\n",
    "    else:\n",
    "        return binaryHelperSearch(array, target, mid_point + 1, right_p)\n",
    "\n",
    "def binarySearch(array, target):\n",
    "    return binaryHelperSearch(array, target, 0, len(array) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "713037"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarySearch(array, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def __rand_int_suffix(col, n_rows):\n",
    "    random_suffix = str(randint(1, n_rows))\n",
    "    return col + '_' + random_suffix\n",
    "\n",
    "def skewness_reducer(col_to_rand, n_rows):\n",
    "    udf_rand_suffix = udf(__rand_int_suffix, StringType())\n",
    "    return udf_rand_suffix(col_to_rand, lit(n_rows)).alias('rand_id')\n",
    "\n",
    "def join_skewness(self, df, on, how):\n",
    "    remove_skewness_df = df.select('*', skewness_reducer(on, df.))\n",
    "    remove_skewness_self\n",
    "    return self.join(df, on, how)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+--------------+\n",
      "|age|        job| marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|deposit|       rand_id|\n",
      "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+--------------+\n",
      "| 59|     admin.| married|secondary|     no|   2343|    yes|  no|unknown|  5|  may|    1042|       1|   -1|       0| unknown|    yes|secondary_6916|\n",
      "| 56|     admin.| married|secondary|     no|     45|     no|  no|unknown|  5|  may|    1467|       1|   -1|       0| unknown|    yes|secondary_6308|\n",
      "| 41| technician| married|secondary|     no|   1270|    yes|  no|unknown|  5|  may|    1389|       1|   -1|       0| unknown|    yes|secondary_8919|\n",
      "| 55|   services| married|secondary|     no|   2476|    yes|  no|unknown|  5|  may|     579|       1|   -1|       0| unknown|    yes|secondary_7061|\n",
      "| 54|     admin.| married| tertiary|     no|    184|     no|  no|unknown|  5|  may|     673|       2|   -1|       0| unknown|    yes| tertiary_6751|\n",
      "| 42| management|  single| tertiary|     no|      0|    yes| yes|unknown|  5|  may|     562|       2|   -1|       0| unknown|    yes|tertiary_10828|\n",
      "| 56| management| married| tertiary|     no|    830|    yes| yes|unknown|  6|  may|    1201|       1|   -1|       0| unknown|    yes| tertiary_8554|\n",
      "| 60|    retired|divorced|secondary|     no|    545|    yes|  no|unknown|  6|  may|    1030|       1|   -1|       0| unknown|    yes|secondary_7522|\n",
      "| 37| technician| married|secondary|     no|      1|    yes|  no|unknown|  6|  may|     608|       1|   -1|       0| unknown|    yes|secondary_1745|\n",
      "| 28|   services|  single|secondary|     no|   5090|    yes|  no|unknown|  6|  may|    1297|       3|   -1|       0| unknown|    yes|secondary_7073|\n",
      "| 38|     admin.|  single|secondary|     no|    100|    yes|  no|unknown|  7|  may|     786|       1|   -1|       0| unknown|    yes|secondary_7196|\n",
      "| 30|blue-collar| married|secondary|     no|    309|    yes|  no|unknown|  7|  may|    1574|       2|   -1|       0| unknown|    yes|secondary_8811|\n",
      "| 29| management| married| tertiary|     no|    199|    yes| yes|unknown|  7|  may|    1689|       4|   -1|       0| unknown|    yes| tertiary_8385|\n",
      "| 46|blue-collar|  single| tertiary|     no|    460|    yes|  no|unknown|  7|  may|    1102|       2|   -1|       0| unknown|    yes|  tertiary_487|\n",
      "| 31| technician|  single| tertiary|     no|    703|    yes|  no|unknown|  8|  may|     943|       2|   -1|       0| unknown|    yes| tertiary_5150|\n",
      "| 35| management|divorced| tertiary|     no|   3837|    yes|  no|unknown|  8|  may|    1084|       1|   -1|       0| unknown|    yes| tertiary_1884|\n",
      "| 32|blue-collar|  single|  primary|     no|    611|    yes|  no|unknown|  8|  may|     541|       3|   -1|       0| unknown|    yes|  primary_9162|\n",
      "| 49|   services| married|secondary|     no|     -8|    yes|  no|unknown|  8|  may|    1119|       1|   -1|       0| unknown|    yes|secondary_3423|\n",
      "| 41|     admin.| married|secondary|     no|     55|    yes|  no|unknown|  8|  may|    1120|       2|   -1|       0| unknown|    yes|secondary_9214|\n",
      "| 49|     admin.|divorced|secondary|     no|    168|    yes| yes|unknown|  8|  may|     513|       1|   -1|       0| unknown|    yes|secondary_1298|\n",
      "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_csv.select('*', skewness_reducer('education', bank_csv.Shape[0])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Sort Columns Asc* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.sortColAsc().show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FreqItems from Pyspark Vs freq_items from SparkWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.freqItems(['job', 'marital', 'education'], support=0.5).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.sampleBy('marital', fractions={'married':0.5, 'single':0.5, 'divorced':0.5}).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.sample(withReplacement=True, fraction=0.5).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Sample N* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.sampleN(20).show(30, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#from itertools import chain\n",
    "#from pyspark.sql.functions import create_map, lit\n",
    "#\n",
    "#map_values = {\n",
    "#    'no': '0',\n",
    "#    'yes': '1'\n",
    "#}\n",
    "#mapping_expr = create_map([lit(x) for x in chain(*map_values.items())])\n",
    "#test = bank_csv.select('*', (mapping_expr[bank_csv['deposit']]).alias('bin_deposit'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Select Regex, Select Contains & Regex Expr* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.selectRegex(regex_expr(['day', 'ous', 'pr'])).show(5, False)\n",
    "bank_csv.selectContains(['day', 'ous', 'pr']).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dict_ = {\n",
    "    'balance': 'mean, count, sum, stddev, var, min',\n",
    "    'housing': 'count, sum',\n",
    "    'loan': 'count',\n",
    "    'age': 'mean'\n",
    "}\n",
    "embedded_rep_test = bank_csv.toMatrix('education', 'deposit', agg_dict_)\n",
    "embedded_rep_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Cosstab* de Pyspark Vs Matrix de SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.where(string_match('contact == telephone')).crosstab('education', 'campaign').orderBy('education_campaign').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agg_dict_ = {\n",
    "    'balance': 'count'\n",
    "}\n",
    "bank_csv.where(string_match('contact == telephone')).toMatrix('education', 'campaign', test_agg_dict_).orderBy('education_campaign').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *crossPct* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_agg_dict_ = {\n",
    "    'balance': 'mean',\n",
    "    'age': 'mean',\n",
    "    'loan': 'count'\n",
    "}\n",
    "test = bank_csv.crossPct('education', 'deposit', _agg_dict_, cols='all')\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tra = test.toPanel('education_deposit', ['label', 'value'])\n",
    "tra.show(50, False)\n",
    "tra.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Select StartsWith & EndsWith* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.selectStartswith('m').show(5, False)\n",
    "bank_csv.selectStartswith(['m', 'd']).show(5, False)\n",
    "\n",
    "bank_csv.selectEndswith('y').show(5, False)\n",
    "bank_csv.selectEndswith(['y', 'h']).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    bank_csv.sampleN(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *ForEach Col* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_3_dict = {\n",
    "    'balance': 'mean',\n",
    "    'housing': 'count',\n",
    "    'age': 'mean'\n",
    "}\n",
    "bank_csv.foreachCol('education', 'deposit', test_3_dict, 'sum').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1247.3143 + 2232.0278"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *ResumeDF & tabularTable* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, functions as f\n",
    "\n",
    "def resume_dataframe(self, group_by=None, new_col=None):\n",
    "    cols_types = [k for k, v in self.dtypes if v != 'string']\n",
    "    if group_by is None:\n",
    "        try:\n",
    "            agg_df = self.agg(*[f.sum(c).alias(c) for c in cols_types])\n",
    "            return agg_df.withColumn(new_col, f.lit('+++ total')).select(new_col, *cols_types)\n",
    "        except Exception as e:\n",
    "            raise AssertionError(f\"empty expression found. {e}\")\n",
    "    return self.groupBy(group_by).agg(*[f.sum(c).alias(c) for c in cols_types])\n",
    "DataFrame.resumeDF = resume_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __expression(cols_list, expr):\n",
    "    expr_dict = {\n",
    "        'sum': '+'.join(cols_list),\n",
    "        'sub': '-'.join(cols_list),\n",
    "        'mul': '*'.join(cols_list),\n",
    "        'div': '/'.join(cols_list),\n",
    "    }\n",
    "    return expr_dict[expr]\n",
    "\n",
    "def foreach_col(self, group_by, pivot_col, agg_dict, oper):\n",
    "    func = []\n",
    "    regex_keys = list(agg_dict.keys())\n",
    "    regex_values = list(agg_dict.values())\n",
    "    df = self.toMatrix(group_by, pivot_col, agg_dict)\n",
    "    for i in range(len(regex_keys)):\n",
    "        cols_list = df.selectRegex(regex_expr(regex_keys[i])).columns\n",
    "        expression = f.expr(__expression(cols_list, oper))\n",
    "        func.append(expression.alias(f'{regex_keys[i]}_{regex_values[i]}_{oper}'))\n",
    "    return df.select('*', *func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tab_table(self, group_by, pivot_col, agg_dict, oper='sum'):\n",
    "    sum_by_col_df = foreach_col(self, group_by, pivot_col, agg_dict, oper)\n",
    "    return sum_by_col_df.union(resume_dataframe(sum_by_col_df, new_col=self.columns[0]))\n",
    "DataFrame.tabularTable = tab_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.resumeDF(group_by='education').show()\n",
    "bank_csv.resumeDF(new_col='education').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_4_dict = {\n",
    "    'balance': 'mean, sum',\n",
    "    'age': 'mean',\n",
    "    'duration': 'mean'\n",
    "}\n",
    "bank_csv.tabularTable('education', 'deposit', test_4_dict).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Empty Scan* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.emptyScan().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Union All* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('before union:', bank_csv.count())\n",
    "u = union_all([bank_csv, bank_csv, bank_csv, bank_csv, bank_csv])\n",
    "print('after union:', u.count())\n",
    "u.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.rollingDown('balance', 'month', method='mean', window=20).show(25, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.rollingDown('marital', 'month', method='mean', window=5).show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df_orders.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_orders.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#from functools import reduce\n",
    "#\n",
    "#\n",
    "#list_comp = [test.groupBy(i).count().sort(f.col(\"count\").desc()).limit(1).select(f.lit(i).alias(\"col\"), f.col(i).alias(\"mode\")) for i in test.columns]\n",
    "#mode = reduce(lambda a, b: a.union(b), list_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#mode.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# multiplicaciÃ³n de df's cuadradas\n",
    "# multiplicaciÃ³n de df x df_vector\n",
    "# multiplicaciÃ³n de escalar x df y df_vector\n",
    "# todos usaran el metodo transpose para hacer las multiplicaciones columnares siempre\n",
    "# mÃ©todo split train, test que calcule el tamaÃ±o de la muestra y genere de eso el dataset de train y test\n",
    "# muestreo estratificado aleatorio\n",
    "# toMatrix generando la mediana por cortes [0.5, 0.25, 0.75]\n",
    "# toMatrix generando la moda\n",
    "# separador de strings en columnas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ophelia",
   "language": "python",
   "name": "ophelia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
