{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_subarea.output_text.output_stream.output_stdout > pre {\n",
    "    width:max-content;\n",
    "}\n",
    ".p-Widget.jp-RenderedText.jp-OutputArea-output > pre {\n",
    "   width:max-content;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lag, first, count, max as spark_max, min as spark_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Spark Wrapper Demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrapper import SparkWrapper, string_match, union_all, regex_expr\n",
    "\n",
    "dic = {\n",
    "    'Product': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C'],\n",
    "    'Year': [2010, 2010, 2010, 2011, 2011, 2011, 2012, 2012, 2012],\n",
    "    'Revenue': [100, 200, 300, 110, 190, 320, 120, 220, 350]\n",
    "}\n",
    "dic_to_df = spark.createDataFrame(pd.DataFrame(data=dic))\n",
    "print(dic_to_df.Shape)\n",
    "dic_to_df.show()\n",
    "dic_to_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Shape* SparkWrapper:\n",
    "### The shape wrapper is added to the Spark DataFrame class in order to have the must commonly used method in Pandas and Numpy type objects, this is pretty useful when you want to track the dimension of the Spark DataFrame at some spaecific transformation stage and get an insight of what your rows and columns number are gathering into different dimensions.\n",
    "> Important note: *shape* method is called as the traditional **.shape** of Pandas an Numpy objects.\n",
    "\n",
    "### It returns: \n",
    "- Tuple with: (total row number, total column number), *such as (n, m) matrix dimension*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *pctChange* from SparkWrapper:\n",
    "### The pct_change wrapper is added to the Spark DataFrame class in order to have the must commonly used method in Pandas objects, this is for getting the relative percentage change between one observation to another sorted by some sortable date-type column and lagged by some laggable numeric-type column. \n",
    "> Important note: you can call *pct_change* method as the traditional **.pct_change** way for Pandas dataframe objects or you can rather specify the parameters of the function. So if any parameter is specified then the method will infere which column to sort and which column to lag in order to get the **relative percentage change**.\n",
    "\n",
    "### It returns: \n",
    "- Tuple with: (total row number, total column number), *such as (n, m) matrix dimension*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One of the many options that we can use this *.pctChange* method is with no parameter specified thus it will infere which column to sort and which column to lag in order to get the **relative percentage change**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.pctChange().show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another option is configuring all recept parameters from the function, the following are:\n",
    "- **periods**; this parameter will control the offset of the lag periods since the default value is 1 this will always return a lag-1 information DataFrame\n",
    "- **partition_by**; the partition parameter will fixed the partition column over the DataFrame e.g. _\"bank_segment\", \"assurance_product_type\"_\n",
    "- **order_by**; order by parameter will be the specific column to order the sequential observations, e.g. _\"balance_date\", \"trade_close_date\", \"contract_date\"_\n",
    "- **pct_cols**; percentage change col (pct_cols) will be the spacific column to lag-over giving back the relative change between one element to other, e.g. *$(x_{t} \\div{x_{t-1}})$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this case we will specify only the **periods** parameter to yield a lag of -2 days over the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.pctChange(periods=2).na.fill(0).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With parameters **partition_by, order_by & pct_cols**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.pctChange(partition_by=\"Product\", order_by=\"Year\", pct_cols=\"Revenue\").na.fill(0).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.pctChange(partition_by=\"Product\", order_by=\"Year\", pct_cols=[\"Year\", \"Revenue\"]).na.fill(0).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_change_df = dic_to_df.pctChange(partition_by=\"Product\", order_by=\"Year\", pct_cols=\"Revenue\").na.fill(0)\n",
    "print(pct_change_df.Shape)\n",
    "pct_change_df.show(5, False)\n",
    "pct_change_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Matrix* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#+-------+----+-------+\n",
    "#|Product|Year|Revenue|\n",
    "#+-------+----+-------+\n",
    "#|      A|2010|    100|\n",
    "#|      B|2010|    200|\n",
    "#|      C|2010|    300|\n",
    "#|      A|2011|    110|\n",
    "#|      B|2011|    190|\n",
    "#|      C|2011|    320|\n",
    "#|      A|2012|    120|\n",
    "#|      B|2012|    220|\n",
    "#|      C|2012|    350|\n",
    "#+-------+----+-------+\n",
    "\n",
    "agg_dict_test = {'Revenue': 'max'}\n",
    "to_matrix_df = dic_to_df.toMatrix(group_by=\"Product\", pivot_col=\"Year\", agg_dict=agg_dict_test).orderBy(\"Product_Year\")\n",
    "print(to_matrix_df.Shape)\n",
    "to_matrix_df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Panel* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_matrix_df.toPanel(pivot_col='Product_Year', new_col='mean_Revenue').show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Cartesian* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_matrix_df.cartRDD('Product_Year').take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *CorrMat* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "to_matrix_df.corrMatrix().show()\n",
    "end = time.time()\n",
    "print('elapsed:', int(end - start), 'segundos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_matrix_df.corrMatrix(offset=0.9).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *corrStat* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.corrStat('Year', 'Product', agg_dict_test).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *uniqueRow* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.uniqueRow('Product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_df.uniqueRow('Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *vecAssembler* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_matrix_df.vecAssembler(to_matrix_df.columns[1:]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Join Small* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_matrix_df.joinSmall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test con más datos:\n",
    "\n",
    "### Para fines de ejemplos con más variables se usa el csv de bank el cual contiene información de las campañas de marketing de una institución financiera portuguesa (nota: descargar el comprimido [bank.zip](https://archive.ics.uci.edu/ml/machine-learning-databases/00222/)) que pertenece al compendio de datasets para ML que dispone el repositorio de la universidad de UCI donde se puede encontrar el [diccionario de datos](https://archive.ics.uci.edu/ml/datasets/bank+marketing) para investigación en el campo de modelos de aprendizaje automático."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv = spark.read.csv('bank.csv', header=True, inferSchema=True)\n",
    "bank_csv.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Sort Columns Asc* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.sortColAsc().show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FreqItems from Pyspark Vs freq_items from SparkWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.freqItems(['job', 'marital', 'education'], support=0.5).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bank_csv.sampleBy('marital', fractions={'married':0.5, 'single':0.5, 'divorced':0.5}).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bank_csv.sample(withReplacement=True, fraction=0.5).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Sample N* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.sampleN(20).show(30, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Select Regex, Select Contains & Regex Expr* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.selectRegex(regex_expr(['day', 'ous', 'pr'])).show(5, False)\n",
    "bank_csv.selectContains(['day', 'ous', 'pr']).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dict_ = {\n",
    "    'balance': 'mean, count, sum, stddev, var, min',\n",
    "    'housing': 'count, sum',\n",
    "    'loan': 'count',\n",
    "    'age': 'mean'\n",
    "}\n",
    "embedded_rep_test = bank_csv.toMatrix('education', 'deposit', agg_dict_)\n",
    "embedded_rep_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Cosstab* de Pyspark Vs Matrix de SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.where(string_match('contact == telephone')).crosstab('education', 'campaign').orderBy('education_campaign').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agg_dict_ = {\n",
    "    'balance': 'count'\n",
    "}\n",
    "bank_csv.where(string_match('contact == telephone')).toMatrix('education', 'campaign', test_agg_dict_).orderBy('education_campaign').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *crossPct* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_agg_dict_ = {\n",
    "    'balance': 'mean',\n",
    "    'age': 'mean',\n",
    "    'loan': 'count'\n",
    "}\n",
    "test = bank_csv.crossPct('education', 'deposit', _agg_dict_, cols='all')\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tra = test.toPanel('education_deposit', ['label', 'value'])\n",
    "tra.show(50, False)\n",
    "tra.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Select StartsWith & EndsWith* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.selectStartswith('m').show(5, False)\n",
    "bank_csv.selectStartswith(['m', 'd']).show(5, False)\n",
    "\n",
    "bank_csv.selectEndswith('y').show(5, False)\n",
    "bank_csv.selectEndswith(['y', 'h']).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    bank_csv.sampleN(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *ForEach Col* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_3_dict = {\n",
    "    'balance': 'mean',\n",
    "    'housing': 'count',\n",
    "    'age': 'mean'\n",
    "}\n",
    "test_3 = bank_csv.toMatrix('education', 'deposit', test_3_dict)\n",
    "test_3.foreachCol(test_3_dict, 'sum').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *ResumeDF & tabularTable* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_4_dict = {\n",
    "    'balance': 'mean',\n",
    "    'age': 'mean'\n",
    "}\n",
    "test_4 = bank_csv.toMatrix('education', 'deposit', test_4_dict)\n",
    "sum_df = test_4.foreachCol(test_4_dict, 'sum')\n",
    "\n",
    "sum_df.resumeDF(new_col='education_deposit').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.tabularTable('education', 'deposit', test_4_dict).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Empty Scan* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.emptyScan().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Union All* from SparkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('before union:', bank_csv.count())\n",
    "u = union_all([bank_csv, bank_csv, bank_csv, bank_csv, bank_csv])\n",
    "print('after union:', u.count())\n",
    "u.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_csv.rollingDown('balance', 'month', method='mean', window=20).show(25, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ophelia",
   "language": "python",
   "name": "ophelia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
