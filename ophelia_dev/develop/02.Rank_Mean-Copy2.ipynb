{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_subarea.output_text.output_stream.output_stdout > pre {\n",
    "    width:max-content;\n",
    "}\n",
    ".p-Widget.jp-RenderedText.jp-OutputArea-output > pre {\n",
    "   width:max-content;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.optimize as sco\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.sql import SparkSession, Window, Row\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "from pyspark.sql.functions import lit, year, month, dayofmonth, last_day, col, array, explode, struct, udf, to_date, month, year, percent_rank, when\n",
    "from pyspark.sql.functions import sum as spark_sum, max as spark_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Rank_Median').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## por temas de replicación, siempre es mejor convertir el archivo a leer en formato csv\n",
    "\n",
    "monthly_data = spark.read.parquet(\"data/master/ophelia/data/OpheliaData/analytical_base_table\")\n",
    "index_vector = spark.read.csv(\"data/raw/csv/unique_dateprice_vector.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def datetime_object(datetime_str):\n",
    "    print(datetime_str)\n",
    "    datetime_object = datetime.strptime(datetime_str, '%d/%m/%y').date()\n",
    "    return datetime_object\n",
    "\n",
    "datetime_object_udf = udf(f=datetime_object, returnType=DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_vector_schema = index_vector.select(\"MXWDU_Index\", datetime_object_udf(col(\"operation_date\")).alias('operation_date'))\n",
    "index_vector_schema.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_date_index = index_vector_schema.select(\"*\", month(col(\"operation_date\")).alias(\"month\"), year(col(\"operation_date\")).alias(\"year\"))\\\n",
    "                                      .groupBy(\"year\", \"month\").agg(spark_max(\"operation_date\").alias(\"operation_date\")).drop(\"year\", \"month\")\\\n",
    "                                      .join(index_vector_schema, on=\"operation_date\", how=\"left\")\n",
    "close_date_index.orderBy(col(\"operation_date\").desc()).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_portfolio_df = close_date_index.join(monthly_data, on=\"operation_date\", how=\"left\")\n",
    "index_portfolio_df.orderBy(col(\"operation_date\").desc()).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "np.random.seed(777)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolved_time_frame(df):\n",
    "    table = df.toPandas()\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    for c in table.columns.values:\n",
    "        plt.plot(table.index, table[c], lw=3, alpha=0.8,label=c)\n",
    "    #plt.legend(loc='upper left', fontsize=12)\n",
    "    plt.ylabel('price in $')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolved_time_frame(index_portfolio_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_data = index_portfolio_df.toPandas()\n",
    "print(\"nuestros datos:\", monthly_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## seleccionamos la primera columna y lo convertimos a vector, esta columna representa el indice que estamos analizando, \n",
    "## 'MXWDU_Index', la idea es medir esta columna con el resto, ya que las demás son acciones que forman parte de ése índice.\n",
    "\n",
    "benchmark_month = monthly_data[\"MXWDU_Index\"]\n",
    "\n",
    "# nuestros datos\n",
    "print(benchmark_month.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculamos el porcentaje de cambio de un día contra otro.\n",
    "## fórmula: (t+1 / t)-1\n",
    "## pseudo-código: (precio_hoy / precio_ayer)-1\n",
    "\n",
    "pct_benchmark_month = benchmark_month.pct_change(1)\n",
    "print(pct_benchmark_month.shape)\n",
    "\n",
    "# nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## el vector de \"percentage change\" se convierte a un arreglo numpy de dimensión (147,)\n",
    "## NOTA: en los arreglos (objetos) de tipo numpy.array preservan los valores, tanto por fila, como por columna,\n",
    "##       el órden de los elementos, cómo un 'índice implícito'\n",
    "\n",
    "pct_benchmark_month_array = np.array(pct_benchmark_month)\n",
    "print(pct_benchmark_month_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lo mismo hacemos, pero para el resto de variables equity,\n",
    "## seleccionamos la primera columna y lo convertimos a vector, esta columna representa el indice que estamos analizando, \n",
    "## 'MXWDU_Index', la idea es medir esta columna con el resto, ya que las demás son acciones que forman parte de ese índice.\n",
    "\n",
    "investment_universe_month = monthly_data.drop(['operation_id', \"operation_date\", \"MXWDU_Index\"], axis = 1)\n",
    "\n",
    "# nuestros datos\n",
    "investment_universe_month.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (t+1 / t)-1\n",
    "## (precio_hoy / precio_ayer)-1\n",
    "\n",
    "pct_investment_month = investment_universe_month.pct_change(1)\n",
    "\n",
    "# nuestros datos\n",
    "pct_investment_month.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## el vector de percentage change se convierte a un arreglo numpy de dimensión (147,70)\n",
    "\n",
    "pct_investment_month_array = np.array(pct_investment_month)\n",
    "#_pct_investment_month_array = np.array(_pct_investment_month)\n",
    "\n",
    "# nuestros datos\n",
    "print(pct_investment_month_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creamos arreglos numpy con dimensiones X+1 = 148, rellenas de ceros, para ser imputados con nuevos vectores\n",
    "\n",
    "up_month = np.zeros((pct_benchmark_month_array.shape[0]+1, 1))\n",
    "down_month = np.zeros((pct_benchmark_month_array.shape[0]+1, 1))\n",
    "up_move = np.zeros((pct_benchmark_month_array.shape[0]+1, pct_investment_month_array.shape[1]))\n",
    "down_move = np.zeros((pct_benchmark_month_array.shape[0]+1, pct_investment_month_array.shape[1]))\n",
    "\n",
    "print(up_month.shape)\n",
    "print(down_month.shape)\n",
    "print(up_move.shape)\n",
    "print(down_move.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rellenamos las matrices de ceros con valores que aprueben las condiciones, \n",
    "## se realiza una comparación dentro de los arreglos de porcentajes de cambio, \n",
    "## sí alguno de esos porcentajes es superior a 0, entonces entra a los arreglos \n",
    "## de movimientos positivos (incrementos), pero sí alguno es menor que 0, entonces\n",
    "## el porcentaje se almacena en los arreglos de movimientos negativos (decrementos).\n",
    "\n",
    "## Básicamente, se separan los porcentajes de cambio en dos matrices: \n",
    "## matriz de positivos cuando el porcentaje es > 0 \n",
    "## matriz de negativos cuando el porcentaje es <= 0\n",
    "\n",
    "size_benchmark_matrix = pct_benchmark_month_array.shape[0]\n",
    "for i in range (1, size_benchmark_matrix):\n",
    "    if pct_benchmark_month_array[i] > 0:\n",
    "        up_month[i] = pct_benchmark_month_array[i]\n",
    "        up_move[i] = pct_investment_month_array[i, 0:pct_investment_month_array.shape[1]]\n",
    "    else:\n",
    "        down_month[i] = pct_benchmark_month_array[i]\n",
    "        down_move[i] = pct_investment_month_array[i, 0:pct_investment_month_array.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculamos los vectores 'peor más alto' y 'mejor más alto'\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "greater_worse = down_move / down_month\n",
    "greater_better = (up_move / up_month) * float(-1.0)\n",
    "\n",
    "print(greater_worse.shape)\n",
    "print(greater_better.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ambos vectores los convertimos a pandas dataframes, y solo nos quedamos con los vectores que tengan valores != np.nan\n",
    "## una de las ventajas de los pandas dataframes es que mantienen un ídince único por row, esto lo hace poder separarse, y juntarse\n",
    "## en cuantos sub-conjuntos se requieran y siempre se podrá mantener un órden.\n",
    "\n",
    "greater_worse_df = pd.DataFrame(data=greater_worse).dropna()\n",
    "greater_better_df = pd.DataFrame(data=greater_better).dropna()\n",
    "\n",
    "print(greater_worse_df.shape)\n",
    "print(greater_better_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculamos ahora, la mediana acumulada con los pandas dataframes que construimos, \n",
    "## con un periodo mínimo (método expanding) de al menos 1 observación dada.\n",
    "\n",
    "median_down = greater_worse_df.expanding().median()\n",
    "median_up = greater_better_df.expanding().median()\n",
    "\n",
    "print(median_down.shape)\n",
    "print(median_up.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se transponen ambos pandas df por la columna periodos, columna que almacena números no consecutivos desde 1 hasta 147\n",
    "\n",
    "down_transpose = median_down.T\n",
    "up_transpose = median_up.T\n",
    "\n",
    "print(down_transpose.shape)\n",
    "print(up_transpose.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se rankean los resultados (top 10) entre las fechas cierre (periodo) y se vuelve a transponer la tabla ranked_down\n",
    "\n",
    "ranked_down = down_transpose.rank(method='first')\n",
    "transpose_ranked_down = ranked_down.T\n",
    "\n",
    "print(ranked_down.shape)\n",
    "print(transpose_ranked_down.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se rankean los resultados (top 10) entre las fechas cierre (periodo) y se vuelve a transponer la tabla ranked_up\n",
    "\n",
    "ranked_up = up_transpose.rank(method='first')\n",
    "transpose_ranked_up = ranked_up.T\n",
    "\n",
    "print(ranked_up.shape)\n",
    "print(transpose_ranked_up.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se añade variable 'label' con la idea de que al juntar ambos dataframes se puedan distinguir los 'worse' de los 'better'\n",
    "## y se unen ambos dataframes con la etiqueta creada, se usó el método 'insert' por lo que no se deberá correr de nuevo, una\n",
    "## vez ejecutado ya que fallará por duplicidad de columnas.\n",
    "\n",
    "worse_better_df = pd.concat([transpose_ranked_up, transpose_ranked_down]).sort_index()\n",
    "worse_better_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se crea un índice 'closing_id' para cada registro, éste corre de [1:N+1] \n",
    "## con la idea de etiquetar el id del mes de registro de cierre,\n",
    "## de la misma forma que lo anterior, NO se deberá ejecutar de nuevo; una vez hecho.\n",
    "\n",
    "worse_better_df['closing_id'] = range(1, len(worse_better_df) + 1)\n",
    "worse_better_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se transponen ambos dataframes, de antes tener una dimensión (68, 70), es decir; \n",
    "## 68 registros i.e. 'Rows' (variables)\n",
    "## 70 columnas fijas (a menos que sea añadido otro asset desde el csv inicial)\n",
    "\n",
    "## a tener una dimensión 'transpuesta' (invertída sí querés...) de (70, 68), es decir;\n",
    "## 70 registros i.e. 'Rows' fijos (a menos que sea añadido otro asset desde el csv inicial)\n",
    "## 68 columnas (variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## de pandas dataframes, una vez separados en dos conjuntos ['worse', 'better'],\n",
    "## creamos por separado dos spark dataframes.\n",
    "\n",
    "worse_better = spark.createDataFrame(worse_better_df)\n",
    "worse_better.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worse_better.coalesce(1).write.mode(\"overwrite\").parquet(\"data/ophelia/out/engine/RankMedian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finish Rank Median Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start TopAssetRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worse_better = spark.read.parquet(\"data/ophelia/out/engine/RankMedian/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se crea un generador \"shape long format\", una lista con iteraciones, esta lista trabajará con dos variables principales,\n",
    "## 1-. la variable 'equity_index', será la que contenga los 'id' de los activos\n",
    "## 2-. la variable 'median_down', será la que contenga la mediana acumulada por cada activo.\n",
    "## Se mantendrá a lo largo de la transformación 1 columna fija; 'closing_id',\n",
    "## closing_id: variable que indica el mes de cierre y reporte de precio\n",
    "\n",
    "def panel_format(df, pivot_col, new_columns: list = []):\n",
    "    first_col = str(new_columns[0])\n",
    "    second_col = str(new_columns[1])\n",
    "    piv_col = [pivot_col]\n",
    "    df_types = df.dtypes\n",
    "    cols, dtype = zip(*((c, t) for (c, t) in df_types if c not in piv_col))\n",
    "    if len(set(dtype)) == 1:\n",
    "        ValueError(\"Columns not the same data type...\")\n",
    "    generator_explode = explode(array([\n",
    "        struct(lit(c).alias(first_col), col(c).alias(second_col)) for c in cols\n",
    "    ])).alias(\"column_explode\")\n",
    "    column_to_explode = [\"column_explode.\"+first_col, \"column_explode.\"+second_col]\n",
    "    panel_df = df.select(piv_col + [generator_explode])\\\n",
    "                 .select(piv_col + column_to_explode)\n",
    "    return panel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se crea nuevo spark-dataframe donde solo se mostrará por partición ['closing_id'] \n",
    "## el top 10 mejores meses donde tuvo menos malos que el resto de los registros; [\"top_rank\"].\n",
    "new_columns = [\"asset_id\", \"rank\"]\n",
    "asset_ranking_df = panel_format(worse_better, \"closing_id\", new_columns)\n",
    "asset_ranking_df.printSchema()\n",
    "asset_ranking_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_ranking_df.coalesce(1).write.mode(\"overwrite\").parquet(\"data/ophelia/out/engine/LongMedianRank/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finish LongMedianRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start TrainPortfolioSimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_ranking_df = spark.read.parquet(\"data/ophelia/out/engine/LongMedianRank/\")\n",
    "asset_rank_10 = asset_ranking_df.where(col(\"rank\") <= 10).orderBy(\"closing_id\", \"rank\").toPandas()\n",
    "asset_rank_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lo que se pretende ahora, es obtener una matriz de dimensión (146, 10), es decir, \n",
    "## tener en cada row las fechas de cierre [closing_id],\n",
    "## en cada columna (header) el número de ranking top 10 [top_rank],\n",
    "## y en cada campo, el id del activo [asset_id].\n",
    "\n",
    "pandas_df = asset_rank_10.astype({'asset_id':'int32'})\n",
    "newselect = asset_rank_10[[\"closing_id\", \"asset_id\"]]\n",
    "num_of_assets = 10\n",
    "\n",
    "indexed = np.zeros((investment_universe_month.shape[0], num_of_assets))\n",
    "\n",
    "for q in range(1,investment_universe_month.shape[0]):\n",
    "    selection = newselect.loc[pandas_df[\"closing_id\"]==q]\n",
    "    newselect_transpose = selection.T\n",
    "    newpdf = newselect_transpose['asset_id':].head()\n",
    "    indexed[q] = newpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed = indexed[1:investment_universe_month.shape[0]+1]\n",
    "index_row = indexed.astype(np.int64)\n",
    "newrow = [0]*num_of_assets #np.zeros((1,num_of_assets))\n",
    "index_row = np.vstack([index_row,newrow])\n",
    "portfolio = np.zeros((investment_universe_month.shape[0], num_of_assets))\n",
    "\n",
    "for r in range(0,investment_universe_month.shape[0]-1):\n",
    "    s = r+1\n",
    "    columns = index_row[s]\n",
    "    portfolio[s] = pct_investment_month_array[s,[columns]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = portfolio[1:investment_universe_month.shape[0]-1]\n",
    "performance = np.dot(portfolio,(1/num_of_assets))\n",
    "returns = np.zeros((investment_universe_month.shape[0]-1,1))\n",
    "equalw = np.zeros((investment_universe_month.shape[0]-1,1))\n",
    "eweights = 1/(pct_investment_month_array.shape[1])\n",
    "eweighted = np.dot(pct_investment_month_array,eweights)\n",
    "\n",
    "for x in range (1,investment_universe_month.shape[0]):\n",
    "    returns[x-1] = sum(performance[x])\n",
    "    equalw[x-1] = sum(eweighted[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beg = 100\n",
    "start = 100\n",
    "commence = 100\n",
    "bench = pct_benchmark_month_array[1:investment_universe_month.shape[0]]\n",
    "bmk = np.zeros((investment_universe_month.shape[0],))\n",
    "port = np.zeros((investment_universe_month.shape[0]-1,))\n",
    "ew = np.zeros((investment_universe_month.shape[0]-1,))\n",
    "\n",
    "for i in range (0,investment_universe_month.shape[0]-1):\n",
    "    bmk[i] = beg*(1+bench[i])\n",
    "    beg = bmk[i]\n",
    "    port[i] = start*(1+returns[i])\n",
    "    start = port[i]\n",
    "    ew[i] = commence*(1+equalw[i])\n",
    "    commence = ew[i]\n",
    "\n",
    "plt.plot(bmk[0:investment_universe_month.shape[0]-1])\n",
    "plt.plot(port)\n",
    "plt.plot(ew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_assets = index_row[index_row.shape[0]-2]\n",
    "input_assets1 = pct_investment_month_array[:, input_assets]\n",
    "dataframe = pd.DataFrame(input_assets1)\n",
    "input_assets1 = dataframe.dropna()\n",
    "mean_returns = input_assets1.mean()\n",
    "covar_matrix = input_assets1.cov()\n",
    "tickers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "def calc_portfolio_perf(weights, mean_returns, cov, rf):\n",
    "    portfolio_return = np.sum(mean_returns * weights) * 12\n",
    "    portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov, weights))) * np.sqrt(12)\n",
    "    sharpe_ratio = (portfolio_return - rf) / portfolio_std\n",
    "    return portfolio_return, portfolio_std, sharpe_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_random_portfolios(num_portfolios, mean_returns, cov, rf):\n",
    "    results_matrix = np.zeros((len(mean_returns)+3, num_portfolios))\n",
    "    for i in range(num_portfolios):\n",
    "        weights = np.random.random(len(mean_returns))\n",
    "        weights /= np.sum(weights)\n",
    "        portfolio_return, portfolio_std, sharpe_ratio = calc_portfolio_perf(weights, mean_returns, cov, rf)\n",
    "        results_matrix[0,i] = portfolio_return\n",
    "        results_matrix[1,i] = portfolio_std\n",
    "        results_matrix[2,i] = sharpe_ratio\n",
    "\n",
    "        for j in range(len(weights)):\n",
    "            results_matrix[j+3,i] = weights[j]\n",
    "\n",
    "    results_df = pd.DataFrame(results_matrix.T, columns=['ret','stdev','sharpe'] + [ticker for ticker in tickers])\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: revisar la sobrecarga de memoria en los stages marcados\n",
    "\n",
    "mean_returns = input_assets1.mean()\n",
    "cov = input_assets1.cov()\n",
    "num_portfolios = 10000000\n",
    "rf = 0.0\n",
    "\n",
    "results_frame = simulate_random_portfolios(num_portfolios, mean_returns, cov, rf)\n",
    "\n",
    "max_sharpe_port = results_frame.iloc[results_frame['sharpe'].idxmax()]\n",
    "\n",
    "min_vol_port = results_frame.iloc[results_frame['stdev'].idxmin()]\n",
    "\n",
    "max_return_port = results_frame.iloc[results_frame['ret'].idxmax()]\n",
    "\n",
    "plt.subplots(figsize=(15,10))\n",
    "plt.scatter(results_frame.stdev, results_frame.ret, c=results_frame.sharpe, cmap='RdYlBu')\n",
    "plt.xlabel('Standard Deviation')\n",
    "plt.ylabel('Returns')\n",
    "plt.colorbar()\n",
    "plt.scatter(max_sharpe_port['stdev'], max_sharpe_port['ret'], marker=(5,1,0), color='r', s=500)\n",
    "plt.scatter(min_vol_port['stdev'], min_vol_port['ret'], marker=(5,1,0), color='g', s=500)\n",
    "plt.scatter(max_return_port['stdev'], max_return_port['ret'], marker=(5,1,0), color='y', s=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_columns = {\n",
    "    0: 'fund_w_zero',\n",
    "    1: 'fund_w_one',\n",
    "    2: 'fund_w_two',\n",
    "    3: 'fund_w_three',\n",
    "    4: 'fund_w_four',\n",
    "    5: 'fund_w_five',\n",
    "    6: 'fund_w_six',\n",
    "    7: 'fund_w_seven',\n",
    "    8: 'fund_w_eight',\n",
    "    9: 'fund_w_nine'\n",
    "}\n",
    "results_frame_renamed = results_frame.rename(columns=renamed_columns)\n",
    "results_frame_renamed.to_csv('data/ophelia/out/model/tmp/TrainSimulation.csv', encoding='utf-8', index=False)\n",
    "results_frame_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_series_to_dict(pd_series):\n",
    "    to_python_dict = dict(pd_series)\n",
    "    return {str(k): float(v) for k, v in to_python_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_map_rename(dic, new_key, old_key):\n",
    "    for key in range(len(old_key)):\n",
    "        dic[new_key[key]] = dic.pop(old_key[key])\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_metadata_model(dic_sample, json_name):\n",
    "    dict_to_json = spark.read.json(sc.parallelize([dic_sample]))\n",
    "    dict_to_json.coalesce(1).write.mode(\"overwrite\").json(\"data/ophelia/out/model/model_info/\"+str(json_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_return_port_sample = pd_series_to_dict(max_return_port)\n",
    "persist_metadata_model(max_return_port_sample, \"maxReturnTrainWeights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sharpe_port_sample = pd_series_to_dict(max_sharpe_port)\n",
    "persist_metadata_model(max_sharpe_port_sample, \"maxSharpeTrainWeights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_vol_port_sample = pd_series_to_dict(min_vol_port)\n",
    "persist_metadata_model(min_vol_port_sample, \"minVolatileTrainWeights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp_read.coalesce(1).write.mode(\"overwrite\").parquet(\"data/ophelia/out/model/TrainSimulation/\")\n",
    "#tmp_read.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finish TrainPortfolioSimulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start EfficientFrontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#key_o = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "#key_n = ['fund_w_zero', 'fund_w_one', 'fund_w_two', 'fund_w_three', 'fund_w_four', \n",
    "#         'fund_w_five', 'fund_w_six', 'fund_w_seven', 'fund_w_eight', 'fund_w_nine']\n",
    "#\n",
    "#min_vol_port_sample = key_map_rename(pd_series_to_dict(min_vol_port), key_n, key_o)\n",
    "#min_vol_port_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_spark_df = spark.read.csv(\"data/ophelia/out/model/tmp/TrainSimulation.csv\", header='true')\n",
    "results_spark_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, current_date, current_timestamp\n",
    "\n",
    "results_index = results_spark_df.select(\"*\", (monotonically_increasing_id() + 1000).alias(\"id\"), current_date().alias(\"information_date\"), current_timestamp().alias(\"model_date\"))\n",
    "results_index.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_bucketizer = results_index.select(\"*\", percent_rank().over(Window.orderBy(results_index['sharpe'])).alias(\"percentile\"))\\\n",
    "                                     .select(\"*\", \n",
    "                                             when(col(\"percentile\") < 0.2, lit(5.0)).otherwise(\n",
    "                                                 when((col(\"percentile\") >= 0.2) & (col(\"percentile\") < 0.4), lit(4.0)).otherwise(\n",
    "                                                     when((col(\"percentile\") >= 0.4) & (col(\"percentile\") < 0.6), lit(3.0)).otherwise(\n",
    "                                                         when((col(\"percentile\") >= 0.6) & (col(\"percentile\") < 0.8), lit(2.0)).otherwise(\n",
    "                                                             when((col(\"percentile\") >= 0.8) & (col(\"percentile\") <= 1.0), lit(1.0)))))).alias(\"bucket\"))\n",
    "percentile_bucketizer.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_min_return(df, dic):\n",
    "    for col in min_vol_port_sample:\n",
    "        filter_df = df.filter(df[col] >= dic[col])\n",
    "    return filter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiter_min_return_df = filter_min_return(percentile_bucketizer, min_vol_port_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiter_min_return_df.groupBy(\"bucket\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centile_bucketizer.coalesce(1).write.mode(\"overwrite\").parquet('data/ophelia/out/model/RandSimulation/', partitionBy=\"information_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "# cual es el mejor port con base en el sharpe para cada cliente\n",
    "\n",
    "sorted_results = centile_bucketizer.orderBy(col(\"sharpe\").desc())\n",
    "sorted_results.show(10, False)\n",
    "pd_results = sorted_results.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ret_vol_sr(weights):\n",
    "    weights = np.array(weights)\n",
    "    ret = np.sum(mean_returns * weights) * 12\n",
    "    vol = np.sqrt(np.dot(weights.T, np.dot(cov, weights))) * np.sqrt(12)\n",
    "    sr = ret/vol\n",
    "    return np.array([ret, vol, sr])\n",
    "\n",
    "def neg_sharpe(weights):\n",
    "    # the number 2 is the sharpe ratio index from the get_ret_vol_sr\n",
    "    return get_ret_vol_sr(weights)[2] * -1\n",
    "\n",
    "def check_sum(weights):\n",
    "    #return 0 if sum of the weights is 1\n",
    "    return np.sum(weights)-1\n",
    "\n",
    "cons = ({'type':'eq', 'fun':check_sum})\n",
    "bounds = ((0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1))\n",
    "init_guess = [.1,.1,.1,.1,.1,.1,.1,.1,.1,.1]\n",
    "opt_results = sco.minimize(neg_sharpe, init_guess, method='SLSQP', bounds=bounds, constraints=cons)\n",
    "print(\"opt_results:\", opt_results)\n",
    "\n",
    "get_ret_vol_sr(opt_results.x)\n",
    "\n",
    "vol_arr = results_frame.stdev\n",
    "ret_arr = results_frame.ret\n",
    "sharpe_arr = results_frame.sharpe\n",
    "\n",
    "range_vol_min = min(vol_arr)\n",
    "range_vol_max = max(vol_arr)\n",
    "range_ret_min = min(ret_arr)\n",
    "range_ret_max = max(ret_arr)\n",
    "range_sharpe_min = min(sharpe_arr)\n",
    "range_sharpe_max = max(sharpe_arr)\n",
    "\n",
    "frontier_y = np.linspace(range_ret_min, range_ret_max, 200)\n",
    "frontier_x = []\n",
    "\n",
    "\n",
    "def minimize_volatility(weights):\n",
    "    return get_ret_vol_sr(weights)[1]\n",
    "\n",
    "for possible_return in frontier_y:\n",
    "    cons = ({'type':'eq', 'fun': check_sum},\n",
    "            {'type':'eq', 'fun': lambda w: get_ret_vol_sr(w)[0] - possible_return})\n",
    "    result = sco.minimize(minimize_volatility, init_guess, method='SLSQP', bounds=bounds, constraints=cons)\n",
    "    frontier_x.append(result['fun'])\n",
    "\n",
    "\n",
    "# cmap='RdYlBu' \n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(vol_arr, ret_arr, c=sharpe_arr, cmap='RdYlBu')\n",
    "plt.colorbar(label='Sharpe Ratio')\n",
    "plt.xlabel('Volatility')\n",
    "plt.ylabel('Return')\n",
    "plt.plot(frontier_x, frontier_y, 'r--', linewidth=3)\n",
    "# plt.savefig('cover.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ophelia",
   "language": "python",
   "name": "ophelia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
