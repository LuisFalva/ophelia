{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_subarea.output_text.output_stream.output_stdout > pre {\n",
    "    width:max-content;\n",
    "}\n",
    ".p-Widget.jp-RenderedText.jp-OutputArea-output > pre {\n",
    "   width:max-content;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "#sys.path.insert(0, '..')\n",
    "sys.path.append('..')\n",
    "pd.set_option('display.max_columns', 10000000)\n",
    "pd.set_option('display.max_rows', 10000000)\n",
    "pd.set_option('display.width', 10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualize\n",
    "from com.ophelia.OpheliaVendata import OpheliaVendata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ophelia = OpheliaVendata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = ophelia.ophelia_session\n",
    "path = \"data/ophelia/out/model/RiskClassifier/\"\n",
    "customer_banking = ophelia.ophelia_read.read_file(spark, path, \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_banking.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Se ha decidido aplicar el algoritmo Gradient-Boosted Tree Classifier, la rezón es por tener el potenciador del gradiente descendente, este ha demostrado tener buenos resultados, dado que el algoritmo no soporta clasificación multiclass, se trabajará un tratamiento especial a los datos.\n",
    "\n",
    "## Se crearán 5 GBTClassifier, uno para cada clase de riesgo {'A', 'MA', 'M', 'MC', 'C'}, convertiremos la clase $k_{i}$ en 1 y el resto en cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.sql.functions import when, col, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_string_columns = [\n",
    "    \"job\",\n",
    "    \"marital\",\n",
    "    \"education\",\n",
    "    \"gender\"\n",
    "]\n",
    "cast_numeric_columns = [\n",
    "    col(\"age\").cast(\"float\"),\n",
    "    col(\"child\").cast(\"float\"),\n",
    "    col(\"saving\").cast(\"float\"),\n",
    "    col(\"insight\").cast(\"float\"),\n",
    "    col(\"backup\").cast(\"float\")\n",
    "]\n",
    "build_target = when(col(\"risk_label\") == \"C\", lit(1.0)).otherwise(0.0)\n",
    "mapped_classes = customer_banking.select(\n",
    "    *initial_string_columns,\n",
    "    *cast_numeric_columns,\n",
    "    build_target.alias(\"label\")\n",
    ")\n",
    "mapped_classes.show(5, False)\n",
    "mapped_classes.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bitácora de experimentos:\n",
    "- para el primer experimento, entrenamos un modelo Gradient Boosted Tree Classifier prediciendo la clase 'C' (conservador) como variable 'target' = 1, la distribución es de [1:526, 0:10636], la clase 1 (positiva) presenta desbalanceo 20:1, observaremos los resultado producidos con el modelo defacto a entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_class = mapped_classes.groupBy(\"label\").count().where(col(\"label\") == 0.0).select(\"count\").collect()[0][0]\n",
    "positive_class = mapped_classes.groupBy(\"label\").count().where(col(\"label\") == 1.0).select(\"count\").collect()[0][0]\n",
    "\n",
    "print(\"Class proportion\", str(round(negative_class/positive_class)) + \":1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe(transform_list):\n",
    "    return Pipeline(stages=transform_list)\n",
    "\n",
    "def fit(pipe, df):\n",
    "    return pipe.fit(df)\n",
    "\n",
    "def transform(model, df):\n",
    "    return model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_indexer(col_list):\n",
    "    return [StringIndexer(inputCol=column, outputCol=\"{0}_index\".format(column)) for column in col_list]\n",
    "\n",
    "def build_string_index(df, col_list):\n",
    "    indexers = string_indexer(col_list)\n",
    "    pipe = pipe(indexers)\n",
    "    fit_model = fit(pipe, df)\n",
    "    return transform(fit_model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "if a is list:\n",
    "    print(\"hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_string_indexer(single_col: str) -> StringIndexer:\n",
    "    return StringIndexer(inputCol=single_col, outputCol=single_col + \"_index\")\n",
    "\n",
    "def multi_string_indexer(multi_col: list) -> List[StringIndexer]:\n",
    "    indexer = [StringIndexer(\n",
    "        inputCol=column,\n",
    "        outputCol=\"{0}_index\".format(column)) for column in multi_col]\n",
    "    return indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_string_index(df: DataFrame, indexer_type: str, col_name: str = None, col_list: list = None) -> DataFrame:\n",
    "    dict_indexer = {\n",
    "        \"single\": single_string_indexer,\n",
    "        \"multi\": multi_string_indexer\n",
    "    }\n",
    "    if col_list is not None:\n",
    "        pipe_ml = pipe(dict_indexer[indexer_type](col_list))\n",
    "    elif col_name is not None:\n",
    "        pipe_ml = pipe(dict_indexer[indexer_type](col_name))\n",
    "    else:\n",
    "        raise ValueError('Unexpected indexer type:{}'.format(dict_indexer[indexer_type]))\n",
    "    fit_model = fit(pipe_ml, df)\n",
    "    return transform(fit_model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['job', 'marital', 'education', 'gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_df = build_string_index(df=mapped_classes, indexer_type=\"multi\", col_name=categorical_columns)\n",
    "transform_df.show(5, False)\n",
    "transform_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer, BucketedRandomProjectionLSH, BucketedRandomProjectionLSHModel, Bucketizer, \\\n",
    "    ChiSqSelector, ChiSqSelectorModel, CountVectorizer, CountVectorizerModel, DCT, ElementwiseProduct, FeatureHasher, \\\n",
    "    HashingTF, IDF, IDFModel, Imputer, ImputerModel, IndexToString, MaxAbsScaler, MaxAbsScalerModel, MinHashLSH, \\\n",
    "    MinHashLSHModel, MinMaxScaler, MinMaxScalerModel, NGram, Normalizer, OneHotEncoder, \\\n",
    "    OneHotEncoderModel, PCA, PCAModel, PolynomialExpansion, QuantileDiscretizer, RegexTokenizer, RFormula, \\\n",
    "    RFormulaModel, StringIndexer, StringIndexerModel, VectorSlicer, Word2Vec, VectorSizeHint, StopWordsRemover, \\\n",
    "    StandardScalerModel, StandardScaler, SQLTransformer, Tokenizer, VectorAssembler, VectorIndexer, VectorIndexerModel, \\\n",
    "    Word2VecModel\n",
    "\n",
    "\n",
    "class OpheliaJavaTransformers:\n",
    "\n",
    "    __all__ = [Binarizer, BucketedRandomProjectionLSH, BucketedRandomProjectionLSHModel,\n",
    "               Bucketizer, ChiSqSelector, ChiSqSelectorModel, CountVectorizer, CountVectorizerModel,\n",
    "               DCT, ElementwiseProduct, FeatureHasher, HashingTF, IDF, IDFModel, Imputer, ImputerModel,\n",
    "               IndexToString, MaxAbsScaler, MaxAbsScalerModel, MinHashLSH, MinHashLSHModel, MinMaxScaler,\n",
    "               MinMaxScalerModel, NGram, Normalizer, OneHotEncoder, OneHotEncoderModel,\n",
    "               PCA, PCAModel, PolynomialExpansion, QuantileDiscretizer, RegexTokenizer, RFormula, RFormulaModel,\n",
    "               SQLTransformer, StandardScaler, StandardScalerModel, StopWordsRemover, StringIndexer, StringIndexerModel,\n",
    "               Tokenizer, VectorAssembler, VectorIndexer, VectorIndexerModel, VectorSizeHint, VectorSlicer, Word2Vec,\n",
    "               Word2VecModel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_type(dtype):\n",
    "    return dtype.__class__\n",
    "\n",
    "def class_name(dtype):\n",
    "    return dtype.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(pipe, df: DataFrame):\n",
    "    if class_type(pipe) not in OpheliaJavaTransformers.__all__:\n",
    "        raise TypeError(\"'pipe' must be OpheliaMLObjects not \" + class_name(pipe))\n",
    "    return pipe.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_estimator(col_list):\n",
    "    indexers = string_indexer(col_list)\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCols=[indexer.getOutputCol() for indexer in indexers],\n",
    "        outputCols=[\"{0}_encoded\".format(indexer.getOutputCol()) for indexer in indexers]\n",
    "    )\n",
    "    return encoder\n",
    "\n",
    "def build_one_hot_encoder(df, col_list):\n",
    "    encoder = ohe_estimator(col_list)\n",
    "    encode_vector = fit(encoder, df)\n",
    "    return transform(encode_vector, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_lambda = lambda k: k.endswith('_index')\n",
    "indexed_columns = list(filter(columns_lambda, transform_df.columns))\n",
    "encode_vector_df = build_one_hot_encoder(transform_df, categorical_columns).drop(*indexed_columns)\n",
    "encode_vector_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexer_encoded(index_list):\n",
    "    indexers = string_indexer(index_list)\n",
    "    encode_index_list = []\n",
    "    for c in range(len(indexers)):\n",
    "        encode_index_list.append(indexers[c].getOutputCol() + \"_encoded\")\n",
    "    return encode_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = string_indexer(categorical_columns)\n",
    "string_indexer_cols = []\n",
    "for c in range(len(indexers)):\n",
    "    string_indexer_cols.append(indexers[c].getOutputCol() + \"_encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_indexer_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericCols = ['age', 'child', 'saving', 'insight', 'backup']\n",
    "assemblerInputs = string_indexer_cols + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assemble_model = assembler.transform(encode_vector_df)\n",
    "assemble_model.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = mapped_classes.columns\n",
    "selectedCols = cols + ['features']\n",
    "vectorized_customer_banking = assemble_model.select(selectedCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_customer_banking.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = vectorized_customer_banking.randomSplit([0.7, 0.3], seed = 2020)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))\n",
    "\n",
    "print(\"\\nTrain distribution label\")\n",
    "train.groupBy(\"label\").count().show()\n",
    "\n",
    "print(\"Test distribution label\")\n",
    "test.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label', maxIter=10)\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_type(lrModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name(lrModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summary(model):\n",
    "    beta = np.sort(model.coefficientMatrix)\n",
    "    plt.plot(beta)\n",
    "    plt.ylabel('Beta Coefficients')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel.coefficientMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel.elasticNetParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel.interceptVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.plot(lrModel.summary.roc.select('FPR').collect(),\n",
    "         lrModel.summary.roc.select('TPR').collect())\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel.summary.roc.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "beta = np.sort(lrModel.coefficients)\n",
    "plt.plot(beta)\n",
    "plt.ylabel('Beta Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "plt.plot(roc['FPR'],roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = trainingSummary.pr.toPandas()\n",
    "plt.plot(pr['recall'],pr['precision'])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_classes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_cols = mapped_classes.columns + ['prediction', 'probability', 'rawPrediction']\n",
    "predictions = lrModel.transform(test)\n",
    "predictions.select(*predict_cols).where(col(\"label\") == 1.0).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from typing import Any\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.linalg import DenseVector, DenseMatrix\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel, \\\n",
    "    BinaryLogisticRegressionTrainingSummary\n",
    "\n",
    "def __build_parameters_model(**kargs: Any) -> dict:\n",
    "        \n",
    "    features_col = str(kargs.get(\"featuresCol\"))\n",
    "    label_col = str(kargs.get(\"labelCol\"))\n",
    "    max_iter = int(kargs.get(\"maxIter\"))\n",
    "    prediction_col = str(kargs.get(\"predictionCol\"))\n",
    "    reg_param = float(kargs.get(\"regParam\"))\n",
    "    elastic_net_param = float(kargs.get(\"elasticNetParam\"))\n",
    "    tolerance = float(kargs.get(\"tol\"))\n",
    "    fit_intercept = bool(kargs.get(\"fitIntercept\"))\n",
    "    threshold = float(kargs.get(\"threshold\"))\n",
    "    thresholds = kargs.get(\"thresholds\")\n",
    "    probability_col = str(kargs.get(\"probabilityCol\"))\n",
    "    raw_prediction_col = str(kargs.get(\"rawPredictionCol\"))\n",
    "    standardization = bool(kargs.get(\"standardization\"))\n",
    "    weight_col = kargs.get(\"weightCol\")\n",
    "    aggregation_depth = int(kargs.get(\"aggregationDepth\"))\n",
    "    family = str(kargs.get(\"family\"))\n",
    "    lower_bounds_coefficients = kargs.get(\"lowerBoundsOnCoefficients\")\n",
    "    upper_bounds_coefficients = kargs.get(\"upperBoundsOnCoefficients\")\n",
    "    lower_bounds_intercepts = kargs.get(\"lowerBoundsOnIntercepts\")\n",
    "    upper_bounds_intercepts = kargs.get(\"upperBoundsOnIntercepts\")\n",
    "    conf = {\n",
    "        \"features_col\": 'features' if features_col is None else features_col,\n",
    "        \"label_col\": 'label' if label_col is None else label_col,\n",
    "        \"max_iter\": 10 if max_iter is None else max_iter,\n",
    "        \"prediction_col\": 'prediction' if prediction_col is None else prediction_col,\n",
    "        \"reg_param\": 0.0 if reg_param is None else reg_param,\n",
    "        \"elastic_net_param\": 0.0 if elastic_net_param is None else elastic_net_param,\n",
    "        \"tolerance\": 1e-6 if tolerance is None else tolerance,\n",
    "        \"fit_intercept\": True if fit_intercept is None else fit_intercept,\n",
    "        \"threshold\": 0.5 if threshold is None else threshold,\n",
    "        \"thresholds\": None if thresholds is None else thresholds,\n",
    "        \"probability_col\": 'probability' if probability_col is None else probability_col,\n",
    "        \"raw_prediction_col\": 'rawPrediction' if raw_prediction_col is None else raw_prediction_col,\n",
    "        \"standardization\": True if standardization is None else standardization,\n",
    "        \"weight_col\": None if weight_col is None else weight_col,\n",
    "        \"aggregation_depth\": 2 if aggregation_depth is None else aggregation_depth,\n",
    "        \"family\": 'auto' if family is None else family,\n",
    "        \"lower_bounds_coefficients\": None if lower_bounds_coefficients is None else lower_bounds_coefficients,\n",
    "        \"upper_bounds_coefficients\": None if upper_bounds_coefficients is None else upper_bounds_coefficients,\n",
    "        \"lower_bounds_intercepts\": None if lower_bounds_intercepts is None else lower_bounds_intercepts,\n",
    "        \"upper_bounds_intercepts\": None if upper_bounds_intercepts is None else upper_bounds_intercepts\n",
    "    }\n",
    "    return conf\n",
    "\n",
    "def train(df: DataFrame, **kargs: Any) -> LogisticRegressionModel:\n",
    "    config_parameters = __build_parameters_model(**kargs)\n",
    "    lr = LogisticRegression(\n",
    "        featuresCol=config_parameters[\"features_col\"],\n",
    "        labelCol=config_parameters[\"label_col\"],\n",
    "        maxIter=config_parameters[\"max_iter\"],\n",
    "        predictionCol=config_parameters[\"prediction_col\"],\n",
    "        regParam=config_parameters[\"reg_param\"],\n",
    "        elasticNetParam=config_parameters[\"elastic_net_param\"],\n",
    "        tol=config_parameters[\"tolerance\"],\n",
    "        fitIntercept=config_parameters[\"fit_intercept\"],\n",
    "        threshold=config_parameters[\"threshold\"],\n",
    "        thresholds=config_parameters[\"thresholds\"],\n",
    "        probabilityCol=config_parameters[\"probability_col\"],\n",
    "        rawPredictionCol=config_parameters[\"raw_prediction_col\"],\n",
    "        standardization=config_parameters[\"standardization\"],\n",
    "        weightCol=config_parameters[\"weight_col\"],\n",
    "        aggregationDepth=config_parameters[\"aggregation_depth\"],\n",
    "        family=config_parameters[\"family\"],\n",
    "        lowerBoundsOnCoefficients=config_parameters[\"lower_bounds_coefficients\"],\n",
    "        upperBoundsOnCoefficients=config_parameters[\"upper_bounds_coefficients\"],\n",
    "        lowerBoundsOnIntercepts=config_parameters[\"lower_bounds_intercepts\"],\n",
    "        upperBoundsOnIntercepts=config_parameters[\"upper_bounds_intercepts\"]\n",
    "    )\n",
    "    return lr.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train, maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploraremos el valor de la información (iv) y el peso de la evidencia (woe) que aporta cada variable predictora al resultado de la variable dependiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __class_mark(observation, min_val, class_length, bins):\n",
    "    \"\"\"\n",
    "    Computes the corresponding bin to a certain data observation given the data set minimum, size, bins\n",
    "    and the class length\n",
    "    :param observation: float, value of which is required to obtain your bin number\n",
    "    :param min_val: float, minimum value observed in the rdd\n",
    "    :param class_length: float, length of each sub interval\n",
    "    :param bins: int, number of sub intervals\n",
    "    :return: int, bin corresponding to the given observation\n",
    "    \"\"\"\n",
    "    interval = int((observation - min_val) / class_length)\n",
    "    if interval >= bins:\n",
    "        return bins - 1\n",
    "    else:\n",
    "        return interval\n",
    "    \n",
    "def __frequency_rdd_continuous(data_set_rdd, min_val, class_length, bins, n):\n",
    "    \"\"\"\n",
    "    Generates the frequency table rdd from certain continuous column rdd\n",
    "\n",
    "    :param data_set_rdd: rdd, rdd of the continuous column of which the histogram will be computed\n",
    "    :param min_val: float, minimum value observed in the rdd\n",
    "    :param class_length: float, length of each sub interval\n",
    "    :param bins: int, number of sub intervals\n",
    "    :param n: int, table length\n",
    "    :return: rdd, rdd containing the frequencies for each class of the histogram\n",
    "    \"\"\"\n",
    "    frequency_rdd = data_set_rdd \\\n",
    "        .map(lambda x: (__class_mark(x, min_val, class_length, bins), 1)) \\\n",
    "        .reduceByKey(lambda x, y: x + y) \\\n",
    "        .map(lambda y: (y[0]+1, min_val+class_length*y[0], min_val+class_length*(y[0]+1), y[1], y[1]/n))\n",
    "    return frequency_rdd\n",
    "\n",
    "def __frequency_rdd_discrete(data_set_rdd, n):\n",
    "    \"\"\"\n",
    "    Generates the frequency table rdd from certain discrete column rdd\n",
    "    :param data_set_rdd: rdd, rdd of the continuous column of which the histogram will be computed\n",
    "    :param n: int, table length\n",
    "    :return: rdd, rdd containing the frequencies for each class of the histogram\n",
    "    \"\"\"\n",
    "    frequency_rdd = data_set_rdd \\\n",
    "        .map(lambda r: (r, 1)) \\\n",
    "        .reduceByKey(lambda x, y: x + y) \\\n",
    "        .map(lambda x: (x[0], x[1], x[1] / n))\n",
    "    return frequency_rdd\n",
    "\n",
    "def frequency_table_continuous(data_set_df, column, bins=None, suffix=''):\n",
    "    \"\"\"\n",
    "    Computes the histogram frequency table from a column with continuous values for a table DataFrameUtils\n",
    "\n",
    "    :param data_set_df: DataFrameUtils, table of which it is required to calculate the frequency histogram of some of\n",
    "    its columns\n",
    "    :param column: string, column with continuous values which is required to calculate its histogram\n",
    "    :param bins: int, number of sub intervals\n",
    "    :param suffix: string, assign the suffix to each column of the frequency table\n",
    "    :return: DataFrameUtils with the histogram frequency table\n",
    "    \"\"\"\n",
    "    freq_schema = ['bin', 'lower_limit'+suffix, 'upper_limit'+suffix, 'fa_'+column+suffix, 'f_'+column+suffix]\n",
    "    window_freq = Window.orderBy('bin').rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "    cumulative_rel_freq = spark_sum('f_'+column+suffix)\\\n",
    "        .over(window_freq)\\\n",
    "        .alias('F_cumulative_'+column+suffix)\n",
    "\n",
    "    cumulative_abs_freq = spark_sum('fa_'+column+suffix)\\\n",
    "        .over(window_freq)\\\n",
    "        .alias('Fa_cumulative_'+column+suffix)\n",
    "\n",
    "    data_set_rdd = data_set_df.select(column).rdd.map(lambda row: (row[0]))\n",
    "    n = data_set_rdd.count()\n",
    "\n",
    "    if bins is None:\n",
    "        bins = 1 + int(3.322 * np.log(n))\n",
    "\n",
    "    maximum = data_set_rdd.max()\n",
    "    minimum = data_set_rdd.min()\n",
    "    class_length = (maximum - minimum) / bins\n",
    "\n",
    "    frequency_table_df = __frequency_rdd_continuous(data_set_rdd, minimum, class_length, bins, n)\\\n",
    "        .toDF(freq_schema)\\\n",
    "        .select('*', cumulative_abs_freq, cumulative_rel_freq)\n",
    "    return frequency_table_df\n",
    "\n",
    "def frequency_table_discrete(data_set_df, column, suffix=''):\n",
    "    \"\"\"\n",
    "    Generates the frequency table rdd from certain discrete column rdd.\n",
    "\n",
    "    :param data_set_df: DataFrameUtils, table of which it is required to calculate the frequency histogram of some of\n",
    "    its columns\n",
    "    :param column: string, column with continuous values ​​which is required to calculate its histogram\n",
    "    :param suffix: string, assign the suffix to each column of the frequency table\n",
    "    :return: DataFrameUtils with the histogram frequency table\n",
    "    \"\"\"\n",
    "    freq_schema = ['bin', 'fa_' + column + suffix, 'f_' + column + suffix]\n",
    "    window_freq = Window.orderBy('bin').rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "    cumulative_rel_freq = spark_sum('f_' + column + suffix).over(window_freq).alias(\n",
    "        'F_cumulative_' + column + suffix)\n",
    "\n",
    "    cumulative_abs_freq = spark_sum('fa_' + column + suffix).over(window_freq).alias(\n",
    "        'Fa_cumulative_' + column + suffix)\n",
    "\n",
    "    data_set_rdd = data_set_df.select(column).rdd.map(lambda row: (row[0]))\n",
    "\n",
    "    n = data_set_rdd.count()\n",
    "\n",
    "    frequency_table_df = __frequency_rdd_discrete(data_set_rdd, n)\\\n",
    "        .toDF(freq_schema)\\\n",
    "        .select('*', cumulative_abs_freq, cumulative_rel_freq)\n",
    "    return frequency_table_df\n",
    "\n",
    "def compute_information_value(mixed_dist_df, column):\n",
    "    \"\"\"\n",
    "    Calculate the weight of evidence (woe) and the information value (iv) give a table that contains the\n",
    "    histograms frequency tables of the negative and positive populations.\n",
    "\n",
    "    :param mixed_dist_df: DataFrameUtils, table with the mixed histograms of negative and positive populations\n",
    "    :param column: string, column with continuous values which is required to calculate its histogram\n",
    "    :return: DataFrameUtils, table that includes the woe and iv for each bin inside the histograms\n",
    "    \"\"\"\n",
    "    epsilon = 0.000000001\n",
    "    woe_expr = spark_log((col('f_' + column + '_0') + epsilon) / (col('f_' + column + '_1') + epsilon))\n",
    "    iv_expr = (col('f_' + column + '_0') - col('f_' + column + '_1')) * col('woe_' + column)\n",
    "\n",
    "    iv_table_df = mixed_dist_df\\\n",
    "        .fillna(0)\\\n",
    "        .withColumn('woe_' + column, woe_expr)\\\n",
    "        .withColumn('iv_' + column, iv_expr)\\\n",
    "        .fillna(0, subset=['woe_' + column, 'iv_' + column])\\\n",
    "        .orderBy('bin')\n",
    "    return iv_table_df\n",
    "\n",
    "def information_value_continuous(data_set_df, column, target, neg_label=0, pos_label=1, bins=None):\n",
    "    \"\"\"\n",
    "    Computes de frequency table histograms for the negative and positive populations and then compute the\n",
    "    weight of evidence (woe) and the information value (iv) table.\n",
    "\n",
    "    :param data_set_df: DataFrameUtils, table of which it is required to calculate the frequency histogram of some of\n",
    "    its columns\n",
    "    :param column: string, column with continuous values which is required to calculate its histogram\n",
    "    :param target: string, target column name that contains observations of negative and positive populations\n",
    "    :param neg_label: int, value to identify a negative row observation\n",
    "    :param pos_label: int, value to identify a positive row observation\n",
    "    :param bins: int, number of sub intervals\n",
    "    :return: DataFrameUtils, table that includes the woe and iv for each bin of frequency table histogram\n",
    "    \"\"\"\n",
    "    freq_neg_schema = ['bin', 'lower_limit_' + column, 'upper_limit_' + column, 'fa_' + column + '_0', 'f_' + column + '_0']\n",
    "    freq_pos_schema = ['bin', 'lower_limit_1', 'upper_limit_1', 'fa_' + column + '_1', 'f_' + column + '_1']\n",
    "\n",
    "    data_set_rdd = data_set_df.select(column).rdd.map(lambda row: (row[0]))\n",
    "    n = data_set_rdd.count()\n",
    "\n",
    "    if bins is None:\n",
    "        bins = 1 + int(3.322 * np.log(n))\n",
    "\n",
    "    maximum = data_set_rdd.max()\n",
    "    minimum = data_set_rdd.min()\n",
    "    class_length = (maximum - minimum) / bins\n",
    "\n",
    "    lower_lim_expr = (lit(minimum) + lit(class_length) * (col('bin') - lit(1))).alias('lower_limit')\n",
    "    upper_lim_expr = (lit(minimum) + lit(class_length) * col('bin')).alias('upper_limit')\n",
    "\n",
    "    neg_rdd = data_set_df\\\n",
    "        .where(col(target) == neg_label)\\\n",
    "        .select(column).rdd\\\n",
    "        .map(lambda row: (row[0]))\n",
    "\n",
    "    pos_rdd = data_set_df\\\n",
    "        .where(col(target) == pos_label)\\\n",
    "        .select(column).rdd\\\n",
    "        .map(lambda row: (row[0]))\n",
    "\n",
    "    freq_neg_df = __frequency_rdd_continuous(neg_rdd, minimum, class_length, bins, n)\\\n",
    "        .toDF(freq_neg_schema)\\\n",
    "        .select('bin', 'f_' + column + '_0')\n",
    "\n",
    "    freq_pos_df = __frequency_rdd_continuous(pos_rdd, minimum, class_length, bins, n)\\\n",
    "        .toDF(freq_pos_schema)\\\n",
    "        .select('bin', 'f_' + column + '_1')\n",
    "\n",
    "    mixed_dist_df = freq_pos_df.join(freq_neg_df, on='bin', how='full')\\\n",
    "        .select('*', lower_lim_expr, upper_lim_expr)\\\n",
    "\n",
    "    return compute_information_value(mixed_dist_df, column)\n",
    "\n",
    "def information_value_discrete(data_set_df, column, target, neg_label=0, pos_label=1):\n",
    "    \"\"\"\n",
    "    Computes de frequency table histograms for the negative and positive populations and then compute the\n",
    "    weight of evidence (woe) and the information value (iv) table.\n",
    "\n",
    "    :param data_set_df: DataFrameUtils, table of which it is required to calculate the frequency histogram of some of\n",
    "    its columns\n",
    "    :param column: string, column with continuous values ​​which is required to calculate its histogram\n",
    "    :param target: string, target column name that contains observations of negative and positive populations\n",
    "    :param neg_label: int, value to identify a negative row observation\n",
    "    :param pos_label: int, value to identify a positive row observation\n",
    "    :return: DataFrameUtils, table that includes the woe and iv for each bin of frequency table histogram\n",
    "    \"\"\"\n",
    "    freq_neg_schema = ['bin', 'fa_' + column + '_0', 'f_' + column + '_0']\n",
    "    freq_pos_schema = ['bin', 'fa_' + column + '_1', 'f_' + column + '_1']\n",
    "\n",
    "    data_set_rdd = data_set_df.select(column).rdd.map(lambda row: (row[0]))\n",
    "    n = data_set_rdd.count()\n",
    "\n",
    "    neg_rdd = data_set_df.where(col(target) == neg_label)\\\n",
    "        .select(column).rdd\\\n",
    "        .map(lambda row: (row[0]))\n",
    "\n",
    "    pos_rdd = data_set_df.where(col(target) == pos_label)\\\n",
    "        .select(column).rdd\\\n",
    "        .map(lambda row: (row[0]))\n",
    "\n",
    "    freq_neg_df = __frequency_rdd_discrete(neg_rdd, n)\\\n",
    "        .toDF(freq_neg_schema)\\\n",
    "        .select('bin', 'f_' + column + '_0')\n",
    "\n",
    "    freq_pos_df = __frequency_rdd_discrete(pos_rdd, n)\\\n",
    "        .toDF(freq_pos_schema)\\\n",
    "        .select('bin', 'f_' + column + '_1')\n",
    "\n",
    "    mixed_dist_df = freq_pos_df\\\n",
    "        .join(freq_neg_df, on='bin', how='full')\\\n",
    "        .fillna(0)\n",
    "    return compute_information_value(mixed_dist_df, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "from pyspark_dist_explore import hist\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, lit, sum as spark_sum, log as spark_log\n",
    "\n",
    "def create_frequency_table(df, coln, targt, function_type):\n",
    "    f_type = {\n",
    "        \"continuous\": information_value_continuous,\n",
    "        \"discrete\": information_value_discrete\n",
    "    }\n",
    "    \n",
    "    freq_df = f_type[function_type](data_set_df=df, column=coln, target=targt)\n",
    "    \n",
    "    return freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_frequency_table(predictions, \"job\", \"prediction\", \"discrete\").show(100, False)\n",
    "create_frequency_table(predictions, \"age\", \"prediction\", \"continuous\").show(100, False)\n",
    "create_frequency_table(predictions, \"education\", \"prediction\", \"discrete\").show(100, False)\n",
    "create_frequency_table(predictions, \"gender\", \"prediction\", \"discrete\").show(100, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol='label', maxDepth = 3)\n",
    "dtModel = dt.fit(train)\n",
    "predictions = dtModel.transform(test)\n",
    "predictions.select(*predict_cols).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='label')\n",
    "rfModel = rf.fit(train)\n",
    "predictions = rfModel.transform(test)\n",
    "predictions.select(*predict_cols).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-Boosted Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(maxIter=10)\n",
    "gbtModel = gbt.fit(train)\n",
    "predictions = gbtModel.transform(test)\n",
    "predictions.select(*predict_cols).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtModel.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtModel.getNumTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtModel.treeWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtModel.featureSubsetStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtModel.cacheNodeIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtModel.checkpointInterval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtModel.impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtModel.lossType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "                .addGrid(gbt.maxDepth, [2, 4, 6])\\\n",
    "                .addGrid(gbt.maxBins, [20, 60])\\\n",
    "                .addGrid(gbt.maxIter, [10, 20])\\\n",
    "                .build()\n",
    "\n",
    "# Run cross validations.  This can take about 6 minutes since it is training over 20 trees!\n",
    "cv = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "cvModel = cv.fit(train)\n",
    "\n",
    "boosting_predictions = cvModel.transform(test)\n",
    "evaluator.evaluate(boosting_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.bestModel.featureImportances.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.bestModel.evaluateEachIteration(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.bestModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cvModel.bestModel.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.bestModel.totalNumNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosting_predictions.select(*predict_cols).where(col(\"risk_label\") == \"C\").show(1000, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec\n",
    "# tokenizer\n",
    "# CountVectorizer\n",
    "# TF-IDF\n",
    "# FeatureHasher\n",
    "# StopWordsRemover\n",
    "# n-gram\n",
    "# binarizer\n",
    "# PCA\n",
    "# PolynomialExpansion\n",
    "# Discrete Cosine Transform\n",
    "# IndexToString\n",
    "# Interaction\n",
    "# VectorIndexer\n",
    "# Normalizer\n",
    "# StandardScaler\n",
    "# RobustScaler\n",
    "# MinMaxScaler\n",
    "# MaxAbsScaler\n",
    "# Bucketizer\n",
    "# ElementwiseProduct\n",
    "# SQLTransformer\n",
    "# VectorSizeHint\n",
    "# QuantileDiscretizer\n",
    "# Imputer\n",
    "# VectorSlicer\n",
    "# RFormula\n",
    "# ChiSqSelector\n",
    "# Locality Sensitive Hashing\n",
    "# Approx Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ophelia",
   "language": "python",
   "name": "ophelia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
