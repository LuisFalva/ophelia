{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500 # numero de registros\n",
    "p = 2 # numero de columnas (features)\n",
    "ci_dist = 0.5 # distancia entre circulos\n",
    "\n",
    "X, Y = make_circles(n_samples=n, factor=ci_dist, noise=0.05) # X matriz de features (coordenadas). Y matriz de binarios, (variable target)\n",
    "Y = Y[:, np.newaxis]\n",
    "print(\"x shape:\", X.shape)\n",
    "print(\"y shape:\", Y.shape)\n",
    "plt.scatter(X[Y[:, 0] == 0, 0], X[Y[:, 0] == 0, 1], c=\"skyblue\")\n",
    "plt.scatter(X[Y[:, 0] == 1, 0], X[Y[:, 0] == 1, 1], c=\"salmon\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos una clase para crear capas de la red neuronal, a esto le llamamos \"objeto\":\n",
    "class neural_layer():\n",
    "    \n",
    "    def __init__(self, num_connexions, num_neurons, activation_function):\n",
    "        \n",
    "        # variables para re-escalar las variables \"bias\" y \"W\"\n",
    "        rescale = 2\n",
    "        center = 1\n",
    "        \n",
    "        # el objeto recibirá la función de activación deseada (sigmoide, relu.\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "        # el entrenamiento inicial (antes del gradiente) será aleatorio gracias a estas dos variables:\n",
    "        self.bias = np.random.rand(1, num_neurons) * rescale - center\n",
    "        self.W = np.random.rand(num_connexions, num_neurons) * rescale - center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos las funciones de activacion (función anónima \"lambda\"):\n",
    "activation_f = {\n",
    "    \"sigmoidal\": lambda x: 1 / (1 + np.e ** (-x)),\n",
    "    \"sigmo_deriv\": lambda x: x * (1 - x),\n",
    "    \"relu\": lambda x: np.maximum(0, x),\n",
    "    \"leaky_relu\": lambda x: np.maximum(0.1 * x, x),\n",
    "    \"tanh\": lambda x: (2 / (1 + np.e ** (-2 * x))) - 1\n",
    "}\n",
    "\n",
    "# visualicemos el comportamiento de la función:\n",
    "_x = np.linspace(-5, 5, 100)\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "axes = fig.subplots(nrows=2, ncols=2)\n",
    "axes[0, 0].plot(_x, activation_f[\"sigmoidal\"](_x), label=\"Sigmoidal\")\n",
    "axes[0, 1].plot(_x, activation_f[\"relu\"](_x), label=\"ReLU\")\n",
    "axes[1, 0].plot(_x, activation_f[\"leaky_relu\"](_x), label=\"Leaky ReLU\")\n",
    "axes[1, 1].plot(_x, activation_f[\"tanh\"](_x), label=\"Hyperbolic Tangent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_net(topology, activation_function):\n",
    "    neural_net = []\n",
    "    for l, layer in enumerate(topology[:-1]):\n",
    "        neural_net.append(neural_layer(topology[l], topology[l+1], activation_function))\n",
    "        \n",
    "    return neural_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la ultima capa solo tiene una neurona pues queremos resolver un problema de [0, 1]:\n",
    "topology = [p, 4, 8, 1]\n",
    "neural_net = create_neural_net(topology=topology, activation_function=activation_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function = {\n",
    "    \"sme\": lambda Y_predict, Y_real: np.mean((Y_predict - Y_real) ** 2),\n",
    "    \"sme_deriv\": lambda Y_predict, Y_real: (Y_predict - Y_real)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _forward_propagation(neural_net, X, Y, cost_f, act_f):\n",
    "    \n",
    "    out_put = [(None, X)]\n",
    "    \n",
    "    # forward pass:\n",
    "    for l, layer in enumerate(neural_net):\n",
    "        weighted_sum = out_put[-1][1] @ neural_net[l].W + neural_net[l].bias # suma ponderada que se ejecuta en la primera capa\n",
    "        activation = neural_net[l].activation_function[act_f](weighted_sum)\n",
    "        out_put.append((weighted_sum, activation))\n",
    "    #print(\"Feed Forward Propagation Error\", \"[\", cost_f, \"]:\", cost_function[cost_f](out_put[-1][1], Y))\n",
    "    \n",
    "    return out_put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gradient_descent(neural_net, l, out_put, deltas, learn_r):\n",
    "    neural_net[l].bias = neural_net[l].bias - np.mean(deltas[0], axis=0, keepdims=True) * learn_r\n",
    "    neural_net[l].W = neural_net[l].W - out_put[l][1].T @ deltas[0] * learn_r\n",
    "    \n",
    "    return out_put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _back_propagation(neural_net, Y, out_put, cost_f, act_f, learn_r):\n",
    "    \n",
    "    deltas = []\n",
    "    \n",
    "    # backward pass:\n",
    "    for l in reversed(range(0, len(neural_net))):\n",
    "        weighted_sum = out_put[l+1][0]\n",
    "        activation = out_put[l+1][1]\n",
    "        \n",
    "        # back-propagation algorithm:\n",
    "        if l == len(neural_net) - 1:\n",
    "            deltas.insert(0, cost_function[cost_f](activation, Y) * neural_net[l].activation_function[act_f](activation))\n",
    "        else:\n",
    "            deltas.insert(0, deltas[0] @ W_vector.T * neural_net[l].activation_function[act_f](activation))\n",
    "        W_vector = neural_net[l].W\n",
    "        \n",
    "        _gradient_descent(neural_net=neural_net, l=l, out_put=out_put, deltas=deltas, learn_r=learn_r)\n",
    "    \n",
    "    return out_put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(neural_net, X, Y, cost_f, act_f, learn_r=0.05, train=True):\n",
    "    \n",
    "    # iniciamos con el entrenamiento hacia adelante:\n",
    "    _feed_forward_output = _forward_propagation(\n",
    "        neural_net=neural_net, \n",
    "        X=X, \n",
    "        Y=Y, \n",
    "        cost_f=cost_f, \n",
    "        act_f=act_f,\n",
    "    )\n",
    "    \n",
    "    if train:\n",
    "        # seguimos con el entrenamiento hacia atrás:\n",
    "        _back_propagation(\n",
    "            neural_net=neural_net,\n",
    "            Y=Y, \n",
    "            out_put=_feed_forward_output, \n",
    "            cost_f=\"sme_deriv\", \n",
    "            act_f=\"sigmo_deriv\",\n",
    "            learn_r=learn_r\n",
    "        )\n",
    "    \n",
    "    return _feed_forward_output[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejecutemos una prueba del método \"train()\"\n",
    "train(\n",
    "    neural_net=neural_net, \n",
    "    X=X, \n",
    "    Y=Y, \n",
    "    cost_f=\"sme\", \n",
    "    act_f=\"sigmoidal\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_n = create_neural_net(topology, activation_f)\n",
    "loss = []\n",
    "\n",
    "for i in range(1000):\n",
    "    predict_Y = train(neural_n, X, Y, cost_function, \"sigmoidal\", learn_r=0.05)\n",
    "    if i % 25 == 0:\n",
    "        loss.append(cost_function[\"sme\"](predict_Y, Y))\n",
    "        res = 50\n",
    "\n",
    "        _x0 = np.linspace(-1.5, 1.5, res)\n",
    "        _x1 = np.linspace(-1.5, 1.5, res)\n",
    "\n",
    "        _Y = np.zeros((res, res))\n",
    "        for i0, x0 in enumerate(_x0):\n",
    "            for i1, x1 in enumerate(_x1):\n",
    "                _Y[i0, i1] = train(neural_n, np.array([[x0, x1]]), Y, cost_function, \"sigmoidal\", train=False)[0][0]\n",
    "\n",
    "        plt.pcolormesh(_x0, _x1, _Y, cmap=\"coolwarm\")\n",
    "        plt.axis(\"equal\")\n",
    "\n",
    "        plt.scatter(X[Y[:, 0] == 0, 0], X[Y[:, 0] == 0, 1], c=\"skyblue\")\n",
    "        plt.scatter(X[Y[:, 0] == 1, 0], X[Y[:, 0] == 1, 1], c=\"salmon\")\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        plt.show()\n",
    "        plt.plot(range(len(loss)), loss)\n",
    "        plt.show()\n",
    "        time.sleep(0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
