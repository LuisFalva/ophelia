{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.optimize as sco\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "from pyspark.sql.functions import lit, last_day, col, array, explode, struct, udf, to_date, pandas_udf, PandasUDFType, sum as spark_sum, month, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Rank_Mean').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## por temas de replicación, siempre es mejor convertir el archivo a leer en formato csv\n",
    "\n",
    "monthly_data = spark.read.parquet(\"../data/master/ophelia/data/OpheliaData/analytical_base_table\")\n",
    "index_vector = spark.read.csv(\"../data/raw/csv/unique_dateprice_vector.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w = index_vector.limit(10)\n",
    "#w.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#from scipy import stats\n",
    "\n",
    "#multiply = pandas_udf(multiply_func, returnType=LongType())\n",
    "\n",
    "#@pandas_udf('double')\n",
    "#def cdf(v):\n",
    "#    return pd.Series(stats.norm.cdf(v))\n",
    "\n",
    "\n",
    "#asi = w.withColumn('cumulative_probability', cdf(w.MXWDU_Index))\n",
    "#asi.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def datetime_object(datetime_str):\n",
    "    print(datetime_str)\n",
    "    datetime_object = datetime.strptime(datetime_str, '%d/%m/%y').date()\n",
    "    return datetime_object\n",
    "\n",
    "datetime_object_udf = udf(f=datetime_object, returnType=DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_vector_schema = index_vector.select(\"MXWDU_Index\", datetime_object_udf(col(\"operation_date\")).alias('operation_date'))\n",
    "index_vector_schema.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_date_index = index_vector_schema.select(\"*\", last_day(col(\"operation_date\")).alias(\"close_date\"))\\\n",
    "                                      .where(col(\"operation_date\") == col(\"close_date\")).orderBy(col(\"operation_date\"))\\\n",
    "                                      .drop(\"close_date\")\n",
    "close_date_index.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_portfolio = monthly_data.select(\"*\", last_day(col(\"operation_date\")).alias(\"close_date\"))\\\n",
    "                                .where(col(\"operation_date\") == col(\"close_date\")).drop(\"close_date\")\\\n",
    "                                .orderBy(\"operation_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_portfolio_df = monthly_portfolio.join(close_date_index, on=\"operation_date\", how=\"inner\")\n",
    "index_portfolio_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_data = index_portfolio_df.toPandas()\n",
    "print(\"nuestros datos:\", monthly_data.shape)\n",
    "print(\"datos muestra:\", _monthly_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## seleccionamos la primera columna y lo convertimos a vector, esta columna representa el indice que estamos analizando, \n",
    "## 'MXWDU_Index', la idea es medir esta columna con el resto, ya que las demás son acciones que forman parte de ése índice.\n",
    "\n",
    "benchmark_month = monthly_data[\"MXWDU_Index\"]\n",
    "\n",
    "# nuestros datos\n",
    "print(benchmark_month.shape)\n",
    "benchmark_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculamos el porcentaje de cambio de un día contra otro.\n",
    "## fórmula: (t+1 / t)-1\n",
    "## pseudo-código: (precio_hoy / precio_ayer)-1\n",
    "\n",
    "pct_benchmark_month = benchmark_month.pct_change(1)\n",
    "print(pct_benchmark_month.shape)\n",
    "\n",
    "# nuestros datos\n",
    "pct_benchmark_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## el vector de \"percentage change\" se convierte a un arreglo numpy de dimensión (147,)\n",
    "## NOTA: en los arreglos (objetos) de tipo numpy.array preservan los valores, tanto por fila, como por columna,\n",
    "##       el órden de los elementos, cómo un 'índice implícito'\n",
    "\n",
    "pct_benchmark_month_array = np.array(pct_benchmark_month)\n",
    "print(pct_benchmark_month_array.shape)\n",
    "pct_benchmark_month_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lo mismo hacemos, pero para el resto de variables equity,\n",
    "## seleccionamos la primera columna y lo convertimos a vector, esta columna representa el indice que estamos analizando, \n",
    "## 'MXWDU_Index', la idea es medir esta columna con el resto, ya que las demás son acciones que forman parte de ese índice.\n",
    "\n",
    "investment_universe_month = monthly_data.drop(['operation_id', \"operation_date\", \"MXWDU_Index\"], axis = 1)\n",
    "\n",
    "# nuestros datos\n",
    "investment_universe_month.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (t+1 / t)-1\n",
    "## (precio_hoy / precio_ayer)-1\n",
    "\n",
    "pct_investment_month = investment_universe_month.pct_change(1)\n",
    "\n",
    "# nuestros datos\n",
    "pct_investment_month.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## el vector de percentage change se convierte a un arreglo numpy de dimensión (147,70)\n",
    "\n",
    "pct_investment_month_array = np.array(pct_investment_month)\n",
    "#_pct_investment_month_array = np.array(_pct_investment_month)\n",
    "\n",
    "# nuestros datos\n",
    "print(pct_investment_month_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creamos arreglos numpy con dimensiones X+1 = 148, rellenas de ceros, para ser imputados con nuevos vectores\n",
    "\n",
    "up_month = np.zeros((pct_benchmark_month_array.shape[0]+1, 1))\n",
    "down_month = np.zeros((pct_benchmark_month_array.shape[0]+1, 1))\n",
    "up_move = np.zeros((pct_benchmark_month_array.shape[0]+1, pct_investment_month_array.shape[1]))\n",
    "down_move = np.zeros((pct_benchmark_month_array.shape[0]+1, pct_investment_month_array.shape[1]))\n",
    "\n",
    "print(up_month.shape)\n",
    "print(down_month.shape)\n",
    "print(up_move.shape)\n",
    "print(down_move.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rellenamos las matrices de ceros con valores que aprueben las condiciones, \n",
    "## se realiza una comparación dentro de los arreglos de porcentajes de cambio, \n",
    "## sí alguno de esos porcentajes es superior a 0, entonces entra a los arreglos \n",
    "## de movimientos positivos (incrementos), pero sí alguno es menor que 0, entonces\n",
    "## el porcentaje se almacena en los arreglos de movimientos negativos (decrementos).\n",
    "\n",
    "## Básicamente, se separan los porcentajes de cambio en dos matrices: \n",
    "## matriz de positivos cuando el porcentaje es > 0 \n",
    "## matriz de negativos cuando el porcentaje es <= 0\n",
    "\n",
    "size_benchmark_matrix = pct_benchmark_month_array.shape[0]\n",
    "for i in range (1, size_benchmark_matrix):\n",
    "    if pct_benchmark_month_array[i] > 0:\n",
    "        up_month[i] = pct_benchmark_month_array[i]\n",
    "        up_move[i] = pct_investment_month_array[i, 0:pct_investment_month_array.shape[1]]\n",
    "    else:\n",
    "        down_month[i] = pct_benchmark_month_array[i]\n",
    "        down_move[i] = pct_investment_month_array[i, 0:pct_investment_month_array.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculamos los vectores 'peor más alto' y 'mejor más alto'\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "greater_worse = down_move / down_month\n",
    "greater_better = (up_move / up_month) * float(-1.0)\n",
    "\n",
    "print(greater_worse.shape)\n",
    "print(greater_better.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ambos vectores los convertimos a pandas dataframes, y solo nos quedamos con los vectores que tengan valores != np.nan\n",
    "## una de las ventajas de los pandas dataframes es que mantienen un ídince único por row, esto lo hace poder separarse, y juntarse\n",
    "## en cuantos sub-conjuntos se requieran y siempre se podrá mantener un órden.\n",
    "\n",
    "greater_worse_df = pd.DataFrameUtils(data=greater_worse).dropna()\n",
    "greater_better_df = pd.DataFrameUtils(data=greater_better).dropna()\n",
    "\n",
    "print(greater_worse_df.shape)\n",
    "print(greater_better_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculamos ahora, la mediana acumulada con los pandas dataframes que construimos, \n",
    "## con un periodo mínimo (método expanding) de al menos 1 observación dada.\n",
    "\n",
    "median_down = greater_worse_df.expanding().median()\n",
    "median_up = greater_better_df.expanding().median()\n",
    "\n",
    "print(median_down.shape)\n",
    "print(median_up.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se transponen ambos pandas df por la columna periodos, columna que almacena números no consecutivos desde 1 hasta 147\n",
    "\n",
    "down_transpose = median_down.T\n",
    "up_transpose = median_up.T\n",
    "\n",
    "print(down_transpose.shape)\n",
    "print(up_transpose.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se rankean los resultados (top 10) entre las fechas cierre (periodo) y se vuelve a transponer la tabla ranked_down\n",
    "\n",
    "ranked_down = down_transpose.rank(method='first')\n",
    "transpose_ranked_down = ranked_down.T\n",
    "\n",
    "print(ranked_down.shape)\n",
    "print(transpose_ranked_down.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se rankean los resultados (top 10) entre las fechas cierre (periodo) y se vuelve a transponer la tabla ranked_up\n",
    "\n",
    "ranked_up = up_transpose.rank(method='first')\n",
    "transpose_ranked_up = ranked_up.T\n",
    "\n",
    "print(ranked_up.shape)\n",
    "print(transpose_ranked_up.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se añade variable 'label' con la idea de que al juntar ambos dataframes se puedan distinguir los 'worse' de los 'better'\n",
    "## y se unen ambos dataframes con la etiqueta creada, se usó el método 'insert' por lo que no se deberá correr de nuevo, una\n",
    "## vez ejecutado ya que fallará por duplicidad de columnas.\n",
    "\n",
    "worse_better_df = pd.concat([transpose_ranked_up, transpose_ranked_down]).sort_index()\n",
    "worse_better_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se crea un índice 'closing_id' para cada registro, éste corre de [1:N] \n",
    "## con la idea de etiquetar el id del mes de registro de cierre,\n",
    "## de la misma forma que lo anterior, NO se deberá ejecutar de nuevo; una vez hecho.\n",
    "\n",
    "worse_better_df['closing_id'] = range(1, len(worse_better_df) + 1)\n",
    "worse_better_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se transponen ambos dataframes, de antes tener una dimensión (68, 70), es decir; \n",
    "## 68 registros i.e. 'Rows' (variables)\n",
    "## 70 columnas fijas (a menos que sea añadido otro asset desde el csv inicial)\n",
    "\n",
    "## a tener una dimensión 'transpuesta' (invertída sí querés...) de (70, 68), es decir;\n",
    "## 70 registros i.e. 'Rows' fijos (a menos que sea añadido otro asset desde el csv inicial)\n",
    "## 68 columnas (variables!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## de pandas dataframes, una vez separados en dos conjuntos ['worse', 'better'],\n",
    "## creamos por separado dos spark dataframes.\n",
    "\n",
    "worse_better = spark.createDataFrame(worse_better_df)\n",
    "worse_better.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se crea un generador \"shape long format\", una lista con iteraciones, esta lista trabajará con dos variables principales,\n",
    "## 1-. la variable 'equity_index', será la que contenga los 'id' de los activos\n",
    "## 2-. la variable 'median_down', será la que contenga la mediana acumulada por cada activo.\n",
    "## Se mantendrá a lo largo de la transformación 1 columna fija; 'closing_id',\n",
    "## closing_id: variable que indica el mes de cierre y reporte de precio\n",
    "\n",
    "def panel_format(df, pivot_col):\n",
    "    \n",
    "    piv_col = [pivot_col]\n",
    "    df_types = df.dtypes\n",
    "    cols, dtype = zip(*((c, t) for (c, t) in df_types if c not in piv_col))\n",
    "    \n",
    "    if len(set(dtype)) == 1:\n",
    "        ValueError(\"Columns not the same data type...\")\n",
    "    \n",
    "    generator_explode = explode(array([\n",
    "        struct(lit(c).alias(\"asset_id\"), col(c).alias(\"rank\")) for c in cols\n",
    "    ])).alias(\"column_explode\")\n",
    "    \n",
    "    panel_df = df.select(piv_col + [generator_explode])\\\n",
    "                 .select(piv_col + [\"column_explode.asset_id\", \"column_explode.rank\"])\n",
    "\n",
    "    return panel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se crea nuevo spark-dataframe donde solo se mostrará por partición ['closing_id'] \n",
    "## el top 10 mejores meses donde tuvo menos malos que el resto de los registros; [\"top_rank\"].\n",
    "\n",
    "asset_ranking_df = panel_format(worse_better, \"closing_id\")\n",
    "asset_ranking_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_rank_10 = asset_ranking_df.where(col(\"rank\") <= 10).orderBy(\"closing_id\", \"rank\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A partir de aqui se escribe en formato csv para trabajar con Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lo que se pretende ahora, es obtener una matriz de dimensión (146, 10), es decir, \n",
    "## tener en cada row las fechas de cierre [closing_id],\n",
    "## en cada columna (header) el número de ranking top 10 [top_rank],\n",
    "## y en cada campo, el id del activo [asset_id].\n",
    "\n",
    "pandas_df = asset_rank_10.astype({'asset_id':'int32'})\n",
    "newselect = asset_rank_10[[\"closing_id\", \"asset_id\"]]\n",
    "num_of_assets = 10\n",
    "\n",
    "indexed = np.zeros((investment_universe_month.shape[0], num_of_assets))\n",
    "\n",
    "for q in range(1,investment_universe_month.shape[0]):\n",
    "    selection = newselect.loc[pandas_df[\"closing_id\"]==q]\n",
    "    newselect_transpose = selection.T\n",
    "    newpdf = newselect_transpose['asset_id':].head()\n",
    "    indexed[q] = newpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed = indexed[1:investment_universe_month.shape[0]+1]\n",
    "index_row = indexed.astype(np.int64)\n",
    "newrow = [0]*num_of_assets #np.zeros((1,num_of_assets))\n",
    "index_row = np.vstack([index_row,newrow])\n",
    "portfolio = np.zeros((investment_universe_month.shape[0], num_of_assets))\n",
    "\n",
    "for r in range(0,investment_universe_month.shape[0]-1):\n",
    "    s = r+1\n",
    "    columns = index_row[s]\n",
    "    portfolio[s] = pct_investment_month_array[s,[columns]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = portfolio[1:investment_universe_month.shape[0]-1]\n",
    "performance = np.dot(portfolio,(1/num_of_assets))\n",
    "returns = np.zeros((investment_universe_month.shape[0]-1,1))\n",
    "equalw = np.zeros((investment_universe_month.shape[0]-1,1))\n",
    "eweights = 1/(pct_investment_month_array.shape[1])\n",
    "eweighted = np.dot(pct_investment_month_array,eweights)\n",
    "\n",
    "for x in range (1,investment_universe_month.shape[0]):\n",
    "    returns[x-1] = sum(performance[x])\n",
    "    equalw[x-1] = sum(eweighted[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beg = 100\n",
    "start = 100\n",
    "commence = 100\n",
    "bench = pct_benchmark_month_array[1:investment_universe_month.shape[0]]\n",
    "bmk = np.zeros((investment_universe_month.shape[0],))\n",
    "port = np.zeros((investment_universe_month.shape[0]-1,))\n",
    "ew = np.zeros((investment_universe_month.shape[0]-1,))\n",
    "\n",
    "for i in range (0,investment_universe_month.shape[0]-1):\n",
    "    bmk[i] = beg*(1+bench[i])\n",
    "    beg = bmk[i]\n",
    "    port[i] = start*(1+returns[i])\n",
    "    start = port[i]\n",
    "    ew[i] = commence*(1+equalw[i])\n",
    "    commence = ew[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bmk[0:investment_universe_month.shape[0]-1])\n",
    "plt.plot(port)\n",
    "plt.plot(ew)\n",
    "input_assets = index_row[index_row.shape[0]-2]\n",
    "input_assets1 = pct_investment_month_array[:, input_assets]\n",
    "dataframe = pd.DataFrameUtils(input_assets1)\n",
    "input_assets1 = dataframe.dropna()\n",
    "mean_returns = input_assets1.mean()\n",
    "covar_matrix = input_assets1.cov()\n",
    "tickers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "def calc_portfolio_perf(weights, mean_returns, cov, rf):\n",
    "    portfolio_return = np.sum(mean_returns * weights) * 12\n",
    "    portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov, weights))) * np.sqrt(252)\n",
    "    sharpe_ratio = (portfolio_return - rf) / portfolio_std\n",
    "    return portfolio_return, portfolio_std, sharpe_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_random_portfolios(num_portfolios, mean_returns, cov, rf):\n",
    "    results_matrix = np.zeros((len(mean_returns)+3, num_portfolios))\n",
    "    for i in range(num_portfolios):\n",
    "        weights = np.random.random(len(mean_returns))\n",
    "        weights /= np.sum(weights)\n",
    "        portfolio_return, portfolio_std, sharpe_ratio = calc_portfolio_perf(weights, mean_returns, cov, rf)\n",
    "        results_matrix[0,i] = portfolio_return\n",
    "        results_matrix[1,i] = portfolio_std\n",
    "        results_matrix[2,i] = sharpe_ratio\n",
    "\n",
    "        for j in range(len(weights)):\n",
    "            results_matrix[j+3,i] = weights[j]\n",
    "\n",
    "    results_df = pd.DataFrameUtils(results_matrix.T,columns=['ret','stdev','sharpe'] + [ticker for ticker in tickers])\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_returns = input_assets1.mean()\n",
    "cov = input_assets1.cov()\n",
    "num_portfolios = 10000\n",
    "rf = 0.0\n",
    "results_frame = simulate_random_portfolios(num_portfolios, mean_returns, cov, rf)\n",
    "\n",
    "max_sharpe_port = results_frame.iloc[results_frame['sharpe'].idxmax()]\n",
    "\n",
    "min_vol_port = results_frame.iloc[results_frame['stdev'].idxmin()]\n",
    "\n",
    "max_return_port = results_frame.iloc[results_frame['ret'].idxmax()]\n",
    "\n",
    "plt.subplots(figsize=(15,10))\n",
    "plt.scatter(results_frame.stdev,results_frame.ret, c=results_frame.sharpe, cmap='RdYlBu')\n",
    "plt.xlabel('Standard Deviation')\n",
    "plt.ylabel('Returns')\n",
    "plt.colorbar()\n",
    "plt.scatter(max_sharpe_port['stdev'], max_sharpe_port['ret'], marker=(5,1,0), color='r', s=500)\n",
    "plt.scatter(min_vol_port['stdev'], min_vol_port['ret'], marker=(5,1,0), color='g', s=500)\n",
    "plt.scatter(max_return_port['stdev'], max_return_port['ret'], marker=(5,1,0), color='y', s=500)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
