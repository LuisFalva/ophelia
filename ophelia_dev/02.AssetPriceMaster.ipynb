{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, count, avg, isnan, when, lit, monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iniciamos Sesi√≥n con Ophelia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualize\n",
    "from ophelia.ophelib.OpheliaMain import Ophelia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ophelia = Ophelia(\"A wrapper for pyspark\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CSV daily price Funds file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_portfolio_data(path_file, source, date_col, withSchema=True):\n",
    "    spark = ophelia.SparkSession\n",
    "    portfolio_path_file = path_file\n",
    "    portfolio_data = ophelia.Read.read_file(spark, path_file, source)\n",
    "    if withSchema is True:\n",
    "        return ophelia.Read.build_portfolio_schema(portfolio_data, date_col)\n",
    "    return portfolio_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change impure schema portfolio input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_df = read_portfolio_data(\n",
    "    path_file=\"data/raw/csv/data.csv\",\n",
    "    source=\"csv\",\n",
    "    date_col=\"operation_date\",\n",
    ")\n",
    "\n",
    "portfolio_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from com.ophelia.wrapper import SparkWrapper\n",
    "from com.ophelia.utils import ListUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackMetadata(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def meta_numeric(df, partition_by):\n",
    "        infer_type_dict = ListUtils.feature_picking(df)\n",
    "        append_numerics = infer_type_dict['double'] + infer_type_dict['float']\n",
    "        if partition_by is None:\n",
    "            return append_numerics\n",
    "        else:\n",
    "            return [partition_by] + append_numerics\n",
    "    \n",
    "    @staticmethod\n",
    "    def meta_date(df, partition_by):\n",
    "        infer_type_dict = ListUtils.feature_picking(df)\n",
    "        if partition_by is None:\n",
    "            return infer_type_dict['date']\n",
    "        else:\n",
    "            return [partition_by] + infer_type_dict['date']\n",
    "    \n",
    "    @staticmethod\n",
    "    def meta_partition(df, partition_by):\n",
    "        infer_type_dict = ListUtils.feature_picking(df)\n",
    "        append_partitioners = infer_type_dict['long'] + infer_type_dict['int']\n",
    "        if partition_by is None:\n",
    "            return append_partitioners\n",
    "        else:\n",
    "            return [partition_by] + append_partitioners\n",
    "    \n",
    "    @staticmethod\n",
    "    def meta_string(self):\n",
    "        infer_type_dict = ListUtils.feature_picking(self.df)\n",
    "        if self.partition_by is None:\n",
    "            return infer_type_dict['string']\n",
    "        else:\n",
    "            return [partition_by] + infer_type_dict['string']\n",
    "        \n",
    "    @staticmethod\n",
    "    def generate_partition(df, select_list: list, partition_by: str):\n",
    "        if partition_by is not None:\n",
    "            return df.select(partition_by, *select_list)\n",
    "        return df.select(monotonically_increasing_id().alias('partition_id'), *select_list)\n",
    "\n",
    "\n",
    "class NullDebug(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def cleansing_list(self, partition_by: str, offset: float = 0.5):\n",
    "        if partition_by is None:\n",
    "            raise TypeError(f\"'partition_by' required parameter, invalid {partition_by} input.\")\n",
    "        clean_list = self.toPanel(partition_by, ['id', 'value']).groupBy('id')\\\n",
    "                         .agg(count(when(isnan('value') | col('value').isNull(), 'value')).alias('null_count'))\\\n",
    "                         .select('*', (col('null_count') / self.Shape[0]).alias('null_pct'))\\\n",
    "                         .where(col('null_pct') <= offset).uniqueRow('id')\n",
    "        return clean_list\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_imput():\n",
    "        pass\n",
    "    \n",
    "    def median_imput():\n",
    "        pass\n",
    "    \n",
    "    def moving_imput():\n",
    "        pass\n",
    "    \n",
    "    def weight_imput():\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def null_clean(self, partition_by=None, offset=0.5):\n",
    "        numerics_list = BackMetadata.meta_numeric(self, partition_by)\n",
    "        gen_partition = BackMetadata.generate_partition(self, numerics_list, partition_by)\n",
    "        if partition_by is None:\n",
    "            cleansing_list = NullDebug.cleansing_list(gen_partition, 'partition_id', offset)\n",
    "            return gen_partition.select('partition_id', *cleansing_list)\n",
    "        cleansing_list = NullDebug.cleansing_list(gen_partition, partition_by, offset)\n",
    "        return self.select(partition_by, *cleansing_list)\n",
    "\n",
    "DataFrame.nullClean = NullDebug.null_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NullDebug.cleansing_list(portfolio_df, 'operation_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_df.nullClean(offset=0.5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, Window\n",
    "from pyspark.sql.functions import (when, row_number, lit, count, first, sum as spark_sum,\n",
    "                                   min as spark_min, max as spark_max, mean, stddev, variance)\n",
    "\n",
    "def spark_methods():\n",
    "    return {\n",
    "        'sum': spark_sum,\n",
    "        'min': spark_min,\n",
    "        'max': spark_max,\n",
    "        'mean': mean,\n",
    "        'stddev': stddev,\n",
    "        'var': variance,\n",
    "        'first': first,\n",
    "        'count': count,\n",
    "    }\n",
    "\n",
    "def rolling_down(self, op_col, nat_order, min_p=2, window=2, method='sum'):\n",
    "    w = Window.orderBy(nat_order).rowsBetween(\n",
    "        Window.currentRow - (window - 1), Window.currentRow)\n",
    "    if isinstance(op_col, list):\n",
    "        rolling = [spark_methods()[method](c).over(w).alias(f'{c}_rolling_{method}') for c in op_col]\n",
    "        return self.select(*rolling)\n",
    "    if method == 'count':\n",
    "        if isinstance(op_col, list):\n",
    "            rolling = [spark_methods()[method](c).over(w).alias(f'{c}_rolling_{method}') for c in op_col]\n",
    "            return self.select(*rolling)\n",
    "        rolling = spark_methods()[method](op_col).over(w).alias(f'{op_col}_rolling_{method}')\n",
    "        return self.select('*', rolling)\n",
    "    _unbounded_w = Window.orderBy(nat_order).rowsBetween(\n",
    "        Window.unboundedPreceding, Window.currentRow)\n",
    "    rolling = when(\n",
    "        row_number().over(_unbounded_w) >= min_p,\n",
    "        spark_methods()[method](op_col).over(w),\n",
    "        ).otherwise(lit(None)).alias(f'{op_col}_rolling_{method}')\n",
    "    return self.select('*', rolling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = portfolio_df.select(portfolio_df.columns[:4])\n",
    "rolling_down(sel, op_col='AXESCP', nat_order='operation_date', method='mean').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = portfolio_df.select(portfolio_df.columns[:4])\n",
    "rolling_down(sel, op_col=['SCOTIAG'], nat_order='operation_date', method='mean').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#portfolio_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in portfolio_df.columns[1:]]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Year parameters input array:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data, analytic base table structuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_date_window(df, from_year, to_year, col_date):\n",
    "    year_array = ophelia.ophelia_array.year_array(from_year, to_year)\n",
    "    split_dates = ophelia.ophelia_df.split_date(df, col_date)\n",
    "    operation_dates_list = ophelia.ophelia_array.sorted_date_list(df, col_date)\n",
    "    date_index_udf = ophelia.ophelia_array.dates_index(operation_dates_list)\n",
    "    portfolio_dates = split_dates.where(col(col_date+\"_year\").isin(year_array))\\\n",
    "                                 .select('*', (date_index_udf(col(col_date))).alias(col_date[:9]+\"_id\"))\n",
    "    return portfolio_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_window_df = portfolio_date_window(\n",
    "    df=portfolio_df, \n",
    "    from_year=\"2016\", \n",
    "    to_year=\"2019\", \n",
    "    col_date=\"operation_date\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitoring_empty_vector(df, feature_type):\n",
    "    float_cols = ophelia.ophelia_array.feature_picking(df)[str(feature_type)]\n",
    "    count_by_col = [spark_count(col(x)).alias(str(x)) for x in float_cols]\n",
    "    aggregate_columns = df.select(*count_by_col)\n",
    "    return aggregate_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_null(panel, missing_days, N):\n",
    "    null_count = panel.select([col(c).alias(c) for c in panel.columns]).collect()[0].asDict()\n",
    "    clean_null_list = [k for k, v in null_count.items() if v < abs(missing_days - N)]\n",
    "    return clean_null_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_empty_vector(df, feature_type, missing_days=10):\n",
    "    sample_count = df.count()\n",
    "    empty_panel = monitoring_empty_vector(df, feature_type)\n",
    "    clean_null_list = debug_null(empty_panel, missing_days, sample_count)\n",
    "    debug_vector = df.drop(*clean_null_list)\n",
    "    return debug_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_none_df = debug_empty_vector(portfolio_window_df, feature_type=\"float\")\n",
    "remove_none_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_impute(df):\n",
    "    float_cols = ophelia.ophelia_array.feature_picking(df)[\"float\"]\n",
    "    numerical_fields = df.agg(*(spark_avg(c).alias(c) for c in df.columns if c in float_cols))\n",
    "    portfolio_base_table = df.na.fill(numerical_fields.collect()[0].asDict())\n",
    "    return portfolio_base_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can write our masterized analytical base table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_base_table = mean_impute(remove_none_df).drop(\"operation_date_year\", \"operation_date_month\", \"operation_date_day\")\n",
    "portfolio_base_table.orderBy(col(\"operation_date\").desc()).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_path = ophelia.ophelia_write.write_parquet(\n",
    "    df=portfolio_base_table,\n",
    "    output_type=\"engine\",\n",
    "    project=\"OpheliaPortfolio\",\n",
    "    part=\"operation_date\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ophelia",
   "language": "python",
   "name": "ophelia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
