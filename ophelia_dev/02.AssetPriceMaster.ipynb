{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, isnan, when, lit, monotonically_increasing_id, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iniciamos Sesi√≥n con Ophelia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualize\n",
    "from ophelia.ophelib.OpheliaMain import Ophelia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:28:56.200 Ophelia [TAPE] +---------------------------------------------------------------------+\n",
      "00:28:56.201 Ophelia [INFO] | My name is Ophelia Vendata                                          |\n",
      "00:28:56.201 Ophelia [INFO] | I am an artificial assistant for data mining & ML engine with spark |\n",
      "00:28:56.201 Ophelia [INFO] | Welcome to Ophelia spark miner engine                               |\n",
      "00:28:56.201 Ophelia [INFO] | Lib Version Ophelia.dev1.0                                          |\n",
      "00:28:56.201 Ophelia [WARN] | V for Vendata...                                                    |\n",
      "00:28:56.201 Ophelia [TAPE] +---------------------------------------------------------------------+\n",
      "00:28:56.201 Ophelia [WARN] Initializing Spark Session\n",
      "00:29:05.307 Ophelia [INFO] Spark Version: 3.0.0\n",
      "00:29:05.307 Ophelia [INFO] This Is: 'AssetPriceMaster' App\n"
     ]
    }
   ],
   "source": [
    "ophelia = Ophelia(\"AssetPriceMaster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CSV daily price Funds file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = ophelia.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_string = split(col('TICKER'), ' ')\n",
    "spark.readFile('data/staging/benchmark/ticker_name', 'parquet').select(\"*\", \n",
    "                                                                       split_string.getItem(0).alias('ticker'),\n",
    "                                                                       split_string.getItem(1).alias('ticker_market'),\n",
    "                                                                       split_string.getItem(2).alias('ticker_asset'),\n",
    "                                                                       (monotonically_increasing_id() + 10).alias('key_id')).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path_file, source, date_col, withSchema=True):\n",
    "    load_data = spark.readFile(path_file, source)\n",
    "    if withSchema is True:\n",
    "        return ophelia.Read.build_portfolio_schema(portfolio_data, date_col)\n",
    "    return portfolio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:50:58.714 Ophelia [INFO] Read Parquet File From Path: ['data/staging/benchmark/close_historic_price']\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o664.parquet.\n: java.lang.ClassCastException: class java.util.ArrayList cannot be cast to class java.lang.String (java.util.ArrayList and java.lang.String are in module java.base of loader 'bootstrap')\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:44)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:737)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-4b387c291dbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mportfolio_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data/staging/benchmark/close_historic_price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mportfolio_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mportfolio_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m../ophelia/ophelib/read/spark_read.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(self, path_source, source, sheet)\u001b[0m\n",
      "\u001b[0;32m../ophelia/ophelib/read/spark_read.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ophelia/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    351\u001b[0m         self._set_opts(mergeSchema=mergeSchema, pathGlobFilter=pathGlobFilter,\n\u001b[1;32m    352\u001b[0m                        recursiveFileLookup=recursiveFileLookup)\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ophelia/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ophelia/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ophelia/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o664.parquet.\n: java.lang.ClassCastException: class java.util.ArrayList cannot be cast to class java.lang.String (java.util.ArrayList and java.lang.String are in module java.base of loader 'bootstrap')\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:44)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:737)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n"
     ]
    }
   ],
   "source": [
    "portfolio_df = spark.readFile('data/staging/benchmark/close_historic_price', 'parquet')\n",
    "portfolio_df.describe().show(5, False)\n",
    "portfolio_df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change impure schema portfolio input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_df = read_data(\n",
    "    path_file=\"data/raw/csv/data.csv\",\n",
    "    source=\"csv\",\n",
    "    date_col=\"operation_date\",\n",
    ")\n",
    "\n",
    "portfolio_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from com.ophelia.wrapper import SparkWrapper\n",
    "from com.ophelia.utils import ListUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def build_portfolio_schema(df: DataFrame, date_col: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Build portfolio schema helps to generate schema data type for fund portfolio price\n",
    "    :param df: spark DataFrame with fund price daily\n",
    "    :param date_col: str name for DataFrame date column\n",
    "    \"\"\"\n",
    "    if df is None or date_col is None:\n",
    "        Read.__logger.error(\"build portfolio schema need 'df' and 'date col' not be None\")\n",
    "        raise TypeError(\"params must not be None\")\n",
    "    Read.__logger.info(\"Build Schema File\")\n",
    "    build_date = date_format(to_date(col(df.columns[0]), 'dd/MM/yyyy'), \"yyyy-MM-dd\")\n",
    "    schema_portfolio = [build_date.cast(\"date\").alias(date_col)] + [col(x).cast('float') for x in df.columns[1:]]\n",
    "    schema_df = df.where(col(df.columns[0]).isNotNull()).select(schema_portfolio)\n",
    "    Read.__logger.info(\"Build Schema Success\")\n",
    "    return schema_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NullDebug(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def cleansing_list(self, partition_by: str, offset: float = 0.5):\n",
    "        if partition_by is None:\n",
    "            raise TypeError(f\"'partition_by' required parameter, invalid {partition_by} input.\")\n",
    "        return self.toNarrow(partition_by, ['pivot', 'value']).groupBy('pivot')\\\n",
    "                   .agg(count(when(isnan('value') | col('value').isNull(), 'value')).alias('null_count'))\\\n",
    "                   .select('*', (col('null_count') / self.Shape[0]).alias('null_pct'))\\\n",
    "                   .where(col('null_pct') <= offset).uniqueRow('pivot')\n",
    "\n",
    "    @staticmethod\n",
    "    def null_clean(self, partition_by=None, offset=0.5):\n",
    "        if partition_by is None:\n",
    "            gen_part = self.select(monotonically_increasing_id().alias('partition_id'), \"*\")\n",
    "            cleansing_list = NullDebug.cleansing_list(gen_part, 'partition_id', offset)\n",
    "            return gen_part.select('partition_id', *cleansing_list)\n",
    "        cleansing_list = NullDebug.cleansing_list(gen_partition, partition_by, offset)\n",
    "        return self.select(partition_by, *cleansing_list)\n",
    "\n",
    "DataFrame.nullClean = NullDebug.null_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lit, explode, struct, array, round as spark_round\n",
    "\n",
    "def panel_format(self, fix_col, new_cols=None):\n",
    "    if isinstance(new_cols, str):\n",
    "        pivot_col, value_col = new_cols.split(',')[0], new_cols.split(',')[1]\n",
    "    elif new_cols is not None and isinstance(new_cols, list):\n",
    "        pivot_col, value_col = new_cols[0], new_cols[1]\n",
    "    else:\n",
    "        pivot_col, value_col = 'no_name_pivot_col', 'no_name_value_col'\n",
    "    cols, dtype = zip(*[(c, t) for (c, t) in self.dtypes if c not in [fix_col]])\n",
    "    \n",
    "    if len(set(map(lambda x: x[-1], dtype[1:]))) != 1:\n",
    "        raise AssertionError(\"Columns must be of the same datatype\")\n",
    "    \n",
    "    generator_explode = explode(array([\n",
    "        struct(lit(c).alias(pivot_col), col(c).alias(value_col)) for c in cols\n",
    "    ])).alias('column_explode')\n",
    "    column_to_explode = [f'column_explode.{pivot_col}', f'column_explode.{value_col}']\n",
    "    \n",
    "    return self.select([fix_col] + [generator_explode]).select([fix_col] + column_to_explode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#panel_format(portfolio_df.drop('close_date', 'close_year'), 'close_timestamp').orderBy(col('close_timestamp')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#portfolio_df.drop('close_date', 'close_year').toPanel('close_timestamp').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#NullDebug.cleansing_list(portfolio_df.drop('close_date', 'close_year'), 'close_timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4643, 71)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_df.drop('close_timestamp', 'close_date', 'close_year').Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4643, 66)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_df.drop('close_timestamp', 'close_date', 'close_year').nullClean(offset=0.1).Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, Window\n",
    "from pyspark.sql.functions import (when, row_number, lit, count, first, sum as spark_sum,\n",
    "                                   min as spark_min, max as spark_max, mean, stddev, variance)\n",
    "\n",
    "def spark_methods():\n",
    "    return {\n",
    "        'sum': spark_sum,\n",
    "        'min': spark_min,\n",
    "        'max': spark_max,\n",
    "        'mean': mean,\n",
    "        'stddev': stddev,\n",
    "        'var': variance,\n",
    "        'first': first,\n",
    "        'count': count,\n",
    "    }\n",
    "\n",
    "def rolling_down(self, op_col, nat_order, min_p=2, window=2, method='sum'):\n",
    "    w = Window.orderBy(nat_order).rowsBetween(\n",
    "        Window.currentRow - (window - 1), Window.currentRow)\n",
    "    if isinstance(op_col, list):\n",
    "        rolling = [spark_methods()[method](c).over(w).alias(f'{c}_rolling_{method}') for c in op_col]\n",
    "        return self.select(*rolling)\n",
    "    if method == 'count':\n",
    "        if isinstance(op_col, list):\n",
    "            rolling = [spark_methods()[method](c).over(w).alias(f'{c}_rolling_{method}') for c in op_col]\n",
    "            return self.select(*rolling)\n",
    "        rolling = spark_methods()[method](op_col).over(w).alias(f'{op_col}_rolling_{method}')\n",
    "        return self.select('*', rolling)\n",
    "    _unbounded_w = Window.orderBy(nat_order).rowsBetween(\n",
    "        Window.unboundedPreceding, Window.currentRow)\n",
    "    rolling = when(\n",
    "        row_number().over(_unbounded_w) >= min_p,\n",
    "        spark_methods()[method](op_col).over(w),\n",
    "        ).otherwise(lit(None)).alias(f'{op_col}_rolling_{method}')\n",
    "    return self.select('*', rolling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = portfolio_df.select(portfolio_df.columns[:4])\n",
    "rolling_down(sel, op_col='AXESCP', nat_order='operation_date', method='mean').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = portfolio_df.select(portfolio_df.columns[:4])\n",
    "rolling_down(sel, op_col=['SCOTIAG'], nat_order='operation_date', method='mean').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#portfolio_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in portfolio_df.columns[1:]]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-02-02 00:00:00', freq='D')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[date for date in pd.date_range(start=\"2020-02-02\",end=\"2020-09-02\")][0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas._libs.tslibs.timestamps.Timestamp"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "datelist = pd.date_range(datetime.today(), periods=5, freq='Y').tolist()\n",
    "datelist[0].__class__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Year parameters input array:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data, analytic base table structuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_date_window(df, from_year, to_year, col_date):\n",
    "    year_array = ophelia.ophelia_array.year_array(from_year, to_year)\n",
    "    split_dates = ophelia.ophelia_df.split_date(df, col_date)\n",
    "    operation_dates_list = ophelia.ophelia_array.sorted_date_list(df, col_date)\n",
    "    date_index_udf = ophelia.ophelia_array.dates_index(operation_dates_list)\n",
    "    portfolio_dates = split_dates.where(col(col_date+\"_year\").isin(year_array))\\\n",
    "                                 .select('*', (date_index_udf(col(col_date))).alias(col_date[:9]+\"_id\"))\n",
    "    return portfolio_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Ophelia' object has no attribute 'ophelia_array'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f6214fe2c9fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m portfolio_window_df = portfolio_date_window(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mportfolio_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfrom_year\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2016\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mto_year\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2019\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcol_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"operation_date\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-809a6f24d446>\u001b[0m in \u001b[0;36mportfolio_date_window\u001b[0;34m(df, from_year, to_year, col_date)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mportfolio_date_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_year\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_year\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0myear_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mophelia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mophelia_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_year\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_year\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msplit_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mophelia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mophelia_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moperation_dates_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mophelia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mophelia_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msorted_date_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdate_index_udf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mophelia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mophelia_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdates_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_dates_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Ophelia' object has no attribute 'ophelia_array'"
     ]
    }
   ],
   "source": [
    "portfolio_window_df = portfolio_date_window(\n",
    "    df=portfolio_df, \n",
    "    from_year=\"2016\", \n",
    "    to_year=\"2019\", \n",
    "    col_date=\"operation_date\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitoring_empty_vector(df, feature_type):\n",
    "    float_cols = ophelia.ophelia_array.feature_picking(df)[str(feature_type)]\n",
    "    count_by_col = [spark_count(col(x)).alias(str(x)) for x in float_cols]\n",
    "    aggregate_columns = df.select(*count_by_col)\n",
    "    return aggregate_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_null(panel, missing_days, N):\n",
    "    null_count = panel.select([col(c).alias(c) for c in panel.columns]).collect()[0].asDict()\n",
    "    clean_null_list = [k for k, v in null_count.items() if v < abs(missing_days - N)]\n",
    "    return clean_null_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_empty_vector(df, feature_type, missing_days=10):\n",
    "    sample_count = df.count()\n",
    "    empty_panel = monitoring_empty_vector(df, feature_type)\n",
    "    clean_null_list = debug_null(empty_panel, missing_days, sample_count)\n",
    "    debug_vector = df.drop(*clean_null_list)\n",
    "    return debug_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_none_df = debug_empty_vector(portfolio_window_df, feature_type=\"float\")\n",
    "remove_none_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_impute(df):\n",
    "    float_cols = ophelia.ophelia_array.feature_picking(df)[\"float\"]\n",
    "    numerical_fields = df.agg(*(spark_avg(c).alias(c) for c in df.columns if c in float_cols))\n",
    "    portfolio_base_table = df.na.fill(numerical_fields.collect()[0].asDict())\n",
    "    return portfolio_base_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can write our masterized analytical base table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_base_table = mean_impute(remove_none_df).drop(\"operation_date_year\", \"operation_date_month\", \"operation_date_day\")\n",
    "portfolio_base_table.orderBy(col(\"operation_date\").desc()).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_path = ophelia.ophelia_write.write_parquet(\n",
    "    df=portfolio_base_table,\n",
    "    output_type=\"engine\",\n",
    "    project=\"OpheliaPortfolio\",\n",
    "    part=\"operation_date\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ophelia",
   "language": "python",
   "name": "ophelia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
